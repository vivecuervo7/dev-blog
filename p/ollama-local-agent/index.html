<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="When a skeptic finds out there's more to AI than just ChatGPT"><title>Running AI models locally with Ollama</title><link rel=canonical href=https://vivecuervo7.github.io/dev-blog/p/ollama-local-agent/><link rel=stylesheet href=/dev-blog/scss/style.min.6f318591210aa4977c35ec8bcde32e29a31493382ff6482b8f441ee61dbc3dff.css><meta property='og:title' content="Running AI models locally with Ollama"><meta property='og:description' content="When a skeptic finds out there's more to AI than just ChatGPT"><meta property='og:url' content='https://vivecuervo7.github.io/dev-blog/p/ollama-local-agent/'><meta property='og:site_name' content='Isaac Dedini'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='Ollama'><meta property='article:tag' content='Qwen'><meta property='article:tag' content='GitHub Copilot'><meta property='article:tag' content='Open WebUI'><meta property='article:published_time' content='2025-07-23T00:00:00+10:00'><meta property='article:modified_time' content='2025-07-23T00:00:00+10:00'><meta property='og:image' content='https://vivecuervo7.github.io/dev-blog/p/ollama-local-agent/image.png'><meta name=twitter:title content="Running AI models locally with Ollama"><meta name=twitter:description content="When a skeptic finds out there's more to AI than just ChatGPT"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://vivecuervo7.github.io/dev-blog/p/ollama-local-agent/image.png'><link rel="shortcut icon" href=/dev-blog/favicon.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/dev-blog/><img src=/dev-blog/img/avatar_hu_ed52bc2f496c1691.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/dev-blog>Isaac Dedini</a></h1><h2 class=site-description></h2></div></header><ol class=menu id=main-menu><li><a href=/dev-blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/dev-blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/dev-blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#getting-started>Getting started</a></li><li><a href=#models>Models</a><ol><li><a href=#choosing-a-model>Choosing a model</a></li></ol></li><li><a href=#open-webui>Open WebUI</a></li><li><a href=#performance>Performance</a></li><li><a href=#ollama-over-the-network>Ollama over the network</a><ol><li><a href=#open-webui-1>Open WebUI</a></li><li><a href=#github-copilot>GitHub Copilot</a></li></ol></li><li><a href=#modelfiles>Modelfiles</a></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/dev-blog/p/ollama-local-agent/><img src=/dev-blog/p/ollama-local-agent/image_hu_1c8c4b7ad3deb5b.png srcset="/dev-blog/p/ollama-local-agent/image_hu_1c8c4b7ad3deb5b.png 800w, /dev-blog/p/ollama-local-agent/image_hu_ec4b9d7565a0d7ce.png 1600w" width=800 height=283 loading=lazy alt="Featured image of post Running AI models locally with Ollama"></a></div><div class=article-details><header class=article-category><a href=/dev-blog/categories/development/ style=background-color:#1c4269;color:#fff>Development
</a><a href=/dev-blog/categories/ai/ style=background-color:#5181b1;color:#fff>AI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/dev-blog/p/ollama-local-agent/>Running AI models locally with Ollama</a></h2><h3 class=article-subtitle>When a skeptic finds out there's more to AI than just ChatGPT</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jul 23, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>10 minute read</time></div></footer></div></header><section class=article-content><p>Fine! I&rsquo;ll have another look at AI. Geez.</p><p>At least, that&rsquo;s how it felt when I begrudgingly decided to look at running some models locally.</p><p>A few months ago, I decided to have a look at <a class=link href=/dev-blog/p/cursor-vibe-check/>vibe coding with Cursor</a>. Now, I&rsquo;ll have to admit that I went into that experiment fairly biased despite my best efforts to keep an open mind, but I walked away not very impressed and with the notion that AI was over-hyped even further cemented in my mind.</p><p>Given a few months and some more poking around with GitHub Copilot, and I think I&rsquo;m ready to say that while I&rsquo;m still quite bearish on AI in general, it has certainly found a place in my workflow. Unfortunately, I have experienced some frustration with it â€” largely along the lines of running out of Claude requests and being left with GPT, which would at times just seemingly disconnect and wouldn&rsquo;t return to working order until I restarted VS Code.</p><p>There&rsquo;s also the whole privacy concern. Personally I find myself unbothered by it, but in the context of customer data I am <em>very</em> cautious about the implications.</p><p>Anyway, waking up with a &ldquo;what do I want to look into today&rdquo; landed me on the topic of running AI models locally.</p><h2 id=getting-started>Getting started</h2><div class=alert>Tip: Install with <code>brew install ollama</code> and then run <code>ollama</code> to see the available commands.</div><p>No surprises that I landed here. Even as someone who has had my head half-buried in the sand when it comes to AI, I was well aware of <a class=link href=https://ollama.com/ target=_blank rel=noopener>Ollama</a>. A bit of light searching yielded alternatives, but I decided early on that I was just going to stick with the basics here.</p><p>Installation was a breeze, just needing a quick <code>brew install ollama</code> to install it locally. Once installed, calling <code>ollama</code> by itself shows us the relatively simple set of commands we can pass it, but to get started there&rsquo;s only a couple we really need.</p><p>First of all though, to start the Ollama server run <code>ollama serve</code>.</p><h2 id=models>Models</h2><p>The basic commands we&rsquo;re after here are <code>ollama run</code>, <code>ollama list</code> and <code>ollama rm</code> to run a model, list all models, and remove models.</p><p>The Ollama <a class=link href=https://ollama.com/search target=_blank rel=noopener>models page</a> details a slew of models that we can run.</p><p><img src=/dev-blog/p/ollama-local-agent/images/ollama-models.png width=3196 height=1344 srcset="/dev-blog/p/ollama-local-agent/images/ollama-models_hu_3c7ccc0a96541239.png 480w, /dev-blog/p/ollama-local-agent/images/ollama-models_hu_3aaa39e87eb0fadf.png 1024w" loading=lazy class=gallery-image data-flex-grow=237 data-flex-basis=570px></p><p>Actually installing and running a model is as simple as calling <code>ollama run llama3.1</code>. This will download it if it hasn&rsquo;t yet been pulled, and runs it immediately. Once installed, we can start typing prompts into the terminal!</p><h3 id=choosing-a-model>Choosing a model</h3><p>I&rsquo;m not going to go too in-depth here â€” mostly because I still don&rsquo;t really understand it! I&rsquo;ll give it a red-hot crack anyway.</p><p>Looking at the list of available models, I saw a bunch of options and was completely lost as to which I needed. Looking at this heavily truncated list below, I was confused as to which I should use. Especially since there we easily 60+ variants to choose from!</p><ul><li><code>llama3.1:405b-text-fp16</code></li><li><code>llama3.1:70b-text-q3_K_S</code></li><li><code>llama3.1:8b-instruct-q5_K_M</code></li></ul><p>Anyway, after a bit of reading I was able to break it down to the following, using <code>llama3.1:8b-instruct-q5_K_M</code> as a reference.</p><ul><li><code>llama3.1</code> â€” the name of the model, each may be better or worse at some things.</li><li><code>8b</code> â€” the number of parameters the model contains. Roughly, the &ldquo;size&rdquo; of the model.</li><li><code>instruct</code> â€” the purpose of the model, which generally is the main function of that model.</li><li><code>q5_K_M</code> â€” the model&rsquo;s quantization, or roughly, level of optimisation.</li></ul><p>The Ollama GitHub repository also provided this helpful note, providing some guidelines around system resource requirements for different model sizes.</p><div class=alert>You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.</div><p>To expand slightly on quantization as that was a completely new term for me, quantization is the process of converting a model&rsquo;s weights from high precision data types to lower precision types. While this generally translates to a smaller resource footprint, it comes at the cost of a potential reduction in accuracy and quality.</p><p>Models can get even more complex than that, and I found <a class=link href=https://developers.redhat.com/articles/2025/04/03/how-navigate-llm-model-names# target=_blank rel=noopener>this page</a> does a good job of explaining the terms without going too deep. In fact, it&rsquo;s just about where I pulled that little quantization explainer from.</p><p>Personally, I landed on the <code>qwen2.5-coder:3b-instruct-q4_K_M</code> and <code>qwen2.5-coder:7b-instruct-q4_K_M</code> models. The 3B model was nice and quick, and seemed to handle most basic tasks with ease. I did find myself frequently switching to the 7B model when the smaller one wasn&rsquo;t quite doing the job.</p><p>I also spent about half an hour looking at new PCs.</p><h2 id=open-webui>Open WebUI</h2><p>As nice as it is to be able to start running models locally with such ease, the interface leaves a little to be desired.</p><p><a class=link href=https://openwebui.com/ target=_blank rel=noopener>Open WebUI</a> offers us a much nicer way to interact with our models â€” even allowing us to run multiple models side-by-side, which is perfect for comparing the output of various models.</p><p><img src=/dev-blog/p/ollama-local-agent/images/open-webui.png width=3202 height=1254 srcset="/dev-blog/p/ollama-local-agent/images/open-webui_hu_19c72c999ea360bd.png 480w, /dev-blog/p/ollama-local-agent/images/open-webui_hu_50a7b16347c3c56a.png 1024w" loading=lazy class=gallery-image data-flex-grow=255 data-flex-basis=612px></p><p>Running the following should both pull and start the docker container. Note that this does also create a volume, so settings, chats, etc. will be persisted.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>docker run -d -p 3000:8080 --add-host<span class=o>=</span>host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main
</span></span></code></pre></td></tr></table></div></div><h2 id=performance>Performance</h2><p>It didn&rsquo;t take long for a wry smile to grow on my face. I&rsquo;ve spent long enough bragging about how well my decision to go with a MacBook Air has been, and after about a year and a half I finally ran into the thing that made me wish I had a little more grunt under the hood.</p><p>Even some of the smallest models were slow enough to be a little annoying, and those that didn&rsquo;t have smaller varieties were just slower than I would consider to be usable. Certainly, with no qualms about GPT harvesting my data, I was not about to pump the brakes on whatever thinking was going on under the hood.</p><p>I decided to flick open my dusty old Windows laptop (in fact it&rsquo;s neither dusty nor old, currently it serves as my game streaming server using <a class=link href=https://github.com/LizardByte/Sunshine target=_blank rel=noopener>Sunlight</a>) and install Ollama there.</p><p>Woah. Okay, now we&rsquo;re running at what looked like 3-4x faster. Turns out having a GPU kinda helps.</p><h2 id=ollama-over-the-network>Ollama over the network</h2><p>Cool. So we&rsquo;ve got a fast-enough-to-be-usable machine running AI models. Unfortunately, I don&rsquo;t actually do any work on that laptop â€” it&rsquo;s pretty much dedicated to gaming* these days. That said, I&rsquo;d had a taste of how the models <em>should</em> be running, and I couldn&rsquo;t look back.</p><p><em>* by which I mean, of course, the times I&rsquo;d love to be playing games if life could just stop throwing me side quests</em></p><p>I&rsquo;m certainly no network buff, and I chose the path of least resistance. I quickly installed <a class=link href=https://ngrok.com/ target=_blank rel=noopener>ngrok</a> and exposed my Ollama endpoint (<code>http://localhost:11434</code>) to see if it was workable. Navigating to the endpoint that ngrok assigned for me, and I could see the very simple page that told me Ollama was running. Sweet!</p><p>Now, I really should clean this up â€” since I only intend to use this while I&rsquo;m sitting within arm&rsquo;s reach of both laptops, I really could just serve this over my local network. For the sake of experimentation, however, that&rsquo;s tomorrow&rsquo;s problem.</p><h3 id=open-webui-1>Open WebUI</h3><p>I guess this one can come first since we&rsquo;ve already talked about setting it up.</p><p>Since I already had this up and running on my MacBook, I just needed to pop into the settings. Under the option for <strong>Connections</strong> there is a section called <strong>Manage Direct Connections</strong>. This is where we can add the URL of our Ollama server.</p><p>With a bit of trial and error, I found that I needed to append the ngrok-provided URL with <code>/v1</code> to get it working. It should pick up the models automatically, although they can be specified explicitly if we want.</p><p>Alternatively, the likely easier way to achieve this would be to simply specify the <code>OLLAMA_BASE_URL</code> while we&rsquo;re creating the container, by adding <code>-e OLLAMA_BASE_URL=</code> to the previous command. Here&rsquo;s the command again, but you&rsquo;ll need to replace the URL with your own.</p><p>Note that this approach does <strong>not</strong> require us to append the URL with <code>v1</code>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>docker run -d -p 3000:8080 --add-host<span class=o>=</span>host.docker.internal:host-gateway -e <span class=nv>OLLAMA_BASE_URL</span><span class=o>=</span>http://localhost:11434 -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main
</span></span></code></pre></td></tr></table></div></div><h3 id=github-copilot>GitHub Copilot</h3><p>I was pretty stoked at this point, but GitHub Copilot was the big one.</p><p>I already knew Ollama was supported â€” in fact, hooking it up to Copilot was the first thing I did after running my first model. Wiring it up to Ollama running on a different computer proved to be a little less obvious, and fortunately I stumbled across a relevant <a class=link href=https://github.com/orgs/community/discussions/156483 target=_blank rel=noopener>GitHub discussion</a>.</p><p>The TL;DR for the above is that we&rsquo;re looking for a setting called <code>github.copilot.chat.byok.ollamaEndpoint</code>. Throwing my ngrok endpoint at that setting allowed me to select the models running on my other machine.</p><p>The only annoyance I found with this is that I had two different sizes of the same model, and GitHub Copilot&rsquo;s interface just showed the same display name for each of them. As a workaround, it looks like I could possibly use a Modelfile to give my models better names, but that opened yet another door â€” and a welcome one at that.</p><h2 id=modelfiles>Modelfiles</h2><p>Basically, one of the things I&rsquo;ve disliked is this idea that I have a generic AI helper that I constantly need to keep feeding cues to. I have a coding style, and part of what has been turning me away from AI is that it won&rsquo;t write the code that I want it to write!</p><p>While, yes, in the context of GitHub Copilot, Cursor, etc., I can just throw a set of rules at it, I disliked the notion that I would need to maintain this on a per-project basis, constantly re-synchronising them with my other projects&mldr; Nope. That&rsquo;s not saving me time.</p><p>So, I started looking into custom models. Here&rsquo;s the <a class=link href=https://ollama.readthedocs.io/en/modelfile/#examples target=_blank rel=noopener>Modelfile reference</a>, which goes into a lot more detail than I have below.</p><div class=code-hint-block><span class=code-hint-path></span><span class=code-hint>svelte-assistant.Modelfile</span></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>FROM qwen2.5-coder:3b-instruct-q4_K_M
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>SYSTEM You are an expert in SvelteKit, the full-stack meta-framework. You are familiar with Svelte 5 syntax. You are familiar with the Svelte documentation found here: https://svelte.dev/docs/svelte/overview and the SvelteKit documentation found here: https://svelte.dev/docs/kit/introduction. When a question or instruction appears to be targeted towards a different web framework, make a suggestion to use a specialised model. Endeavour to answer questions as quickly as possible. Omit any examples or lengthy explanations unless requested.
</span></span></code></pre></td></tr></table></div></div><p>I threw a couple of questions at it, and it answered as much as I expected it to.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&gt;&gt;&gt; What do you specialise in?
</span></span><span class=line><span class=cl>I specialize in SvelteKit and related technologies, including Svelte 5 syntax, documentation from
</span></span><span class=line><span class=cl>svelte.dev/docs/svelte/overview, and the SvelteKit documentation at svelte.dev/docs/kit/introduction.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt;&gt;&gt; Can you help me with a React project?
</span></span><span class=line><span class=cl>For React-related questions or assistance, I suggest using a model specialized in React development.
</span></span></code></pre></td></tr></table></div></div><p>Knowing it was a Hail Mary at best, it unfortunately seemed to not be capable of looking up any information about the newer Svelte 5 syntax â€” which has thus far been a bit of a souring experience when it comes to using AI as a code assistant.</p><p>Additionally, if I asked it a more pointed question about React such as &ldquo;can you show me an example React component&rdquo;, it would just spit out a component instead of telling me to use a model specific to React.</p><p>Honestly, not such a big deal, but I was kind of hoping to get a few models with a &ldquo;soft&rdquo; specialisation that I could feed additional context into with the hope of being able to just run a bunch of smaller, faster, targeted models that better fit the way I tend to use AI.</p><p>At the very least, we can tweak a few parameters and set the &ldquo;tone&rdquo; for the model&rsquo;s responses.</p><h2 id=conclusion>Conclusion</h2><p>It was pretty nice being able to get up and running as easily as it turned out to be. Given my laptop was churning out responses about as quickly as the network-bound GPT or Claude, I&rsquo;m probably going to stick to running this locally for the time being, preserving my capped Claude requests for the truly difficult tasks.</p><p>I&rsquo;m still curious about the potential for those smaller, targeted models, but a lot of the documentation I read about fine-tuning models just went straight over my head. I guess I&rsquo;ll settle for something more within my means, and just set up my Ollama server to run over my local network instead of through ngrok.</p><p>Now to convince my wife that I really do need that expensive PC upgrade&mldr;</p></section><footer class=article-footer><section class=article-tags><a href=/dev-blog/tags/ollama/>Ollama</a>
<a href=/dev-blog/tags/qwen/>Qwen</a>
<a href=/dev-blog/tags/github-copilot/>GitHub Copilot</a>
<a href=/dev-blog/tags/open-webui/>Open WebUI</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><div class="article-list--compact links"><article><a href=https://ollama.com/ target=_blank rel=noopener><div class=article-details><h2 class=article-title>Ollama</h2><footer class=article-time>Get up and running with large language models</footer></div></a></article><article><a href=https://openwebui.com/ target=_blank rel=noopener><div class=article-details><h2 class=article-title>Open WebUI</h2><footer class=article-time>An extensible, self-hosted AI interface</footer></div></a></article></div><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/dev-blog/p/fine-tuning-an-llm/><div class=article-image><img src=/dev-blog/p/fine-tuning-an-llm/image.3cb94e5459cd6e59d8b5290a1e3ca537_hu_d251bf3ae76f45cb.png width=250 height=150 loading=lazy alt="Featured image of post RAG to fine-tuning an LLM" data-key=fine-tuning-an-llm data-hash="md5-PLlOVFnNblnYtSkKHjylNw=="></div><div class=article-details><h2 class=article-title>RAG to fine-tuning an LLM</h2></div></a></article><article class=has-image><a href=/dev-blog/p/cursor-vibe-check/><div class=article-image><img src=/dev-blog/p/cursor-vibe-check/cover.f7cf2a26036d53501b99af2133950a5f_hu_1d3a73dc4b58b6b9.jpg width=250 height=150 loading=lazy alt="Featured image of post Cursor: Are the vibes really worth it?" data-key=cursor-vibe-check data-hash="md5-988qJgNtU1Abma8hM5UKXw=="></div><div class=article-details><h2 class=article-title>Cursor: Are the vibes really worth it?</h2></div></a></article><article class=has-image><a href=/dev-blog/p/deploying-ios-app-to-testflight/><div class=article-image><img src=/dev-blog/p/deploying-ios-app-to-testflight/image.73182ce8e3f7a3580c79b0541c9d3a97_hu_e1de359b0d2e4f59.jpg width=250 height=150 loading=lazy alt="Featured image of post Deploying an iOS app to TestFlight" data-key=deploying-ios-app-to-testflight data-hash="md5-cxgs6OP3o1gMebBUHJ06lw=="></div><div class=article-details><h2 class=article-title>Deploying an iOS app to TestFlight</h2></div></a></article><article class=has-image><a href=/dev-blog/p/react-three-fiber/><div class=article-image><img src=/dev-blog/p/react-three-fiber/image.e467405cfad2c92a64d3a8a9ae313a03_hu_2c7c47c037923ac7.png width=250 height=150 loading=lazy alt="Featured image of post React Three Fiber" data-key=react-three-fiber data-hash="md5-5GdAXPrSySpk06iprjE6Aw=="></div><div class=article-details><h2 class=article-title>React Three Fiber</h2></div></a></article><article class=has-image><a href=/dev-blog/p/less-sass-with-postcss/><div class=article-image><img src=/dev-blog/p/less-sass-with-postcss/cover.330cca0b380f974718c0c04ecb68b100_hu_d78ae7fd53e8af05.jpg width=250 height=150 loading=lazy alt="Featured image of post PostCSS: Styling without the Sass" data-key=less-sass-with-postcss data-hash="md5-MwzKCzgPl0cYwMBOy2ixAA=="></div><div class=article-details><h2 class=article-title>PostCSS: Styling without the Sass</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2024 -
2025 Isaac Dedini</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.27.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/dev-blog/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>