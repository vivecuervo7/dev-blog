<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Building a galactic army of specialized LLMs with MLX"><title>RAG to fine-tuning an LLM</title><link rel=canonical href=https://vivecuervo7.github.io/dev-blog/p/fine-tuning-an-llm/><link rel=stylesheet href=/dev-blog/scss/style.min.6f318591210aa4977c35ec8bcde32e29a31493382ff6482b8f441ee61dbc3dff.css><meta property='og:title' content="RAG to fine-tuning an LLM"><meta property='og:description' content="Building a galactic army of specialized LLMs with MLX"><meta property='og:url' content='https://vivecuervo7.github.io/dev-blog/p/fine-tuning-an-llm/'><meta property='og:site_name' content='Isaac Dedini'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='Ollama'><meta property='article:tag' content='Qwen'><meta property='article:tag' content='Llama'><meta property='article:tag' content='GitHub Copilot'><meta property='article:tag' content='Open WebUI'><meta property='article:published_time' content='2025-07-31T00:00:00+10:00'><meta property='article:modified_time' content='2025-08-20T08:11:00+10:00'><meta property='og:image' content='https://vivecuervo7.github.io/dev-blog/p/fine-tuning-an-llm/image.png'><meta name=twitter:title content="RAG to fine-tuning an LLM"><meta name=twitter:description content="Building a galactic army of specialized LLMs with MLX"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://vivecuervo7.github.io/dev-blog/p/fine-tuning-an-llm/image.png'><link rel="shortcut icon" href=/dev-blog/favicon.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/dev-blog/><img src=/dev-blog/img/avatar_hu_ed52bc2f496c1691.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/dev-blog>Isaac Dedini</a></h1><h2 class=site-description></h2></div></header><ol class=menu id=main-menu><li><a href=/dev-blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/dev-blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/dev-blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#the-use-case>The use case</a><ol><li><a href=#the-pain-point>The pain point</a></li><li><a href=#the-litmus-test>The litmus test</a></li><li><a href=#the-potential-solutions>The potential solutions</a></li><li><a href=#the-reference-material>The reference material</a></li></ol></li><li><a href=#github-copilot-instructions>GitHub Copilot instructions</a></li><li><a href=#retrieval-augmented-generation-rag>Retrieval-Augmented Generation (RAG)</a></li><li><a href=#system-prompt>System prompt</a></li><li><a href=#fine-tuning>Fine-tuning</a><ol><li><a href=#datasets>Datasets</a><ol><li><a href=#metas-synthetic-data-kit>Meta&rsquo;s Synthetic Data Kit</a></li><li><a href=#formats>Formats</a></li><li><a href=#files-required>Files required</a></li></ol></li><li><a href=#mlx-lm>mlx-lm</a><ol><li><a href=#installation>Installation</a></li><li><a href=#training>Training</a></li><li><a href=#fusing>Fusing</a></li></ol></li><li><a href=#llamacpp>llama.cpp</a><ol><li><a href=#installation-1>Installation</a></li><li><a href=#converting-to-gguf>Converting to .gguf</a></li></ol></li><li><a href=#ollama-create>ollama create</a><ol><li><a href=#chat-template>Chat template</a></li><li><a href=#parameters>Parameters</a></li></ol></li></ol></li><li><a href=#results>Results</a><ol><li><a href=#the-prompt>The prompt</a></li><li><a href=#per-model-results>Per-model results</a><ol><li><a href=#qwen-3--17b>Qwen 3 — 1.7B</a></li><li><a href=#qwen-3--4b>Qwen 3 — 4B</a></li><li><a href=#qwen-3--8b>Qwen 3 — 8B</a></li><li><a href=#llama-32--1b>Llama 3.2 — 1B</a></li><li><a href=#llama-32--3b>Llama 3.2 — 3B</a></li></ol></li><li><a href=#summary>Summary</a></li><li><a href=#number-of-layers>Number of layers</a></li></ol></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#next-steps>Next steps</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/dev-blog/p/fine-tuning-an-llm/><img src=/dev-blog/p/fine-tuning-an-llm/image_hu_a57278f99db73ef2.png srcset="/dev-blog/p/fine-tuning-an-llm/image_hu_a57278f99db73ef2.png 800w, /dev-blog/p/fine-tuning-an-llm/image_hu_4a51caf5a64d9d60.png 1600w" width=800 height=482 loading=lazy alt="Featured image of post RAG to fine-tuning an LLM"></a></div><div class=article-details><header class=article-category><a href=/dev-blog/categories/development/ style=background-color:#1c4269;color:#fff>Development
</a><a href=/dev-blog/categories/ai/ style=background-color:#5181b1;color:#fff>AI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/dev-blog/p/fine-tuning-an-llm/>RAG to fine-tuning an LLM</a></h2><h3 class=article-subtitle>Building a galactic army of specialized LLMs with MLX</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jul 31, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>31 minute read</time></div></footer></div></header><section class=article-content><p>A little over a week ago, I started toying with <a class=link href=/dev-blog/p/ollama-local-agent/>running Large Language Models (LLMs) locally using Ollama</a>.</p><p>Apparently, all that did was to get my head spinning with a few ideas. Largely, I was finding myself incredibly curious about the potential of running some of those smaller LLMs locally, and having each of them specialised in a specific domain.</p><p>For context, I should probably clarify how I tend to find myself using LLMs.</p><h2 id=the-use-case>The use case</h2><p>I did have <a class=link href=/dev-blog/p/cursor-vibe-check/>a bit of a play with genuine vibe coding</a>, but quickly found that it wasn&rsquo;t for me. I like understanding my codebase, and while certainly impressive that it could churn out functional code it a matter of minutes, I found it more often than not generated <em>too</em> much. I found the sheer amount of code difficult to meaningfully review, and more often than not found myself blowing away all the changes even after a good half hour or so of prompting.</p><p>AI has still found a home in my workflow however, but I find my use case to be mostly based around &ldquo;I want this very specific thing, and I want it done roughly like this&rdquo;. At a high level my workflow looks more like this:</p><ul><li>Creating a database migration<ul><li>Manually create the file, add to context</li><li>Prompt: <em>Add a migration that adds a <code>users</code> table with <code>id</code>, <code>name</code>, and <code>email</code> columns</em></li></ul></li><li>Creating the model / entity file<ul><li>Manually create the file, add to context</li><li>Add the migration file we created earlier to context</li><li>Prompt: <em>Create an entity that reflects this newly created table</em></li></ul></li></ul><p>&mldr;and so on.</p><p>There&rsquo;s a bit of manual work to say &ldquo;this is where I want you to put the code&rdquo;, and then letting AI run away with the code that needs to be added to that specific file. Which means, usually I want it to <em>specifically</em> write SQL, or add a new SvelteKit endpoint, or maybe append a route to a .NET controller.</p><p>I&rsquo;m not really leveraging the capabilities of a model that is generally knowledgeable in all these things, all at once.</p><h3 id=the-pain-point>The pain point</h3><p>Honestly, this approach has been working for me. It very quickly gives me a chunk of code that is small enough to either quickly tick off or tweak to my liking, and then I can move on and not need to think about that file again.</p><p>The problem I have consistently run into however, is that the training for most (all?) of these LLMs was done prior to the release of Svelte 5. And Svelte 5 brought significant changes to the syntax. As one can imagine, this amounted to a <em>lot</em> of generated code that was just&mldr; wrong.</p><h3 id=the-litmus-test>The litmus test</h3><p>Given that very clear and resounding pain point, I settled on one specific thing I wanted to achieve — something that the models I was using were just completely incapable of in their current state.</p><blockquote><p>Could I teach a model how to write in Svelte 5&rsquo;s syntax?</p></blockquote><p>With my thoughts already occupied with this idea of having a handful of specialised, smaller LLMs, I figured this would be the perfect test.</p><h3 id=the-potential-solutions>The potential solutions</h3><p>There were a few different ways I could go about trying to solve this problem. To date, I hadn&rsquo;t really leaned too heavily on GitHub Copilot&rsquo;s instructions files, but I figured it would be the first stop.</p><p>While that seemed a sane approach, I was very conscious of the fact that using smaller LLMs, I probably needed to be careful with context lengths. While this was likely less of a concern given my short-lived interactions with the LLM, it still felt like a sub-par solution.</p><p>Enter a couple of terms that I had seen bandied about, but really at this point hadn&rsquo;t understood. Namely, <strong>system prompts</strong>, <strong>Retrieval-Augmented Generation (RAG)</strong> and <strong>fine-tuning</strong>.</p><h3 id=the-reference-material>The reference material</h3><p>After spending half a day trying to get an LLM to scrape some meaningful data off the Svelte documentation website, I discovered that the Svelte website actually has a page <a class=link href=https://svelte.dev/docs/llms target=_blank rel=noopener>specific to LLMs</a>.</p><p><img src=/dev-blog/p/fine-tuning-an-llm/images/homer-doh.gif width=480 height=366 srcset="/dev-blog/p/fine-tuning-an-llm/images/homer-doh_hu_2908f4d5d232ede4.gif 480w, /dev-blog/p/fine-tuning-an-llm/images/homer-doh_hu_741fc5cfb016f9b0.gif 1024w" loading=lazy alt="Not the last time I found myself saying that." class=gallery-image data-flex-grow=131 data-flex-basis=314px></p><p>I also discovered that this <code>llms.txt</code> file is a <a class=link href=https://llmstxt.org/ target=_blank rel=noopener>proposed standard</a>, and there&rsquo;s a <a class=link href=https://directory.llmstxt.cloud/ target=_blank rel=noopener>handy directory of products and companies</a> that have adopted it.</p><p>Awesome! What really got me interested, however, was the presence of some text files that had the complete documentation, including compressed versions for smaller LLMs.</p><ul><li><a class=link href=https://svelte.dev/llms.txt target=_blank rel=noopener>/llms.txt</a> — a listing of the available files</li><li><a class=link href=https://svelte.dev/llms-full.txt target=_blank rel=noopener>/llms-full.txt</a> (~1 MB) — complete documentation for Svelte, SvelteKit and the CLI</li><li><a class=link href=https://svelte.dev/llms-medium.txt target=_blank rel=noopener>/llms-medium.txt</a> (~0.5 MB) — compressed documentation for use with medium context windows</li><li><a class=link href=https://svelte.dev/llms-small.txt target=_blank rel=noopener>/llms-small.txt</a> (45 KB) — highly compressed documentation for use with smaller context windows</li></ul><h2 id=github-copilot-instructions>GitHub Copilot instructions</h2><p>I won&rsquo;t go too deep on this one. The short version is that, <a class=link href=https://docs.github.com/en/copilot/how-tos/configure-custom-instructions/add-repository-instructions target=_blank rel=noopener>following the documentation</a>, I was able to essentially paste the contents of the <code>llms-*.txt</code> file into <code>copilot-instructions.md</code>. I actually got some really positive results with this approach. Certainly, once plugged into a fully-scaffolded project, it was able to generate some fairly accurate code.</p><p>Surprisingly however, I got far better results with the <em>smaller</em> <code>llms-small.txt</code> file, which was only 45 KB in size. I figure that this was likely due to the limited context window of the smaller models, although truthfully I didn&rsquo;t really know what to expect if I exceeded that — assuming this was even the case.</p><p>I definitely considered this to be a huge win, and honestly I could have likely stopped here.</p><p>In the spirit of full disclosure, I did run this with GPT and Claude as well, as the local models don&rsquo;t seem to be capable of actually generating files etc. Claude was unsurprisingly by far the standout here, but not without its problems. I&rsquo;ll summarise the experience with each of them below.</p><p>Generally speaking however, one pleasant experience was that I no longer needed to specify which framework the component needed to be written for. I used a very simple prompt of <em>&ldquo;Create a counter component&rdquo;</em>. These were all run via GitHub Copilot, with the scaffolded project loaded into VS Code.</p><div class=table-wrapper><table><thead><tr><th>Model</th><th>Notes</th></tr></thead><tbody><tr><td>Llama3.2 3B</td><td>This one pretty much flopped. The code it spat out didn&rsquo;t use Svelte 5 syntax, and didn&rsquo;t appear to even use <a class=link href=https://svelte.dev/docs/svelte/legacy-reactive-assignments target=_blank rel=noopener>legacy reactive statements</a>. I&rsquo;m not being too critical of it at this point however, as it&rsquo;s easily the smallest model used.</td></tr><tr><td>Qwen 3 8B</td><td>I&rsquo;ve honestly found Qwen to be a little hit and miss in GitHub Copilot specifically, often getting caught up in it&rsquo;s reasoning, getting halfway through a <code>&lt;think></code> block and just&mldr; stopping. That said, the one time it actually generated the code I wanted, it was spot on and told me to put in in the correct place.</td></tr><tr><td>GPT-4.1</td><td>Created a <em>very</em> simple counter component, but put it in the wrong place. Additionally, it was initially created with botched <code>&lt;script></code> tags, and when it finished trying to fix them they were just gone — resulting in code that wouldn&rsquo;t even compile.</td></tr><tr><td>Claude Sonnet 4</td><td>I guess someone had to show off, and that someone was Claude. By a <em>long</em> way — but not necessarily in a <em>good</em> way. Claude checked the project structure, then created the component at the right location. All the correct syntax was used, even cross-referencing other components to confirm. But, in typical Claude fashion, the component was a big 240-line block of code complete with styling and all of the functionality that I <em>didn&rsquo;t</em> want.</td></tr></tbody></table></div><p>I decided to push Claude a bit further here and managed to vibe code my way to a full-blown storefront for a shoe store. I was actually pretty surprised at how easily I could follow along this time — but truth be told, I have been working towards a very succinct stack which meant there was just less code to review. Styling however did get a bit messy, and there were a lot of follow-up prompts to try and get it Claude to keep that manageable.</p><p>And really, that latter point is one of the main reasons why I didn&rsquo;t want to just stop here. If I were to just continue with this pattern, I was going to be spending all my time fighting against Claude, trying to keep it in check.</p><p>Additionally, was I going to just keep needing to add more and more documentation to the instructions? Was there even an <code>llms-*.txt</code> file out there for Tailwind? How do I provide this same information to <a class=link href=https://openwebui.com/ target=_blank rel=noopener>Open WebUI</a>?</p><h2 id=retrieval-augmented-generation-rag>Retrieval-Augmented Generation (RAG)</h2><p>I&rsquo;m going to be completely honest here — I&rsquo;m not entirely sure how this is supposed to work in the context of <em>both</em> GitHub Copilot and Open WebUI, especially when we&rsquo;re talking about having a highly-specialised model.</p><p>My end-goal here was to have a single, unified experience that would be consistent across both GitHub Copilot and Open WebUI. While conceptually speaking RAG isn&rsquo;t overly complex, the best I could really find here was to create a <a class=link href=https://docs.openwebui.com/features/workspace/knowledge/ target=_blank rel=noopener>knowledge base</a> in Open WebUI, and have it reference the knowledge base itself when generating for a prompt.</p><p>Open WebUI also allows us to <a class=link href=https://docs.openwebui.com/tutorials/tips/rag-tutorial#create-a-custom-model-with-the-knowledge-base target=_blank rel=noopener>create new models</a> that have a system prompt, as well as a constant reference to a knowledge base.</p><p>This actually worked <em>really</em> well, honestly. I wasn&rsquo;t sure that this was conceptually any different to GitHub Copilot&rsquo;s instructions, but it certainly did a far better job of just doing the thing I wanted it to do. Maybe GitHub Copilot was just getting in the way? Anyhow, it felt like the &ldquo;other side of the coin&rdquo; to GitHub Copilot&rsquo;s instructions, albeit a little shinier, despite not being plugged into my code editor.</p><p><img src=/dev-blog/p/fine-tuning-an-llm/images/open-webui-svelte.png width=927 height=377 srcset="/dev-blog/p/fine-tuning-an-llm/images/open-webui-svelte_hu_c9b1db40060a26ab.png 480w, /dev-blog/p/fine-tuning-an-llm/images/open-webui-svelte_hu_8e8f257b7423631c.png 1024w" loading=lazy alt="qwen3:8b referencing the Svelte documentation’s llms-small.txt file" class=gallery-image data-flex-grow=245 data-flex-basis=590px></p><p>I should also note that when I tried to split up this file and provide it with a larger number of smaller files, it often struggled to find the <em>right</em> files and would start returning plainly incorrect responses. As with GitHub Copilot instructions, this just didn&rsquo;t offer a portable, consistent experience across different interfaces.</p><p>In the context of GitHub Copilot specifically, I had to keep telling it to look up the documentation, and even then it often just decided to do things it&rsquo;s own way.</p><h2 id=system-prompt>System prompt</h2><p>So, feeling like I&rsquo;d gotten <em>somewhere</em> with the two approaches above, I really wanted to try and consolidate this into a single, consistent model that both GitHub Copilot and Open WebUI could use.</p><p>Enter Ollama&rsquo;s Modelfiles. I <a class=link href=/dev-blog/p/ollama-local-agent/#modelfiles>touched on these briefly</a> while first looking into running models locally, but essentially they provide a way for me to create a completely new model based on an <em>existing</em> model, with some additional tweaks for things such as parameters, <strong>system prompts</strong> and templates. The Modelfile reference can be found <a class=link href=https://ollama.readthedocs.io/en/modelfile/ target=_blank rel=noopener>here</a>.</p><p>Considering the success I&rsquo;d had with the two earlier approaches, I figured that what I really needed was to just have a model that always had this in context, right? That&rsquo;s effectively what was happening with the two separate approaches — although one was being very explicit in telling GitHub Copilot to <em>always</em> consider the instructions, and the other was giving Open WebUI access to the file and <em>hoping</em> that it always referenced it.</p><p>So, it seemed to make sense that I could just whack the contents of <code>llms-small.txt</code> into the system prompt of a new model, and then let both GitHub Copilot and Open WebUI use it directly, with no additional context required.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>FROM qwen3:8b
</span></span><span class=line><span class=cl>SYSTEM {contents of llms-small.txt}
</span></span></code></pre></td></tr></table></div></div><p>Specify the model, and specify the system prompt, which was just a dump of the whole text file. Running the command below then created the new model. Piece of cake!</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>ollama create svelte-system-prompt -f ./Modelfile
</span></span></code></pre></td></tr></table></div></div><p>Surprisingly, this just didn&rsquo;t quite work as well as I&rsquo;d hoped. Not knowing too much about how GitHub Copilot&rsquo;s instructions or Open WebUI&rsquo;s RAG worked in terms of how much weight was given to the additional context, I figured that this might just have been applied a little differently.</p><p>In any case, this was a bit of a flop altogether.</p><h2 id=fine-tuning>Fine-tuning</h2><p>All of my reading up until this point had gradually leaned towards this eventuality. With my previous endeavours failing — or least not quite hitting the mark — I decided to look into what was actually required to fine-tune a model.</p><p>I&rsquo;ll try to touch on the various tools etc. in order here. This isn&rsquo;t quite how it panned out in practice, but it should provide a good overview of what&rsquo;s involved.</p><p>Additionally, I decided to move away from trying to train it on Svelte here. I did give it a good crack at first, with varying levels of success. Ultimately I was left unsure as to whether the dataset I had created was actually any good, or whether the models I was using were just too small. Different training methods added another variable into the mix, and on top of that, with Qwen 3 being a reasoning model I made a bit of a mess trying to insert reasoning data into the training dataset.</p><p>Anyway, I decided to train it on a much more focused topic — <strong>the dimensions and markings of a rugby league field</strong>.</p><p>I grabbed a PDF from <a class=link href=https://www.harrodsport.com/uploads/wysiwyg/file/rugby-league-pitch-dimensions-pdf.pdf target=_blank rel=noopener>here</a>, and used that as the basis for my training dataset.</p><h3 id=datasets>Datasets</h3><p>Of course, the easiest way to create a dataset for training an LLM, was to use an LLM. I tried getting a couple of models to scrape the PDF, with ChatGPT being the quickest way to generate a large file.</p><h4 id=metas-synthetic-data-kit>Meta&rsquo;s Synthetic Data Kit</h4><p>I stumbled across <a class=link href=https://github.com/meta-llama/synthetic-data-kit target=_blank rel=noopener>Meta&rsquo;s Synthetic Data Kit</a> which is purpose built for creating these datasets <em>far</em> too late in the piece, however I found that I wasn&rsquo;t able to get a meaningful dataset anyhow. It simply required a model that was too large to run on my machine.</p><p>I won&rsquo;t go into details on how to run this, but it looks like an effective tool for slurping up large amounts of data and spitting out a usable dataset.</p><p>It just might need either a beefy setup, or using a rented workstation from services like <a class=link href=https://vast.ai/ target=_blank rel=noopener>Vast.ai</a>.</p><h4 id=formats>Formats</h4><p>The file itself needs to be a <code>jsonl</code> file, which is essentially a file with a JSON object per line. The file I ended up with used the <strong>messages</strong> format, and looked like this (multiplied by many, many rows):</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;messages&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;Summarize the dimensions of a rugby league field.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;assistant&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;A rugby league field is 68 metres wide, and 112-122 metres long.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>Now, a couple of things to note here.</p><p>There are actually a few ways to format the dataset. Primarily though, and as might be evident simply by looking at that example, the dataset is essentially a collection of objects that describe a conversation.</p><p>The other types are a little simpler, but I initially opted for the more conversational <strong>messages</strong> format. It is however worth noting that this format requires specific models — or rather, the particular models need to be tuned appropriately. The base llama models for example did not work out of the box, and required me to use the <code>instruct</code> tuned versions.</p><p>In addition to the <strong>messages</strong> format above, there is also the <strong>completions</strong> format:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;prompt&#34;</span><span class=p>:</span> <span class=s2>&#34;Summarize the dimensions of a rugby league field.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;completion&#34;</span><span class=p>:</span> <span class=s2>&#34;A rugby league field is 68 metres wide, and 112-122 metres long.&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>And the <strong>text</strong> format:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;A rugby league field is 68 metres wide, and 112-122 metres long.&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=files-required>Files required</h4><p>Now, training data on its own is all well and good, but we also need verification data. Since we&rsquo;re using <code>mlx-lm</code> for this (sorry, Windows folks — this one is Apple only, but there are options that should work just as well on Windows), we&rsquo;ll need the following files.</p><ul><li><code>train.jsonl</code></li><li><code>valid.jsonl</code></li></ul><p>The <code>valid</code> data should be smaller than the training data itself — I&rsquo;ve seen a few recommendations, but I just opted for around 10-20% of the training data size. A <code>test.jsonl</code> file is also recommended, which can be used to evaluate the model after training.</p><h3 id=mlx-lm>mlx-lm</h3><p>Using <code>mlx-lm</code> was actually one of the easier parts to get right. The setup did require using Python, which is always fun considering how infrequently I use it, but once I jogged the memory on how to set up a virtual environment, we had the ball rolling.</p><p>Before we get into the individual commands used, it&rsquo;s worth mentioning that <code>mlx-lm</code> can pull models directly from <a class=link href=https://huggingface.co/ target=_blank rel=noopener>Hugging Face</a>, meaning we don&rsquo;t need to download models, or figure out where they might be stored. It <em>does</em> however mean that the names might looks a little different to what we&rsquo;re used to seeing with Ollama, but rest assured they&rsquo;re all talking about the same models.</p><h4 id=installation>Installation</h4><p>Running <code>pip install mlx-lm</code> gets us the basic package, and we can actually start playing with models right off the bat with <code>mlx_lm.generate --prompt "Hello"</code>. Starting a continuous chat can be kicked off with <code>mlx_lm.chat</code>.</p><p>I&rsquo;m not really sure which model gets used when you don&rsquo;t supply the argument, but providing a <code>--model</code> argument will use the specified model. As mentioned, these should can be repositories on Hugging Face, so to grab the Qwen3 1.7B model as an example, we simply need to run <code>mlx_lm.generate --model qwen/qwen3-1.7b --prompt "Hello"</code>.</p><h4 id=training>Training</h4><p>Now that we&rsquo;re done playing with our toys, it&rsquo;s time to do our best Sid from Toy Story impersonation and start messing with the guts of our models.</p><p><img src=/dev-blog/p/fine-tuning-an-llm/images/sid-workshop.gif width=498 height=373 srcset="/dev-blog/p/fine-tuning-an-llm/images/sid-workshop_hu_fae35b09a1c591aa.gif 480w, /dev-blog/p/fine-tuning-an-llm/images/sid-workshop_hu_69cac3ea5403e8a7.gif 1024w" loading=lazy class=gallery-image data-flex-grow=133 data-flex-basis=320px></p><p>It&rsquo;s probably a good time to talk about what <em>type</em> of training this is. Or, types. And full disclaimer, this is where I started to get a little lost — suffice to say that I&rsquo;m still not entirely sure how much of a difference there is between the different types of training that <code>mlx-lm</code> offers beyond a very rough idea.</p><div class=table-wrapper><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>Full</td><td>Updates all of the model weights (or parameters) of the pre-trained model. <em>Very</em> resource intensive, and risks &ldquo;over-fitting&rdquo;, causing a model to &ldquo;forget&rdquo; some of its original data.</td></tr><tr><td>LoRA</td><td><strong>Low-Rank Adaptation</strong>. We track the <em>changes</em> we want to make to the weights. We can also freeze some of the layers to reduce the number of parameters we&rsquo;re adjusting. Far more efficient while fine-tuning.</td></tr><tr><td>DoRA</td><td><strong>Weight-Decomposed Low-Rank Adaptation</strong>. Too complicated for me to understand the differences, but the consensus seems to be that it it provides more accurate results than LoRA with similar efficiency gains. More information <a class=link href=https://developer.nvidia.com/blog/introducing-dora-a-high-performing-alternative-to-lora-for-fine-tuning/ target=_blank rel=noopener>here</a>.</td></tr></tbody></table></div><p>QLoRA is also available, which is simply LoRA that works on quantized models. <code>mlx-lm</code> will automatically use this if our <code>--model</code> argument points to a quantized model.</p><p>Now, this video does a great job of explaining a bunch of things that are well and truly over my head — it&rsquo;s definitely worth a watch if you&rsquo;re interested in the details.</p><div class=video-wrapper><iframe loading=lazy src=https://www.youtube.com/embed/t1caDsMzWBk allowfullscreen title="YouTube Video"></iframe></div><p>I tried both full and LoRA training to figure out which was best for my use case. Considering that I wanted a highly specialized model, I wasn&rsquo;t sure if over-fitting would necessarily be the biggest concern.</p><p>In any case, the command to run <strong>full</strong> fine-tuning is as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>mlx_lm.lora --train <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>            --model qwen/qwen3-1.7b <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>            --data data <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>            --iters <span class=m>200</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>            --fine-tune-type full
</span></span></code></pre></td></tr></table></div></div><p>We can easily switch that last argument to <code>lora</code> (or <code>dora</code>) to use the other types of training. This introduces some additional arguments we can pass, but we can leave them at their defaults for now.</p><p>It is worth mentioning here that this is a resource-intensive task, and more than once I found myself running out of memory and watching the training process crash. <a class=link href=https://github.com/ml-explore/mlx-lm/blob/e9b1649662d261e8eefea506c705a7370bb92449/mlx_lm/LORA.md#memory-issues target=_blank rel=noopener>This page</a> details a few methods to try and reduce the memory usage, but I found that the biggest impact was simply to use a smaller model — bearing in mind that this will also reduce the quality of the final model.</p><p>Run the command with your desired combination of parameters, and we should end up with a folder called <code>adapters</code>. Inside, is the result of our fine-tuning in the form of a Safetensor adapter!</p><p>The console output should also look something like the below.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>Loading pretrained model
</span></span><span class=line><span class=cl>Fetching <span class=m>9</span> files: 100%<span class=p>|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████<span class=p>|</span> 9/9 <span class=o>[</span>00:00&lt;00:00, 13217.34it/s<span class=o>]</span>
</span></span><span class=line><span class=cl>Loading datasets
</span></span><span class=line><span class=cl>Training
</span></span><span class=line><span class=cl>Trainable parameters: 0.056% <span class=o>(</span>0.967M/1720.575M<span class=o>)</span>
</span></span><span class=line><span class=cl>Starting training..., iters: <span class=m>100</span>
</span></span><span class=line><span class=cl>Calculating loss...: 100%<span class=p>|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████<span class=p>|</span> 25/25 <span class=o>[</span>00:11&lt;00:00,  2.22it/s<span class=o>]</span>
</span></span><span class=line><span class=cl>Iter 1: Val loss 5.646, Val took 11.289s
</span></span><span class=line><span class=cl>Iter 10: Train loss 4.538, Learning Rate 1.000e-05, It/sec 1.376, Tokens/sec 317.131, Trained Tokens 2304, Peak mem 4.695 GB
</span></span><span class=line><span class=cl>Iter 20: Train loss 2.579, Learning Rate 1.000e-05, It/sec 1.281, Tokens/sec 303.100, Trained Tokens 4671, Peak mem 5.034 GB
</span></span><span class=line><span class=cl>Iter 30: Train loss 1.801, Learning Rate 1.000e-05, It/sec 1.545, Tokens/sec 314.796, Trained Tokens 6708, Peak mem 5.034 GB
</span></span><span class=line><span class=cl>Iter 40: Train loss 1.522, Learning Rate 1.000e-05, It/sec 1.508, Tokens/sec 336.666, Trained Tokens 8941, Peak mem 5.034 GB
</span></span><span class=line><span class=cl>Iter 50: Train loss 1.425, Learning Rate 1.000e-05, It/sec 1.514, Tokens/sec 313.347, Trained Tokens 11010, Peak mem 5.034 GB
</span></span><span class=line><span class=cl>Iter 60: Train loss 1.261, Learning Rate 1.000e-05, It/sec 1.577, Tokens/sec 341.066, Trained Tokens 13173, Peak mem 5.034 GB
</span></span><span class=line><span class=cl>Iter 70: Train loss 1.162, Learning Rate 1.000e-05, It/sec 1.348, Tokens/sec 318.090, Trained Tokens 15532, Peak mem 5.034 GB
</span></span><span class=line><span class=cl>Iter 80: Train loss 1.168, Learning Rate 1.000e-05, It/sec 1.243, Tokens/sec 324.718, Trained Tokens 18144, Peak mem 5.429 GB
</span></span><span class=line><span class=cl>Iter 90: Train loss 1.077, Learning Rate 1.000e-05, It/sec 1.415, Tokens/sec 318.347, Trained Tokens 20394, Peak mem 5.429 GB
</span></span><span class=line><span class=cl>Calculating loss...: 100%<span class=p>|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████<span class=p>|</span> 25/25 <span class=o>[</span>00:11&lt;00:00,  2.18it/s<span class=o>]</span>
</span></span><span class=line><span class=cl>Iter 100: Val loss 1.241, Val took 11.401s
</span></span><span class=line><span class=cl>Iter 100: Train loss 0.746, Learning Rate 1.000e-05, It/sec 1.623, Tokens/sec 339.464, Trained Tokens 22486, Peak mem 5.429 GB
</span></span><span class=line><span class=cl>Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.
</span></span><span class=line><span class=cl>Saved final weights to adapters/adapters.safetensors.
</span></span></code></pre></td></tr></table></div></div><p>Lots of useful information, but critically we want to keep an eye on the <strong>Train / Val loss</strong> values (training and validation loss, respectively). Typically, the lower the better, these essentially indicate how well the model is learning.</p><p>My understanding is that we want the validation loss to be close to the the training loss. If the validation loss is significantly lower than the training loss, it indicates under-fitting (lower accuracy relative to the training data), while a significantly higher validation loss indicates over-fitting (higher accuracy relative to the training data, at the cost of existing knowledge).</p><h4 id=fusing>Fusing</h4><p>Honestly, I prefer to refer to this as &ldquo;baking&rdquo; the adapter in. Apparently the community is dead-set on calling it &ldquo;fusing&rdquo;. That just reminds me of an over-protected childhood where I wasn&rsquo;t allowed to watch Dragonball Z.</p><p><img src=/dev-blog/p/fine-tuning-an-llm/images/fusion.gif width=480 height=360 srcset="/dev-blog/p/fine-tuning-an-llm/images/fusion_hu_b58d9ade0b479220.gif 480w, /dev-blog/p/fine-tuning-an-llm/images/fusion_hu_9b6255783841671.gif 1024w" loading=lazy class=gallery-image data-flex-grow=133 data-flex-basis=320px></p><p>Moving on, this step isn&rsquo;t necessarily required, depending on the model we&rsquo;ve used. I haven&rsquo;t tried this with one of the compatible models, but Ollama&rsquo;s <a class=link href=https://ollama.readthedocs.io/en/modelfile/#safetensor-adapter target=_blank rel=noopener>Modelfile reference</a> does mention the ability to simply reference a Safetensor adapter, which is what we get when we run the <code>mlx_lm.lora --train</code> command above.</p><p>Since neither of the models I was using were compatible, I did need to produce a fused model. We can actually run the model we used to create the adapter with or without the adapter attached to it (add an <code>--adapter-path</code> argument to a <code>mlx_lm.generate</code> command to use the adapter), but this wasn&rsquo;t going to give me a model that could be run by Ollama.</p><p>To combine the model and adapter, we run the following.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>mlx_lm.fuse --model qwen/qwen3-1.7b <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>            --adapter-path adapters <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>            --save-path ./model
</span></span></code></pre></td></tr></table></div></div><p>And, once again we&rsquo;ll get a folder called <code>model</code>, which contains a <em>bunch</em> of files. This is your model!</p><p>If we&rsquo;re itching to see how it works, we can pass <code>--model ./model</code> to <code>mlx_lm.generate</code> or <code>mlx_lm.chat</code>, and it will use the newly created model. Good for a quick turnaround for testing.</p><h3 id=llamacpp>llama.cpp</h3><p>Once again, this may be unnecessary based on your model. The <a class=link href=https://ollama.readthedocs.io/en/modelfile/#build-from-a-safetensors-model target=_blank rel=noopener>Modelfile reference</a> also mentions being able to build from a Safetensors model directly. You&rsquo;d think at this point I would have just made sure to pick one from this list, right?</p><p>Of course, only one of my models was supported. If it&rsquo;s not supported, you&rsquo;ll see something like below when you try to run <code>ollama create</code>. If it is supported, it will simply perform the <strong>conversion</strong> itself, and you&rsquo;ll end up with a model that is listed and runnable by Ollama.</p><div class="highlight error"><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Error: unsupported architecture &#34;Qwen3ForCausalLM&#34;
</span></span></code></pre></td></tr></table></div></div><p>And, that&rsquo;s the key word right there. <strong>Conversion</strong>. To what, exactly? A <code>.gguf</code> file! Which stands for the mouthful of syllables that is &ldquo;GPT-Generated Unified Format&rdquo;. Bottom line is, it&rsquo;s what Ollama wants.</p><h4 id=installation-1>Installation</h4><p>There are a few options for installation, as mentioned on the <a class=link href="https://github.com/ggml-org/llama.cpp?tab=readme-ov-file#quick-start" target=_blank rel=noopener><code>llama.cpp</code> repository</a>.</p><p>I didn&rsquo;t have much luck getting the homebrew version to work, so I ended up cloning the repository and building it myself (they have <a class=link href=https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md target=_blank rel=noopener>instructions for that</a>, too).</p><h4 id=converting-to-gguf>Converting to .gguf</h4><p>The bit we really care about is being able to convert the Safetensor model into a <code>.gguf</code> file. The script that achieves that is called <code>convert_hf_to_gguf.py</code>. I found it easiest to just run this from the directory that had our model in it.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>python ../llama.cpp/convert_hf_to_gguf.py model --outfile model.gguf
</span></span></code></pre></td></tr></table></div></div><p>A keen eye might also notice a script called <code>convert_lora_to_gguf.py</code> sitting next to <code>convert_hf_to_gguf.py</code>. I had no success in trying to use this, but the suggestion is that we could actually convert the <em>adapter</em> itself to a <code>.gguf</code> and pass that to Ollama via a Modelfile&rsquo;s <code>ADAPTER</code> instruction, saving us the need to fuse the adapter into the model.</p><p>Anyway, once we have our <code>model.gguf</code> file, we can now create a model in Ollama that uses it.</p><h3 id=ollama-create>ollama create</h3><p>We&rsquo;ve already seen how to create a model for Ollama, but now we can use the <code>.gguf</code> file we created above. We do this by creating a <code>Modelfile</code> in the same directory as the <code>.gguf</code> file, and referencing our brand spanking new <code>model.gguf</code>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>FROM ./model.gguf
</span></span></code></pre></td></tr></table></div></div><p>The command to create our model doesn&rsquo;t change, so we run that. With a more appropriate name.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>ollama create eight-in-a-row -f ./Modelfile
</span></span></code></pre></td></tr></table></div></div><p><code>ollama list</code> should now show us our model, and we can use it as we would any other model we&rsquo;ve pulled down from Ollama&rsquo;s library, showing up in both GitHub Copilot and Open WebUI.</p><h4 id=chat-template>Chat template</h4><p>The first big problem I noticed came when I tried to run this. And frustratingly, it <em>only</em> happened when I tried to run it via Ollama.</p><p>I was getting some really fun output, where a simple prompt of &ldquo;Hello&rdquo; would return a response along the lines of &ldquo;, I&rsquo;m going to&mldr;&rdquo; and on and on. Noticing the little comma sneaking in there at the start of the response, it took some pain and searching around before I learned all about stop sequences.</p><p>It was only after I spotted someone else showing the output of their model (via <code>ollama show</code>) that I noticed a difference between theirs and mine. Theirs had the <code>TEMPLATE</code> instruction filled in for their Modelfile. I don&rsquo;t know why it hadn&rsquo;t dawned on me earlier, but the example <code>TEMPLATE</code> instruction in the <a class=link href=https://ollama.readthedocs.io/en/modelfile/#template target=_blank rel=noopener>Modelfile reference</a> looked a <em>lot</em> like the <code>template</code> file <a class=link href=https://ollama.com/library/qwen3:1.7b/blobs/ae370d884f10 target=_blank rel=noopener>in the Ollama library</a>.</p><p>Pasting the contents of that file directly into the <code>TEMPLATE</code> instruction of my Modelfile, I was able to get the model to respond as expected. After much pain and suffering, I finally had it working!</p><h4 id=parameters>Parameters</h4><p>While I&rsquo;m not entirely sure how much of a difference it makes, I did also note that running <code>ollama show</code> against the original model had a few parameters set. I&rsquo;m not entirely sure if they were completely necessary, nor what impact they each have — I largely noticed them when I was looking to try and explicitly set my stop sequences via <code>PARAMETER stop &lt;|im_end|></code>.</p><p>Anyhow, I copied the parameter values from the original model&rsquo;s <a class=link href=https://ollama.com/library/qwen3:1.7b/blobs/cff3f395ef37 target=_blank rel=noopener>Ollama library page</a>. Omitting the <code>stop</code> parameters didn&rsquo;t seem to have any impact, but I figured that starting with the remaining values set to those of the original model probably wouldn&rsquo;t be the worst idea.</p><h2 id=results>Results</h2><p>To give everyone a level playing field, I decided to scrap all the models I&rsquo;d been playing with, and really compare these properly. I settled on a consistent set of parameters, and ran the same prompt against each of the models, with the same datasets. I was seeing enough similarity between <strong>LoRA</strong> and <strong>DoRA</strong> training that I didn&rsquo;t feel the need to run both of them against each model, so I decided to just run <strong>Full</strong> and <strong>DoRA</strong> training for each model.</p><p>Each of the models would receive the same prompt, with the response pasted in, giving each model up to three attempts and picking the best response. I&rsquo;ll indicate if this was the case in the results below.</p><p>Here are the datasets used, in the <strong>messages</strong> format:</p><ul><li><a class=link href=files/train.jsonl>train.jsonl</a></li><li><a class=link href=files/valid.jsonl>valid.jsonl</a></li></ul><p>As for the actual parameters, I ran each with <strong>200 iterations</strong> and a <strong>batch size of 1</strong>. For <strong>DoRA</strong> training, I also limited it to training on <strong>4 layers</strong>. This was very focused around reducing the memory usage, as to date I had watched a few training processes crash due to running out of memory. Later on, I do look at the impact of changing the number of layers.</p><h3 id=the-prompt>The prompt</h3><p>The prompt used will be <strong><em>&ldquo;How big is a rugby league field?&rdquo;</em></strong>.</p><p>For reference, the answer is <strong>68m wide</strong>, and <strong>112-122m long</strong>.</p><h3 id=per-model-results>Per-model results</h3><p>In an effort to keep the table headers short, they have been abbreviated. I&rsquo;ll also omit any reasoning returned from the Qwen 3 models, as it was generally quite long and honestly what we <em>really</em> care about is the end result.</p><ul><li><strong>Type</strong>: The type of training used, as described above</li><li><strong>Format</strong>: The format of the dataset used for training</li><li><strong>P%</strong>: The percentage of total model parameters trained</li><li><strong>TLoss</strong>: Training loss reported by MLX</li><li><strong>VLoss</strong>: Validation loss reported by MLX</li><li><strong>Mem</strong>: Peak memory used while training the model</li></ul><h4 id=qwen-3--17b>Qwen 3 — 1.7B</h4><div class=table-wrapper><table><thead><tr><th>Type</th><th>P%</th><th>TLoss</th><th>VLoss</th><th>Mem</th><th>Response</th></tr></thead><tbody><tr><td>Base</td><td>-</td><td>-</td><td>-</td><td>-</td><td><em>[Truncated]</em> A rugby league field is a rectangular area measuring 90 meters (300 feet) in length and 50 meters (164 feet) in width. The field is divided into two halves by a 22-meter (72-foot) line running parallel to the goal line, which separates the two halves. <em>[&mldr;]</em></td></tr><tr><td><strong>Full</strong></td><td><strong>47%</strong></td><td><strong>0.109</strong></td><td><strong>0.315</strong></td><td><strong>9.2GB</strong></td><td><strong>The width of a rugby league field is 68 meters.</strong></td></tr><tr><td>DoRA</td><td>0.014%</td><td>1.448</td><td>1.487</td><td>3.8GB</td><td>The pitch of a rugby league field is approximately 100 meters long.</td></tr></tbody></table></div><h4 id=qwen-3--4b>Qwen 3 — 4B</h4><div class=table-wrapper><table><thead><tr><th>Type</th><th>P%</th><th>TLoss</th><th>VLoss</th><th>Mem (GB)</th><th>Response</th></tr></thead><tbody><tr><td><strong>Base</strong></td><td>-</td><td>-</td><td>-</td><td>-</td><td><em>[Truncated]</em> A rugby league field is a rectangular playing area with the following standard dimensions: Length: 100 meters (approximately 109.36 yards); <strong>Width: 68 meters</strong> (approximately 74.37 yards) <em>[&mldr;]</em></td></tr><tr><td>Full*</td><td>40.147%</td><td>0.158</td><td>0.289</td><td>16.5GB</td><td>The dimensions of a rugby league field are between 122 and 182 meters.</td></tr><tr><td>DoRA</td><td>0.009%</td><td>1.055</td><td>1.108</td><td>8.5GB</td><td>A rugby league field is 100 meters long and 60 meters wide. The pitch is 100 meters long, and the width is 60 meters. The goal posts are 10 meters apart, and the crossbar is 4 meters wide.</td></tr></tbody></table></div><p>* I had to limit iterations to 100 for the 3B model, as it was running out of memory</p><h4 id=qwen-3--8b>Qwen 3 — 8B</h4><div class=table-wrapper><table><thead><tr><th>Type</th><th>P%</th><th>TLoss</th><th>VLoss</th><th>Mem (GB)</th><th>Response</th></tr></thead><tbody><tr><td>Base</td><td>-</td><td>-</td><td>-</td><td>-</td><td><em>[Truncated]</em> A rugby league field has specific dimensions that are standardized for competition. Here&rsquo;s a concise breakdown: Standard Dimensions: Length: 100 meters (328 feet); Width: 53 meters (174 feet) <em>[&mldr;]</em></td></tr><tr><td>Full*</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>DoRA**</td><td>0.005%</td><td>1.185</td><td>1.212</td><td>17GB</td><td><em>[Truncated]</em> The pitch dimensions for rugby league are standardized. Here&rsquo;s the breakdown: <strong>Width: 68 meters</strong>; Length: 126 meters <em>[&mldr;]</em></td></tr></tbody></table></div><p>* Unsurprisingly, this one was never going to fit into the meager 24GB of memory I have on this machine</p><p>** While I just managed to get the full fine-tuning in, I had to quantize the model to be able to run it on my machine</p><h4 id=llama-32--1b>Llama 3.2 — 1B</h4><div class=table-wrapper><table><thead><tr><th>Type</th><th>P%</th><th>TLoss</th><th>VLoss</th><th>Mem</th><th>Response</th></tr></thead><tbody><tr><td>Base</td><td>-</td><td>-</td><td>-</td><td>-</td><td><em>[Truncated]</em> Rugby League fields are typically smaller than American football or Australian Rules football fields. <em>[&mldr;]</em> Keep in mind that different countries or regions might have slightly varying field sizes, but 100m x 70m is the standard for rugby league fields worldwide.</td></tr><tr><td><strong>Full</strong></td><td><strong>78.745%</strong></td><td><strong>0.0077</strong></td><td><strong>0.178</strong></td><td><strong>9.7GB</strong></td><td><strong>The total length of a rugby league field is between 112 and 122 meters.</strong></td></tr><tr><td>DoRA</td><td>0.018%</td><td>0.871</td><td>0.942</td><td>2.8GB</td><td>The size of a rugby league field is typically 130-150 yards (120-137 meters) long. The width can vary, but the most common width is around 55-65 yards (50-59 meters).</td></tr></tbody></table></div><h4 id=llama-32--3b>Llama 3.2 — 3B</h4><div class=table-wrapper><table><thead><tr><th>Type</th><th>P%</th><th>TLoss</th><th>VLoss</th><th>Mem (GB)</th><th>Response</th></tr></thead><tbody><tr><td>Base</td><td>-</td><td>-</td><td>-</td><td>-</td><td>A rugby league field, also known as a rugby pitch or oval, measures 100 meters (328 feet) long and 70 meters (230 feet) wide. The field is oval in shape, with the goalposts at each end of the field, and the try lines marking the boundaries of the playing area.</td></tr><tr><td><strong>Full*</strong></td><td><strong>50.135%</strong></td><td><strong>0.096</strong></td><td><strong>0.193</strong></td><td><strong>15GB</strong></td><td><strong>The size of a rugby league field is between 112 and 122 meters.</strong></td></tr><tr><td>DoRA</td><td>0.011%</td><td>0.836</td><td>0.911</td><td>7GB</td><td>A rugby league field is 102 meters long and 68 meters wide. The goalposts stand at each end, 9 meters tall.</td></tr></tbody></table></div><p>* I had to limit iterations to 100 for the 3B model, as it was running out of memory</p><h3 id=summary>Summary</h3><p>I was a bit surprised by the results, with the larger Qwen model not performing quite as well as the smaller one. Perhaps this has something to do with the overall lower number of parameters that the smaller model is working with, resulting in a heavier weighting towards the new values?</p><p>The LLama3.2 model was the only one to nail the response across both sizes, which tracks with the general feeling being that it punches above it&rsquo;s weight. I&rsquo;m not sure that this will translate into writing code where the reasoning capability of Qwen is touted to give it the edge — but I&rsquo;m not quite sure at which model size that benefit really kicks in.</p><p>The larger of the two Llama3.2 models was clearly the winner in terms of balancing the cost of training with the quality of the results. While it had the wrong length for the DoRA-trained model, it got the width right, and both sizes regurgitated the correct dimensions when undergoing full training.</p><p>All this while I was considering whether I had just given the models a garbage dataset, but I decided to forge ahead with some investigation into the number of layers the DoRA training was impacting. Perhaps I could get the balance right between partial and full fine-tuning?</p><h3 id=number-of-layers>Number of layers</h3><p>With Llama3.2&rsquo;s 3B parameter model impressing, I decided to run the experiment with that model (also because it doesn&rsquo;t require the additional conversion step that Qwen does). We can reference the existing results for the four-layer run, and I decided to creep up the layers incrementally.</p><p>I also decided to keep the <strong>Full</strong> result here as the benchmark response.</p><div class=table-wrapper><table><thead><tr><th>Layers</th><th>P%</th><th>TLoss</th><th>VLoss</th><th>Mem (GB)</th><th>Response</th></tr></thead><tbody><tr><td><strong>Full*</strong></td><td><strong>50.135%</strong></td><td><strong>0.096</strong></td><td><strong>0.193</strong></td><td><strong>15GB</strong></td><td><strong>The size of a rugby league field is between 112 and 122 meters.</strong></td></tr><tr><td>4</td><td>0.011%</td><td>0.836</td><td>0.911</td><td>7GB</td><td>A rugby league field is 102 meters long and 68 meters wide. The goalposts stand at each end, 9 meters tall.</td></tr><tr><td>8</td><td>0.021%</td><td>0.506</td><td>0.571</td><td>7GB</td><td>The size of a rugby league field is 112 meters long and 68 meters wide.</td></tr><tr><td>16</td><td>0.043%</td><td>0.318</td><td>0.421</td><td>7GB</td><td>A rugby league field is between 112 and 122 meters long.</td></tr><tr><td>24</td><td>0.064%</td><td>0.295</td><td>0.394</td><td>7GB</td><td>The length of a rugby league field is between 112 and 122 meters, while the width is between 68 and 72 meters.</td></tr></tbody></table></div><p>I played around with a few of the other parameters, including all 28 layers with 500 iterations, but the results were all pretty similar to the 24-layer run — although I noticed that at this point the responses started getting much more terse, closer in length to the training data even when the prompt wasn&rsquo;t related to that data at all.</p><p>I was fairly impressed at this point. A 3B parameter model is certainly on the smaller end, and the dataset I&rsquo;d given it was likely not very good, but the responses were consistently coming back with reasonably correct dimensions. It was still tripping over a few things, such as not being able to return the correct depth of the in-goal area.</p><p>I decided to offer Qwen a chance at redemption, and ran the same experiment with the 4B model. I was curious to see if it would see similar consistency with a layer number of layers being trained.</p><div class=table-wrapper><table><thead><tr><th>Layers</th><th>P%</th><th>TLoss</th><th>VLoss</th><th>Mem (GB)</th><th>Response</th></tr></thead><tbody><tr><td>4</td><td>0.009%</td><td>1.055</td><td>1.108</td><td>8.5GB</td><td>A rugby league field is 100 meters long and 60 meters wide. The pitch is 100 meters long, and the width is 60 meters. The goal posts are 10 meters apart, and the crossbar is 4 meters wide.</td></tr><tr><td>24</td><td>0.052%</td><td>0.401</td><td>0.594</td><td>9GB</td><td>The pitch is 100 meters long and 68 meters wide.</td></tr></tbody></table></div><p>Hmm, still a bit off. It was spitting out a bunch of relevant info in the thinking output that was clearly from the dataset, but it even with follow-up prompts it couldn&rsquo;t figure out the depth of the in-goal area, nor would it factor that into the total length.</p><h2 id=conclusion>Conclusion</h2><p>Evidently, I underestimated just how much importance is placed on the dataset itself. I did try throwing some tweaked datasets at the problem, but ran into similar issues. This feels very much like a skill that needs to be honed before I&rsquo;ll truly unlock the potential of fine-tuning.</p><p>In any case, I was pleased to see some success with the smaller models, and I think that the results are promising enough to keep banging away at the problem. I can definitely see the potential for this be used to create a bunch of smaller models that can answer questions about specific domains, so long as the datasets are of reasonable enough quality.</p><p>I was far more impressed by the results of the Llama 3.2 model, although while I was first toying with fine-tuning, I did note that Qwen 3 was consistently better when it came to writing code.</p><p>Based purely on the results posted in here however, it&rsquo;s clear that the <strong>Llama 3.2</strong> model is a great first stop when looking to do any fine-tuning, specifically when fine-tuning <strong>8-16 layers</strong>.</p><h2 id=next-steps>Next steps</h2><p>The next steps for me are following on from a seed that was planted in one of the earliest videos I watched on this topic.</p><p>In the video, the creator formats his responses as <code>&lt;calculator>...&lt;/calculator></code>. The suggestion was, unless I&rsquo;m hallucinating after spending so long buried in AI, that this model could effectively be trained to recognise a maths question and return a response that can hand off the calculation to an actual calculator.</p><div class=video-wrapper><iframe loading=lazy src=https://www.youtube.com/embed/yOcUCnLgvt8 allowfullscreen title="YouTube Video"></iframe></div><p>Which got me thinking&mldr; what if instead of a calculator or other tool, we could have a model that called other models?</p><p>Essentially, having a small orchestrator model that can interpret what language, framework, domain etc. the prompt is regarding, and then call the appropriate models to handle each specific task?</p><p>Whether or not that&rsquo;s even a feasible idea is another story entirely, but it certainly feels like learning how to fine-tune a model has only increased my curiosity rather than quenched it.</p></section><footer class=article-footer><section class=article-tags><a href=/dev-blog/tags/ollama/>Ollama</a>
<a href=/dev-blog/tags/qwen/>Qwen</a>
<a href=/dev-blog/tags/llama/>Llama</a>
<a href=/dev-blog/tags/github-copilot/>GitHub Copilot</a>
<a href=/dev-blog/tags/open-webui/>Open WebUI</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Aug 20, 2025 08:11 +1000</span></section></footer></article><div class="article-list--compact links"><article><a href=https://ollama.com/ target=_blank rel=noopener><div class=article-details><h2 class=article-title>Ollama</h2><footer class=article-time>Get up and running with large language models</footer></div></a></article><article><a href=https://huggingface.co/ target=_blank rel=noopener><div class=article-details><h2 class=article-title>Hugging Face</h2><footer class=article-time>The AI community building the future</footer></div></a></article><article><a href=https://github.com/ml-explore/mlx-lm target=_blank rel=noopener><div class=article-details><h2 class=article-title>mlx-lm</h2><footer class=article-time>A Python package for generating text and fine-tuning LLMs on Apple silicon with MLX</footer></div></a></article><article><a href=https://github.com/ggml-org/llama.cpp target=_blank rel=noopener><div class=article-details><h2 class=article-title>llama.cpp</h2><footer class=article-time>An open source software library that performs inference on various LLMs</footer></div></a></article></div><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/dev-blog/p/ollama-local-agent/><div class=article-image><img src=/dev-blog/p/ollama-local-agent/image.60e6144c99615cd0fba35f45855ab0a9_hu_191ce23a862fc012.png width=250 height=150 loading=lazy alt="Featured image of post Running AI models locally with Ollama" data-key=ollama-local-agent data-hash="md5-YOYUTJlhXND7o19FhVqwqQ=="></div><div class=article-details><h2 class=article-title>Running AI models locally with Ollama</h2></div></a></article><article class=has-image><a href=/dev-blog/p/cursor-vibe-check/><div class=article-image><img src=/dev-blog/p/cursor-vibe-check/cover.f7cf2a26036d53501b99af2133950a5f_hu_1d3a73dc4b58b6b9.jpg width=250 height=150 loading=lazy alt="Featured image of post Cursor: Are the vibes really worth it?" data-key=cursor-vibe-check data-hash="md5-988qJgNtU1Abma8hM5UKXw=="></div><div class=article-details><h2 class=article-title>Cursor: Are the vibes really worth it?</h2></div></a></article><article class=has-image><a href=/dev-blog/p/deploying-ios-app-to-testflight/><div class=article-image><img src=/dev-blog/p/deploying-ios-app-to-testflight/image.73182ce8e3f7a3580c79b0541c9d3a97_hu_e1de359b0d2e4f59.jpg width=250 height=150 loading=lazy alt="Featured image of post Deploying an iOS app to TestFlight" data-key=deploying-ios-app-to-testflight data-hash="md5-cxgs6OP3o1gMebBUHJ06lw=="></div><div class=article-details><h2 class=article-title>Deploying an iOS app to TestFlight</h2></div></a></article><article class=has-image><a href=/dev-blog/p/react-three-fiber/><div class=article-image><img src=/dev-blog/p/react-three-fiber/image.e467405cfad2c92a64d3a8a9ae313a03_hu_2c7c47c037923ac7.png width=250 height=150 loading=lazy alt="Featured image of post React Three Fiber" data-key=react-three-fiber data-hash="md5-5GdAXPrSySpk06iprjE6Aw=="></div><div class=article-details><h2 class=article-title>React Three Fiber</h2></div></a></article><article class=has-image><a href=/dev-blog/p/less-sass-with-postcss/><div class=article-image><img src=/dev-blog/p/less-sass-with-postcss/cover.330cca0b380f974718c0c04ecb68b100_hu_d78ae7fd53e8af05.jpg width=250 height=150 loading=lazy alt="Featured image of post PostCSS: Styling without the Sass" data-key=less-sass-with-postcss data-hash="md5-MwzKCzgPl0cYwMBOy2ixAA=="></div><div class=article-details><h2 class=article-title>PostCSS: Styling without the Sass</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2024 -
2025 Isaac Dedini</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.27.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/dev-blog/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>