[{"content":"What was that interesting thing I did last week? I\u0026rsquo;ve always leaned towards the idea of writing blog content when it comes to sharing knowledge. It\u0026rsquo;s low-friction, and I can get a lot of info onto a page.\nAt least, I like the idea of writing blog content.\nLooking back over my career to date, there\u0026rsquo;s been a consistent trend with the way I try to write about my work. I can never come up with a good, blog-worthy topic!\nThe same story keeps repeating itself. I\u0026rsquo;ll do a thing, run into problems, solve them, and come out the other side with some learnings and just\u0026hellip; pick up the next task.\nAnd then, weeks later, I\u0026rsquo;ll mention it to someone and they\u0026rsquo;ll respond with something along the lines of \u0026ldquo;that\u0026rsquo;s interesting, I\u0026rsquo;d love to hear more about that thing that you\u0026rsquo;ve half-forgotten about\u0026rdquo;.\nAs my work has become increasingly funnelled through Claude, the notion struck me that Claude could probably keep track of the things I was doing better than I could–potentially even flagging work that I\u0026rsquo;d done that might be interesting to others, or picking up on potentially reusable techniques or patterns.\nSkill: /journal I landed on the creation of a /journal skill that Claude could invoke (or could be triggered manually) to add an entry to a journal file. There were a few considerations to take into account:\nAppend-only — entries are only added to a single markdown file, to avoid losing previous entries Structured but flexible — each entry has a timestamp and a project, but Claude has latitude in what it includes Context is retained — all the relevant context required for a blog-worthy topic would be captured Non-blocking — the journal runs as a background task so it doesn\u0026rsquo;t interrupt the flow of work What does it do? The skill prompts Claude to reflect on the recent work and produce an entry covering:\nWhat was done — a concise summary of the work completed Decisions made — any non-obvious choices and the reasoning behind them Notable observations — things that surprised me, patterns worth remembering, or potential blog topics Reusable patterns — techniques or approaches that could be extracted into skills, hooks, libraries etc. I did also get it to pop in some Obsidian-style tags, just to try and make it easier to quickly collate journal entries or blog-worthy topics.\nA gentle nudge To make this work passively, I added an instruction to my project\u0026rsquo;s CLAUDE.md:\n~/.claude/CLAUDE.md When you complete a significant piece of work, make a notable decision, solve an interesting problem, or identify something blog-worthy or reusable, use `/journal` as a background task to record it. Don\u0026#39;t interrupt flow — spawn it in the background and continue working. This means Claude will proactively journal without me having to remember to ask.\nI didn\u0026rsquo;t actually see much success in it actually spawning it as a background task, but it ran quickly enough and I started to get used to the frequent \u0026ldquo;hey, I want to pop this in the journal\u0026rdquo;. It was nice to know it was working, and it served as a reminder that I could go back and check the journal if I needed to.\nProject mapping To allow for mapping entries to projects, I needed to maintain this mapping somewhere. Fortunately I already had something in place due to some work that I was also doing to help drive some cross-project ChunkHound search capabilities.\nWhile the ChunkHound work I\u0026rsquo;d done was populating this file on my behalf whenever a new project was indexed, even manually creating the file below would allow for journal entries to be mapped to specific projects rather than all be tagged under #general.\n~/.claude/skills/projects.json 1 2 3 4 5 6 7 8 { \u0026#34;project1\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;path/to/project1\u0026#34; }, \u0026#34;project2\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;path/to/project2\u0026#34; } } Skill definition And of course, the meaty part–the skill definition itself!\nOne gotcha that I ran into that still has some caveats is when running a session that spans multiple days. Initially it was just taking the session start date, but adding a proper date lookup means that if it\u0026rsquo;s running autonomously, the entries should end up under the right day.\nRunning the skill manually after multiple days\u0026rsquo; work would lump them all into the day it was run, meaning the entries more accurately reflects the time a journal entry was written rather than when the work actually happened.\n~/.claude/skills/journal/SKILL.md --- name: journal description: Append a journal entry summarising recent work, decisions, and notable observations argument-hint: \u0026#34;[optional focus or note]\u0026#34; allowed-tools: Bash, Read, Write, Glob --- # Journal Append a structured journal entry to today\u0026#39;s daily file. This should be fast — do not read existing journal files or attempt to deduplicate. ## Step 1: Determine the date and file **IMPORTANT:** Do not rely on your own sense of the current date — it may be stale in long-running sessions. Always run `date +%Y-%m-%d` and `date +%H:%M` via Bash to get the actual current date and time. The journal lives in `~/.claude/journal/`, one file per day, named `YYYY-MM-DD.md`. If today\u0026#39;s file doesn\u0026#39;t exist, create it with a heading: ```markdown # YYYY-MM-DD ``` ## Step 2: Resolve project context Determine the current project by matching the working directory against `~/.claude/skills/projects.json`. If not inside a registered project, use \u0026#34;general\u0026#34; as the project name. ## Step 3: Write the entry Append an entry to the daily file. Each entry should follow this structure: ```markdown ## HH:MM — {project name} #{project name} {Summary of what was done — keep it concise but capture the key points} **Decisions:** {any notable decisions made and why, or \u0026#34;None\u0026#34;} **Topics:** {flag anything blog-worthy, reusable, or worth revisiting — or \u0026#34;None\u0026#34;} ``` Tag the project name in the heading (e.g., `#project1`, `#general`). When flagging topics, prefix with `#blog-worthy`, `#reusable`, or `#demo-worthy`: Guidelines: - Summarise what was accomplished, not every step taken - Capture the \u0026#34;why\u0026#34; behind decisions — this is what you\u0026#39;ll forget - Flag blog-worthy topics with a brief note on why it\u0026#39;s interesting - Flag reusable patterns, utilities, or approaches worth extracting - Flag demo-worthy work — things that would make a good presentation, show-and-tell, or live walkthrough - If the user provided `$ARGUMENTS`, use it to focus or annotate the entry - Keep entries concise — a few lines per section, not paragraphs ## Step 4: Write detail files (when topics are flagged) When a topic is flagged as blog-worthy or reusable, create a supporting detail file that captures the context needed to act on it later. Without this, the journal flags opportunities but loses the detail needed to follow through. Detail files live in `~/.claude/journal/details/` and are named `YYYY-MM-DD-{slug}.md`. Link them from the journal entry: ```markdown **Topics:** - #blog-worthy Teaching Claude to reflect — [detail](details/2026-02-13-reflect-workflow.md) - #reusable The skill/hook pattern — [detail](details/2026-02-13-starter-kit.md) ``` Each detail file should include whichever of the following are relevant: - **Context** — what problem was being solved and why - **Approach** — what was tried, including dead ends and alternatives considered - **Key code** — relevant snippets, patterns, or configurations that were created - **Outcome** — what worked, what didn\u0026#39;t, and why - **Blog angle** — if blog-worthy, what makes it interesting to write about - **Extraction notes** — if reusable, what could be extracted and how it might be generalised These files are meant to preserve enough context that someone (including a future Claude session) could flesh out a blog post or extract reusable code without the original conversation. What the output looks like The resulting entries were split over two aspects. The first was a file-per-day journal structure filled with simple entries, grouped under timestamps and project headings. The other was the tracking of detail pages, where context needed to be retained.\nJournal entries # 2026-02-14 ## 09:59 — general #general Built a comprehensive Claude Code customisation layer: global skills and CLAUDE.md instructions. - Created `/journal` and `/standup` skills for daily work journalling and standup summaries - Added continuous improvement and autonomous journalling instructions to CLAUDE.md - Added two-tier journalling (entries + detail files) with Obsidian-friendly `#tags` - Fixed date staleness in long sessions by shelling out to `date` command - Added `#demo-worthy` as a topic tag alongside `#blog-worthy` and `#reusable` **Decisions:** - Two-tier journal (entries + detail files) — entries stay scannable, detail preserves context - Obsidian-style `#tags` for filtering — zero lock-in, just text elsewhere - Journal skill shells out to `date` — prevents stale dates in long-running sessions - Autonomous journalling wording made explicit with concrete examples and \u0026#34;err on the side of journalling too much\u0026#34; **Topics:** - #blog-worthy The self-improving assistant loop — skills, hooks, /reflect, journalling as a meta-workflow — [detail](details/2026-02-14-self-improving-assistant.md) - #reusable The skill/hook/CLAUDE.md pattern as a transferable Claude Code starter kit — [detail](details/2026-02-14-claude-code-starter-kit.md) - #demo-worthy The full workflow end-to-end: `/search` → work → `/journal` → `/standup` → `/reflect` cycle Each entry ends up as a timestamped section in a markdown file. After a few days of use, the journal becomes a rich log of activity.\nThe only downside of course is that it only tracks what happened via Claude, requiring the mental mapping to any manual work done on the codebase. I expect that as my maturity with the tooling grows, I\u0026rsquo;ll see a growing amount of my work processed through Claude, and these logs should naturally become richer over time.\nDetails / context Under that Topics section, each entry has a link. Those links are to co-located details pages, where the relevant context is captured from the conversation history. This surprised me as to how well it worked, honestly, and it\u0026rsquo;s where I went from curious to excited about the potential for this to truly enrich my workflow.\nA little too verbose to paste one of the larger examples, the contents were well-structured and covered all the bases I\u0026rsquo;d have expected them to. The details for potential blog posts even went as far as providing an angle for the blog post.\n# Semantic code search with Claude Code and ChunkHound ## Context Claude Code defaults to grep/glob for code search — pattern-based, requiring you to know what you\u0026#39;re looking for. Goal: wire in ChunkHound semantic search so Claude finds code by intent using natural language. ## Approach **Architecture:** - Embeddings: local LM Studio (`text-embedding-bge-base-en-v1.5`) — free, fast, private - Search: ChunkHound CLI `--semantic` against per-project DuckDB indexes - Research synthesis: `claude-code-cli` provider — routes LLM through Claude - Index freshness: `Stop` hook runs background reindex after every response Initially tried local model for research synthesis too, but it ran out of context. Hybrid approach (local embeddings + Claude for LLM) was the fix. **Skills:** `/search`, `/research`, `/reindex` **Configuration:** Two JSON files in `~/.claude/skills/` — `chunkhound-config.json` and `projects.json`. Skills read at invocation. Adding a project = editing one file. **Auto-reindex:** `Stop` hook → shell script → background `chunkhound index`. Chose `Stop` over `PostToolUse` (once per turn vs per-file) and `SessionEnd` (stale during session). ## Outcome Claude defaults to semantic search via CLAUDE.md instruction. Zero-maintenance via auto-reindex. ## Blog angle Local-first — embeddings on your hardware, no API costs, code stays on your network. Draft post at `dev-blog/content/post/semantic-code-search/index.md`. ## Extraction notes - Config pattern (external JSON read at invocation) cleaner than hardcoding - Project registry + cwd resolution reused across multiple skills and the hook - `Stop` hook for background maintenance is generic While I haven\u0026rsquo;t yet tried to do anything with the #reusable or #demo-worthy content, the #blog-worthy details make it a very short jump to go from journal entries to draft blog posts.\nSkill: /standup With the journal entries populating nicely, I didn\u0026rsquo;t really have a way to prove any tangible benefit to tracking them without writing a blog post or using them in some way.\nI decided it might be worth the addition of another skill which could make use of them–in this case, a skill that would synthesise a brief summary for a span of days.\nSkill definition Once again I put Claude to work and it spat out something even more robust than I had expected, deciding to include a project filter.\n~/.claude/skills/standup/SKILL.md --- name: standup description: Summarise recent journal entries as a standup update argument-hint: \u0026#34;[days back, default 1] [project name]\u0026#34; allowed-tools: Bash, Read, Glob --- # Standup Summarise recent journal entries. ## Steps ### Step 1: Determine the range Parse `$ARGUMENTS` for: - A number (days to look back, default 1) - A project name (filter to that project only, default all) Examples: - `/standup` — yesterday\u0026#39;s entries, all projects - `/standup 3` — last 3 days, all projects - `/standup project1` — yesterday\u0026#39;s entries for project1 - `/standup 7 project2` — last 7 days for project2 ### Step 2: Read journal files Read the daily files from `~/.claude/journal/` for the date range. Files are named `YYYY-MM-DD.md`. ### Step 3: Present the summary Summarise the entries concisely, grouped by day and project. Highlight: - What was accomplished - Key decisions made - Any flagged blog topics or reusable IP - Anything left in progress Keep it brief — this is a standup, not a report. Output Here\u0026rsquo;s the output for /standup last 2 days.\nFeb 15 — project1 - Ported the Unity client to Godot 4.x (device-ID auth, main menu) - Built a background glow pulse shader, iterated with debug heatmaps to tune luminance thresholds - Built a step-based test harness with Playwright-style traces, CLI composition, and a /godot-test skill (9 files). Smoke tests passing. Feb 14 — general - Built out the Claude Code customisation layer: /search, /research, /commit, /reflect, /journal, /standup skills - Added Stop hook for ChunkHound reindexing, switched research LLM to claude-code-cli - Drafted blog posts on semantic code search and teaching Claude to reflect - Added two-tier journalling (entries + detail files) with Obsidian-friendly tags Flagged topics: - #blog-worthy Shader debug heatmaps workflow - #blog-worthy Self-improving assistant loop - #blog-worthy Local-first semantic code search - #demo-worthy Unity-to-Godot port side-by-side - #reusable Godot step-based test runner, device-ID auth pattern, global skill + project opt-in pattern, Claude Code starter kit The ordering could leave a little to be desired–another run displayed the dates in reverse. Overall though, a very handy way to capture the work that was done, and to recap on any blog-worthy content or reusable IP!\nNext steps I was pleasantly surprised at how well this worked, even in the short span of time I\u0026rsquo;ve been playing with it. Considering it was the first time I really started to look at skills it was a testament to how easy they are to set up.\nThe pattern extraction is still speculative at this point, but capturing the context and the fact that there was even a thing potentially worth extracting is such a huge leap from my current \u0026ldquo;do the thing and forget about it\u0026rdquo;. I\u0026rsquo;m looking forward to trying to progress this onto the next step of actually using that information to build the reusable assets etc.\nThe most notable thing I\u0026rsquo;ve found myself lacking is the absence of screenshots. In one case, I went through an iterative process of visually debugging some changes I had Claude making to a shader in Godot, using a temporary heatmap drawn over the image I was applying the shader to, which drastically helped me work with Claude to get the result I was after. By the time it was solved, I\u0026rsquo;d lost any opportunity to take in-progress screenshots that would have been incredibly valuable for a blog post.\nNow to go full circle and find a way to get Claude to start prompting me to proactively capture media if the current work feels like it might reach a #blog-worthy level!\n","date":"2026-02-17T00:00:00+10:00","image":"https://vivecuervo7.github.io/dev-blog/p/journalling-with-claude/cover_hu_cdfb079425a6a7f5.jpg","permalink":"https://vivecuervo7.github.io/dev-blog/p/journalling-with-claude/","title":"Automated journalling for AI-assisted development"},{"content":"A little over a week ago, I started toying with running Large Language Models (LLMs) locally using Ollama.\nApparently, all that did was to get my head spinning with a few ideas. Largely, I was finding myself incredibly curious about the potential of running some of those smaller LLMs locally, and having each of them specialised in a specific domain.\nFor context, I should probably clarify how I tend to find myself using LLMs.\nThe use case I did have a bit of a play with genuine vibe coding, but quickly found that it wasn\u0026rsquo;t for me. I like understanding my codebase, and while certainly impressive that it could churn out functional code it a matter of minutes, I found it more often than not generated too much. I found the sheer amount of code difficult to meaningfully review, and more often than not found myself blowing away all the changes even after a good half hour or so of prompting.\nAI has still found a home in my workflow however, but I find my use case to be mostly based around \u0026ldquo;I want this very specific thing, and I want it done roughly like this\u0026rdquo;. At a high level my workflow looks more like this:\nCreating a database migration Manually create the file, add to context Prompt: Add a migration that adds a users table with id, name, and email columns Creating the model / entity file Manually create the file, add to context Add the migration file we created earlier to context Prompt: Create an entity that reflects this newly created table \u0026hellip;and so on.\nThere\u0026rsquo;s a bit of manual work to say \u0026ldquo;this is where I want you to put the code\u0026rdquo;, and then letting AI run away with the code that needs to be added to that specific file. Which means, usually I want it to specifically write SQL, or add a new SvelteKit endpoint, or maybe append a route to a .NET controller.\nI\u0026rsquo;m not really leveraging the capabilities of a model that is generally knowledgeable in all these things, all at once.\nThe pain point Honestly, this approach has been working for me. It very quickly gives me a chunk of code that is small enough to either quickly tick off or tweak to my liking, and then I can move on and not need to think about that file again.\nThe problem I have consistently run into however, is that the training for most (all?) of these LLMs was done prior to the release of Svelte 5. And Svelte 5 brought significant changes to the syntax. As one can imagine, this amounted to a lot of generated code that was just\u0026hellip; wrong.\nThe litmus test Given that very clear and resounding pain point, I settled on one specific thing I wanted to achieve — something that the models I was using were just completely incapable of in their current state.\nCould I teach a model how to write in Svelte 5\u0026rsquo;s syntax?\nWith my thoughts already occupied with this idea of having a handful of specialised, smaller LLMs, I figured this would be the perfect test.\nThe potential solutions There were a few different ways I could go about trying to solve this problem. To date, I hadn\u0026rsquo;t really leaned too heavily on GitHub Copilot\u0026rsquo;s instructions files, but I figured it would be the first stop.\nWhile that seemed a sane approach, I was very conscious of the fact that using smaller LLMs, I probably needed to be careful with context lengths. While this was likely less of a concern given my short-lived interactions with the LLM, it still felt like a sub-par solution.\nEnter a couple of terms that I had seen bandied about, but really at this point hadn\u0026rsquo;t understood. Namely, system prompts, Retrieval-Augmented Generation (RAG) and fine-tuning.\nThe reference material After spending half a day trying to get an LLM to scrape some meaningful data off the Svelte documentation website, I discovered that the Svelte website actually has a page specific to LLMs.\nI also discovered that this llms.txt file is a proposed standard, and there\u0026rsquo;s a handy directory of products and companies that have adopted it.\nAwesome! What really got me interested, however, was the presence of some text files that had the complete documentation, including compressed versions for smaller LLMs.\n/llms.txt — a listing of the available files /llms-full.txt (~1 MB) — complete documentation for Svelte, SvelteKit and the CLI /llms-medium.txt (~0.5 MB) — compressed documentation for use with medium context windows /llms-small.txt (45 KB) — highly compressed documentation for use with smaller context windows GitHub Copilot instructions I won\u0026rsquo;t go too deep on this one. The short version is that, following the documentation, I was able to essentially paste the contents of the llms-*.txt file into copilot-instructions.md. I actually got some really positive results with this approach. Certainly, once plugged into a fully-scaffolded project, it was able to generate some fairly accurate code.\nSurprisingly however, I got far better results with the smaller llms-small.txt file, which was only 45 KB in size. I figure that this was likely due to the limited context window of the smaller models, although truthfully I didn\u0026rsquo;t really know what to expect if I exceeded that — assuming this was even the case.\nI definitely considered this to be a huge win, and honestly I could have likely stopped here.\nIn the spirit of full disclosure, I did run this with GPT and Claude as well, as the local models don\u0026rsquo;t seem to be capable of actually generating files etc. Claude was unsurprisingly by far the standout here, but not without its problems. I\u0026rsquo;ll summarise the experience with each of them below.\nGenerally speaking however, one pleasant experience was that I no longer needed to specify which framework the component needed to be written for. I used a very simple prompt of \u0026ldquo;Create a counter component\u0026rdquo;. These were all run via GitHub Copilot, with the scaffolded project loaded into VS Code.\nModel Notes Llama3.2 3B This one pretty much flopped. The code it spat out didn\u0026rsquo;t use Svelte 5 syntax, and didn\u0026rsquo;t appear to even use legacy reactive statements. I\u0026rsquo;m not being too critical of it at this point however, as it\u0026rsquo;s easily the smallest model used. Qwen 3 8B I\u0026rsquo;ve honestly found Qwen to be a little hit and miss in GitHub Copilot specifically, often getting caught up in it\u0026rsquo;s reasoning, getting halfway through a \u0026lt;think\u0026gt; block and just\u0026hellip; stopping. That said, the one time it actually generated the code I wanted, it was spot on and told me to put in in the correct place. GPT-4.1 Created a very simple counter component, but put it in the wrong place. Additionally, it was initially created with botched \u0026lt;script\u0026gt; tags, and when it finished trying to fix them they were just gone — resulting in code that wouldn\u0026rsquo;t even compile. Claude Sonnet 4 I guess someone had to show off, and that someone was Claude. By a long way — but not necessarily in a good way. Claude checked the project structure, then created the component at the right location. All the correct syntax was used, even cross-referencing other components to confirm. But, in typical Claude fashion, the component was a big 240-line block of code complete with styling and all of the functionality that I didn\u0026rsquo;t want. I decided to push Claude a bit further here and managed to vibe code my way to a full-blown storefront for a shoe store. I was actually pretty surprised at how easily I could follow along this time — but truth be told, I have been working towards a very succinct stack which meant there was just less code to review. Styling however did get a bit messy, and there were a lot of follow-up prompts to try and get it Claude to keep that manageable.\nAnd really, that latter point is one of the main reasons why I didn\u0026rsquo;t want to just stop here. If I were to just continue with this pattern, I was going to be spending all my time fighting against Claude, trying to keep it in check.\nAdditionally, was I going to just keep needing to add more and more documentation to the instructions? Was there even an llms-*.txt file out there for Tailwind? How do I provide this same information to Open WebUI?\nRetrieval-Augmented Generation (RAG) I\u0026rsquo;m going to be completely honest here — I\u0026rsquo;m not entirely sure how this is supposed to work in the context of both GitHub Copilot and Open WebUI, especially when we\u0026rsquo;re talking about having a highly-specialised model.\nMy end-goal here was to have a single, unified experience that would be consistent across both GitHub Copilot and Open WebUI. While conceptually speaking RAG isn\u0026rsquo;t overly complex, the best I could really find here was to create a knowledge base in Open WebUI, and have it reference the knowledge base itself when generating for a prompt.\nOpen WebUI also allows us to create new models that have a system prompt, as well as a constant reference to a knowledge base.\nThis actually worked really well, honestly. I wasn\u0026rsquo;t sure that this was conceptually any different to GitHub Copilot\u0026rsquo;s instructions, but it certainly did a far better job of just doing the thing I wanted it to do. Maybe GitHub Copilot was just getting in the way? Anyhow, it felt like the \u0026ldquo;other side of the coin\u0026rdquo; to GitHub Copilot\u0026rsquo;s instructions, albeit a little shinier, despite not being plugged into my code editor.\nI should also note that when I tried to split up this file and provide it with a larger number of smaller files, it often struggled to find the right files and would start returning plainly incorrect responses. As with GitHub Copilot instructions, this just didn\u0026rsquo;t offer a portable, consistent experience across different interfaces.\nIn the context of GitHub Copilot specifically, I had to keep telling it to look up the documentation, and even then it often just decided to do things it\u0026rsquo;s own way.\nSystem prompt So, feeling like I\u0026rsquo;d gotten somewhere with the two approaches above, I really wanted to try and consolidate this into a single, consistent model that both GitHub Copilot and Open WebUI could use.\nEnter Ollama\u0026rsquo;s Modelfiles. I touched on these briefly while first looking into running models locally, but essentially they provide a way for me to create a completely new model based on an existing model, with some additional tweaks for things such as parameters, system prompts and templates. The Modelfile reference can be found here.\nConsidering the success I\u0026rsquo;d had with the two earlier approaches, I figured that what I really needed was to just have a model that always had this in context, right? That\u0026rsquo;s effectively what was happening with the two separate approaches — although one was being very explicit in telling GitHub Copilot to always consider the instructions, and the other was giving Open WebUI access to the file and hoping that it always referenced it.\nSo, it seemed to make sense that I could just whack the contents of llms-small.txt into the system prompt of a new model, and then let both GitHub Copilot and Open WebUI use it directly, with no additional context required.\n1 2 FROM qwen3:8b SYSTEM {contents of llms-small.txt} Specify the model, and specify the system prompt, which was just a dump of the whole text file. Running the command below then created the new model. Piece of cake!\n1 ollama create svelte-system-prompt -f ./Modelfile Surprisingly, this just didn\u0026rsquo;t quite work as well as I\u0026rsquo;d hoped. Not knowing too much about how GitHub Copilot\u0026rsquo;s instructions or Open WebUI\u0026rsquo;s RAG worked in terms of how much weight was given to the additional context, I figured that this might just have been applied a little differently.\nIn any case, this was a bit of a flop altogether.\nFine-tuning All of my reading up until this point had gradually leaned towards this eventuality. With my previous endeavours failing — or least not quite hitting the mark — I decided to look into what was actually required to fine-tune a model.\nI\u0026rsquo;ll try to touch on the various tools etc. in order here. This isn\u0026rsquo;t quite how it panned out in practice, but it should provide a good overview of what\u0026rsquo;s involved.\nAdditionally, I decided to move away from trying to train it on Svelte here. I did give it a good crack at first, with varying levels of success. Ultimately I was left unsure as to whether the dataset I had created was actually any good, or whether the models I was using were just too small. Different training methods added another variable into the mix, and on top of that, with Qwen 3 being a reasoning model I made a bit of a mess trying to insert reasoning data into the training dataset.\nAnyway, I decided to train it on a much more focused topic — the dimensions and markings of a rugby league field.\nI grabbed a PDF from here, and used that as the basis for my training dataset.\nDatasets Of course, the easiest way to create a dataset for training an LLM, was to use an LLM. I tried getting a couple of models to scrape the PDF, with ChatGPT being the quickest way to generate a large file.\nMeta\u0026rsquo;s Synthetic Data Kit I stumbled across Meta\u0026rsquo;s Synthetic Data Kit which is purpose built for creating these datasets far too late in the piece, however I found that I wasn\u0026rsquo;t able to get a meaningful dataset anyhow. It simply required a model that was too large to run on my machine.\nI won\u0026rsquo;t go into details on how to run this, but it looks like an effective tool for slurping up large amounts of data and spitting out a usable dataset.\nIt just might need either a beefy setup, or using a rented workstation from services like Vast.ai.\nFormats The file itself needs to be a jsonl file, which is essentially a file with a JSON object per line. The file I ended up with used the messages format, and looked like this (multiplied by many, many rows):\n1 2 3 4 5 6 7 8 9 10 11 12 { \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Summarize the dimensions of a rugby league field.\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;A rugby league field is 68 metres wide, and 112-122 metres long.\u0026#34; } ] } Now, a couple of things to note here.\nThere are actually a few ways to format the dataset. Primarily though, and as might be evident simply by looking at that example, the dataset is essentially a collection of objects that describe a conversation.\nThe other types are a little simpler, but I initially opted for the more conversational messages format. It is however worth noting that this format requires specific models — or rather, the particular models need to be tuned appropriately. The base llama models for example did not work out of the box, and required me to use the instruct tuned versions.\nIn addition to the messages format above, there is also the completions format:\n1 2 3 4 { \u0026#34;prompt\u0026#34;: \u0026#34;Summarize the dimensions of a rugby league field.\u0026#34;, \u0026#34;completion\u0026#34;: \u0026#34;A rugby league field is 68 metres wide, and 112-122 metres long.\u0026#34; } And the text format:\n1 2 3 { \u0026#34;text\u0026#34;: \u0026#34;A rugby league field is 68 metres wide, and 112-122 metres long.\u0026#34; } Files required Now, training data on its own is all well and good, but we also need verification data. Since we\u0026rsquo;re using mlx-lm for this (sorry, Windows folks — this one is Apple only, but there are options that should work just as well on Windows), we\u0026rsquo;ll need the following files.\ntrain.jsonl valid.jsonl The valid data should be smaller than the training data itself — I\u0026rsquo;ve seen a few recommendations, but I just opted for around 10-20% of the training data size. A test.jsonl file is also recommended, which can be used to evaluate the model after training.\nmlx-lm Using mlx-lm was actually one of the easier parts to get right. The setup did require using Python, which is always fun considering how infrequently I use it, but once I jogged the memory on how to set up a virtual environment, we had the ball rolling.\nBefore we get into the individual commands used, it\u0026rsquo;s worth mentioning that mlx-lm can pull models directly from Hugging Face, meaning we don\u0026rsquo;t need to download models, or figure out where they might be stored. It does however mean that the names might looks a little different to what we\u0026rsquo;re used to seeing with Ollama, but rest assured they\u0026rsquo;re all talking about the same models.\nInstallation Running pip install mlx-lm gets us the basic package, and we can actually start playing with models right off the bat with mlx_lm.generate --prompt \u0026quot;Hello\u0026quot;. Starting a continuous chat can be kicked off with mlx_lm.chat.\nI\u0026rsquo;m not really sure which model gets used when you don\u0026rsquo;t supply the argument, but providing a --model argument will use the specified model. As mentioned, these should can be repositories on Hugging Face, so to grab the Qwen3 1.7B model as an example, we simply need to run mlx_lm.generate --model qwen/qwen3-1.7b --prompt \u0026quot;Hello\u0026quot;.\nTraining Now that we\u0026rsquo;re done playing with our toys, it\u0026rsquo;s time to do our best Sid from Toy Story impersonation and start messing with the guts of our models.\nIt\u0026rsquo;s probably a good time to talk about what type of training this is. Or, types. And full disclaimer, this is where I started to get a little lost — suffice to say that I\u0026rsquo;m still not entirely sure how much of a difference there is between the different types of training that mlx-lm offers beyond a very rough idea.\nType Description Full Updates all of the model weights (or parameters) of the pre-trained model. Very resource intensive, and risks \u0026ldquo;over-fitting\u0026rdquo;, causing a model to \u0026ldquo;forget\u0026rdquo; some of its original data. LoRA Low-Rank Adaptation. We track the changes we want to make to the weights. We can also freeze some of the layers to reduce the number of parameters we\u0026rsquo;re adjusting. Far more efficient while fine-tuning. DoRA Weight-Decomposed Low-Rank Adaptation. Too complicated for me to understand the differences, but the consensus seems to be that it it provides more accurate results than LoRA with similar efficiency gains. More information here. QLoRA is also available, which is simply LoRA that works on quantized models. mlx-lm will automatically use this if our --model argument points to a quantized model.\nNow, this video does a great job of explaining a bunch of things that are well and truly over my head — it\u0026rsquo;s definitely worth a watch if you\u0026rsquo;re interested in the details.\nI tried both full and LoRA training to figure out which was best for my use case. Considering that I wanted a highly specialized model, I wasn\u0026rsquo;t sure if over-fitting would necessarily be the biggest concern.\nIn any case, the command to run full fine-tuning is as follows:\n1 2 3 4 5 mlx_lm.lora --train \\ --model qwen/qwen3-1.7b \\ --data data \\ --iters 200 \\ --fine-tune-type full We can easily switch that last argument to lora (or dora) to use the other types of training. This introduces some additional arguments we can pass, but we can leave them at their defaults for now.\nIt is worth mentioning here that this is a resource-intensive task, and more than once I found myself running out of memory and watching the training process crash. This page details a few methods to try and reduce the memory usage, but I found that the biggest impact was simply to use a smaller model — bearing in mind that this will also reduce the quality of the final model.\nRun the command with your desired combination of parameters, and we should end up with a folder called adapters. Inside, is the result of our fine-tuning in the form of a Safetensor adapter!\nThe console output should also look something like the below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Loading pretrained model Fetching 9 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00\u0026lt;00:00, 13217.34it/s] Loading datasets Training Trainable parameters: 0.056% (0.967M/1720.575M) Starting training..., iters: 100 Calculating loss...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:11\u0026lt;00:00, 2.22it/s] Iter 1: Val loss 5.646, Val took 11.289s Iter 10: Train loss 4.538, Learning Rate 1.000e-05, It/sec 1.376, Tokens/sec 317.131, Trained Tokens 2304, Peak mem 4.695 GB Iter 20: Train loss 2.579, Learning Rate 1.000e-05, It/sec 1.281, Tokens/sec 303.100, Trained Tokens 4671, Peak mem 5.034 GB Iter 30: Train loss 1.801, Learning Rate 1.000e-05, It/sec 1.545, Tokens/sec 314.796, Trained Tokens 6708, Peak mem 5.034 GB Iter 40: Train loss 1.522, Learning Rate 1.000e-05, It/sec 1.508, Tokens/sec 336.666, Trained Tokens 8941, Peak mem 5.034 GB Iter 50: Train loss 1.425, Learning Rate 1.000e-05, It/sec 1.514, Tokens/sec 313.347, Trained Tokens 11010, Peak mem 5.034 GB Iter 60: Train loss 1.261, Learning Rate 1.000e-05, It/sec 1.577, Tokens/sec 341.066, Trained Tokens 13173, Peak mem 5.034 GB Iter 70: Train loss 1.162, Learning Rate 1.000e-05, It/sec 1.348, Tokens/sec 318.090, Trained Tokens 15532, Peak mem 5.034 GB Iter 80: Train loss 1.168, Learning Rate 1.000e-05, It/sec 1.243, Tokens/sec 324.718, Trained Tokens 18144, Peak mem 5.429 GB Iter 90: Train loss 1.077, Learning Rate 1.000e-05, It/sec 1.415, Tokens/sec 318.347, Trained Tokens 20394, Peak mem 5.429 GB Calculating loss...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:11\u0026lt;00:00, 2.18it/s] Iter 100: Val loss 1.241, Val took 11.401s Iter 100: Train loss 0.746, Learning Rate 1.000e-05, It/sec 1.623, Tokens/sec 339.464, Trained Tokens 22486, Peak mem 5.429 GB Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors. Saved final weights to adapters/adapters.safetensors. Lots of useful information, but critically we want to keep an eye on the Train / Val loss values (training and validation loss, respectively). Typically, the lower the better, these essentially indicate how well the model is learning.\nMy understanding is that we want the validation loss to be close to the the training loss. If the validation loss is significantly lower than the training loss, it indicates under-fitting (lower accuracy relative to the training data), while a significantly higher validation loss indicates over-fitting (higher accuracy relative to the training data, at the cost of existing knowledge).\nFusing Honestly, I prefer to refer to this as \u0026ldquo;baking\u0026rdquo; the adapter in. Apparently the community is dead-set on calling it \u0026ldquo;fusing\u0026rdquo;. That just reminds me of an over-protected childhood where I wasn\u0026rsquo;t allowed to watch Dragonball Z.\nMoving on, this step isn\u0026rsquo;t necessarily required, depending on the model we\u0026rsquo;ve used. I haven\u0026rsquo;t tried this with one of the compatible models, but Ollama\u0026rsquo;s Modelfile reference does mention the ability to simply reference a Safetensor adapter, which is what we get when we run the mlx_lm.lora --train command above.\nSince neither of the models I was using were compatible, I did need to produce a fused model. We can actually run the model we used to create the adapter with or without the adapter attached to it (add an --adapter-path argument to a mlx_lm.generate command to use the adapter), but this wasn\u0026rsquo;t going to give me a model that could be run by Ollama.\nTo combine the model and adapter, we run the following.\n1 2 3 mlx_lm.fuse --model qwen/qwen3-1.7b \\ --adapter-path adapters \\ --save-path ./model And, once again we\u0026rsquo;ll get a folder called model, which contains a bunch of files. This is your model!\nIf we\u0026rsquo;re itching to see how it works, we can pass --model ./model to mlx_lm.generate or mlx_lm.chat, and it will use the newly created model. Good for a quick turnaround for testing.\nllama.cpp Once again, this may be unnecessary based on your model. The Modelfile reference also mentions being able to build from a Safetensors model directly. You\u0026rsquo;d think at this point I would have just made sure to pick one from this list, right?\nOf course, only one of my models was supported. If it\u0026rsquo;s not supported, you\u0026rsquo;ll see something like below when you try to run ollama create. If it is supported, it will simply perform the conversion itself, and you\u0026rsquo;ll end up with a model that is listed and runnable by Ollama.\n1 Error: unsupported architecture \u0026#34;Qwen3ForCausalLM\u0026#34; And, that\u0026rsquo;s the key word right there. Conversion. To what, exactly? A .gguf file! Which stands for the mouthful of syllables that is \u0026ldquo;GPT-Generated Unified Format\u0026rdquo;. Bottom line is, it\u0026rsquo;s what Ollama wants.\nInstallation There are a few options for installation, as mentioned on the llama.cpp repository.\nI didn\u0026rsquo;t have much luck getting the homebrew version to work, so I ended up cloning the repository and building it myself (they have instructions for that, too).\nConverting to .gguf The bit we really care about is being able to convert the Safetensor model into a .gguf file. The script that achieves that is called convert_hf_to_gguf.py. I found it easiest to just run this from the directory that had our model in it.\n1 python ../llama.cpp/convert_hf_to_gguf.py model --outfile model.gguf A keen eye might also notice a script called convert_lora_to_gguf.py sitting next to convert_hf_to_gguf.py. I had no success in trying to use this, but the suggestion is that we could actually convert the adapter itself to a .gguf and pass that to Ollama via a Modelfile\u0026rsquo;s ADAPTER instruction, saving us the need to fuse the adapter into the model.\nAnyway, once we have our model.gguf file, we can now create a model in Ollama that uses it.\nollama create We\u0026rsquo;ve already seen how to create a model for Ollama, but now we can use the .gguf file we created above. We do this by creating a Modelfile in the same directory as the .gguf file, and referencing our brand spanking new model.gguf.\n1 FROM ./model.gguf The command to create our model doesn\u0026rsquo;t change, so we run that. With a more appropriate name.\n1 ollama create eight-in-a-row -f ./Modelfile ollama list should now show us our model, and we can use it as we would any other model we\u0026rsquo;ve pulled down from Ollama\u0026rsquo;s library, showing up in both GitHub Copilot and Open WebUI.\nChat template The first big problem I noticed came when I tried to run this. And frustratingly, it only happened when I tried to run it via Ollama.\nI was getting some really fun output, where a simple prompt of \u0026ldquo;Hello\u0026rdquo; would return a response along the lines of \u0026ldquo;, I\u0026rsquo;m going to\u0026hellip;\u0026rdquo; and on and on. Noticing the little comma sneaking in there at the start of the response, it took some pain and searching around before I learned all about stop sequences.\nIt was only after I spotted someone else showing the output of their model (via ollama show) that I noticed a difference between theirs and mine. Theirs had the TEMPLATE instruction filled in for their Modelfile. I don\u0026rsquo;t know why it hadn\u0026rsquo;t dawned on me earlier, but the example TEMPLATE instruction in the Modelfile reference looked a lot like the template file in the Ollama library.\nPasting the contents of that file directly into the TEMPLATE instruction of my Modelfile, I was able to get the model to respond as expected. After much pain and suffering, I finally had it working!\nParameters While I\u0026rsquo;m not entirely sure how much of a difference it makes, I did also note that running ollama show against the original model had a few parameters set. I\u0026rsquo;m not entirely sure if they were completely necessary, nor what impact they each have — I largely noticed them when I was looking to try and explicitly set my stop sequences via PARAMETER stop \u0026lt;|im_end|\u0026gt;.\nAnyhow, I copied the parameter values from the original model\u0026rsquo;s Ollama library page. Omitting the stop parameters didn\u0026rsquo;t seem to have any impact, but I figured that starting with the remaining values set to those of the original model probably wouldn\u0026rsquo;t be the worst idea.\nResults To give everyone a level playing field, I decided to scrap all the models I\u0026rsquo;d been playing with, and really compare these properly. I settled on a consistent set of parameters, and ran the same prompt against each of the models, with the same datasets. I was seeing enough similarity between LoRA and DoRA training that I didn\u0026rsquo;t feel the need to run both of them against each model, so I decided to just run Full and DoRA training for each model.\nEach of the models would receive the same prompt, with the response pasted in, giving each model up to three attempts and picking the best response. I\u0026rsquo;ll indicate if this was the case in the results below.\nHere are the datasets used, in the messages format:\ntrain.jsonl valid.jsonl As for the actual parameters, I ran each with 200 iterations and a batch size of 1. For DoRA training, I also limited it to training on 4 layers. This was very focused around reducing the memory usage, as to date I had watched a few training processes crash due to running out of memory. Later on, I do look at the impact of changing the number of layers.\nThe prompt The prompt used will be \u0026ldquo;How big is a rugby league field?\u0026rdquo;.\nFor reference, the answer is 68m wide, and 112-122m long.\nPer-model results In an effort to keep the table headers short, they have been abbreviated. I\u0026rsquo;ll also omit any reasoning returned from the Qwen 3 models, as it was generally quite long and honestly what we really care about is the end result.\nType: The type of training used, as described above Format: The format of the dataset used for training P%: The percentage of total model parameters trained TLoss: Training loss reported by MLX VLoss: Validation loss reported by MLX Mem: Peak memory used while training the model Qwen 3 — 1.7B Type P% TLoss VLoss Mem Response Base - - - - [Truncated] A rugby league field is a rectangular area measuring 90 meters (300 feet) in length and 50 meters (164 feet) in width. The field is divided into two halves by a 22-meter (72-foot) line running parallel to the goal line, which separates the two halves. [\u0026hellip;] Full 47% 0.109 0.315 9.2GB The width of a rugby league field is 68 meters. DoRA 0.014% 1.448 1.487 3.8GB The pitch of a rugby league field is approximately 100 meters long. Qwen 3 — 4B Type P% TLoss VLoss Mem (GB) Response Base - - - - [Truncated] A rugby league field is a rectangular playing area with the following standard dimensions: Length: 100 meters (approximately 109.36 yards); Width: 68 meters (approximately 74.37 yards) [\u0026hellip;] Full* 40.147% 0.158 0.289 16.5GB The dimensions of a rugby league field are between 122 and 182 meters. DoRA 0.009% 1.055 1.108 8.5GB A rugby league field is 100 meters long and 60 meters wide. The pitch is 100 meters long, and the width is 60 meters. The goal posts are 10 meters apart, and the crossbar is 4 meters wide. * I had to limit iterations to 100 for the 3B model, as it was running out of memory\nQwen 3 — 8B Type P% TLoss VLoss Mem (GB) Response Base - - - - [Truncated] A rugby league field has specific dimensions that are standardized for competition. Here\u0026rsquo;s a concise breakdown: Standard Dimensions: Length: 100 meters (328 feet); Width: 53 meters (174 feet) [\u0026hellip;] Full* - - - - - DoRA** 0.005% 1.185 1.212 17GB [Truncated] The pitch dimensions for rugby league are standardized. Here\u0026rsquo;s the breakdown: Width: 68 meters; Length: 126 meters [\u0026hellip;] * Unsurprisingly, this one was never going to fit into the meager 24GB of memory I have on this machine\n** While I just managed to get the full fine-tuning in, I had to quantize the model to be able to run it on my machine\nLlama 3.2 — 1B Type P% TLoss VLoss Mem Response Base - - - - [Truncated] Rugby League fields are typically smaller than American football or Australian Rules football fields. [\u0026hellip;] Keep in mind that different countries or regions might have slightly varying field sizes, but 100m x 70m is the standard for rugby league fields worldwide. Full 78.745% 0.0077 0.178 9.7GB The total length of a rugby league field is between 112 and 122 meters. DoRA 0.018% 0.871 0.942 2.8GB The size of a rugby league field is typically 130-150 yards (120-137 meters) long. The width can vary, but the most common width is around 55-65 yards (50-59 meters). Llama 3.2 — 3B Type P% TLoss VLoss Mem (GB) Response Base - - - - A rugby league field, also known as a rugby pitch or oval, measures 100 meters (328 feet) long and 70 meters (230 feet) wide. The field is oval in shape, with the goalposts at each end of the field, and the try lines marking the boundaries of the playing area. Full* 50.135% 0.096 0.193 15GB The size of a rugby league field is between 112 and 122 meters. DoRA 0.011% 0.836 0.911 7GB A rugby league field is 102 meters long and 68 meters wide. The goalposts stand at each end, 9 meters tall. * I had to limit iterations to 100 for the 3B model, as it was running out of memory\nSummary I was a bit surprised by the results, with the larger Qwen model not performing quite as well as the smaller one. Perhaps this has something to do with the overall lower number of parameters that the smaller model is working with, resulting in a heavier weighting towards the new values?\nThe LLama3.2 model was the only one to nail the response across both sizes, which tracks with the general feeling being that it punches above it\u0026rsquo;s weight. I\u0026rsquo;m not sure that this will translate into writing code where the reasoning capability of Qwen is touted to give it the edge — but I\u0026rsquo;m not quite sure at which model size that benefit really kicks in.\nThe larger of the two Llama3.2 models was clearly the winner in terms of balancing the cost of training with the quality of the results. While it had the wrong length for the DoRA-trained model, it got the width right, and both sizes regurgitated the correct dimensions when undergoing full training.\nAll this while I was considering whether I had just given the models a garbage dataset, but I decided to forge ahead with some investigation into the number of layers the DoRA training was impacting. Perhaps I could get the balance right between partial and full fine-tuning?\nNumber of layers With Llama3.2\u0026rsquo;s 3B parameter model impressing, I decided to run the experiment with that model (also because it doesn\u0026rsquo;t require the additional conversion step that Qwen does). We can reference the existing results for the four-layer run, and I decided to creep up the layers incrementally.\nI also decided to keep the Full result here as the benchmark response.\nLayers P% TLoss VLoss Mem (GB) Response Full* 50.135% 0.096 0.193 15GB The size of a rugby league field is between 112 and 122 meters. 4 0.011% 0.836 0.911 7GB A rugby league field is 102 meters long and 68 meters wide. The goalposts stand at each end, 9 meters tall. 8 0.021% 0.506 0.571 7GB The size of a rugby league field is 112 meters long and 68 meters wide. 16 0.043% 0.318 0.421 7GB A rugby league field is between 112 and 122 meters long. 24 0.064% 0.295 0.394 7GB The length of a rugby league field is between 112 and 122 meters, while the width is between 68 and 72 meters. I played around with a few of the other parameters, including all 28 layers with 500 iterations, but the results were all pretty similar to the 24-layer run — although I noticed that at this point the responses started getting much more terse, closer in length to the training data even when the prompt wasn\u0026rsquo;t related to that data at all.\nI was fairly impressed at this point. A 3B parameter model is certainly on the smaller end, and the dataset I\u0026rsquo;d given it was likely not very good, but the responses were consistently coming back with reasonably correct dimensions. It was still tripping over a few things, such as not being able to return the correct depth of the in-goal area.\nI decided to offer Qwen a chance at redemption, and ran the same experiment with the 4B model. I was curious to see if it would see similar consistency with a layer number of layers being trained.\nLayers P% TLoss VLoss Mem (GB) Response 4 0.009% 1.055 1.108 8.5GB A rugby league field is 100 meters long and 60 meters wide. The pitch is 100 meters long, and the width is 60 meters. The goal posts are 10 meters apart, and the crossbar is 4 meters wide. 24 0.052% 0.401 0.594 9GB The pitch is 100 meters long and 68 meters wide. Hmm, still a bit off. It was spitting out a bunch of relevant info in the thinking output that was clearly from the dataset, but it even with follow-up prompts it couldn\u0026rsquo;t figure out the depth of the in-goal area, nor would it factor that into the total length.\nConclusion Evidently, I underestimated just how much importance is placed on the dataset itself. I did try throwing some tweaked datasets at the problem, but ran into similar issues. This feels very much like a skill that needs to be honed before I\u0026rsquo;ll truly unlock the potential of fine-tuning.\nIn any case, I was pleased to see some success with the smaller models, and I think that the results are promising enough to keep banging away at the problem. I can definitely see the potential for this be used to create a bunch of smaller models that can answer questions about specific domains, so long as the datasets are of reasonable enough quality.\nI was far more impressed by the results of the Llama 3.2 model, although while I was first toying with fine-tuning, I did note that Qwen 3 was consistently better when it came to writing code.\nBased purely on the results posted in here however, it\u0026rsquo;s clear that the Llama 3.2 model is a great first stop when looking to do any fine-tuning, specifically when fine-tuning 8-16 layers.\nNext steps The next steps for me are following on from a seed that was planted in one of the earliest videos I watched on this topic.\nIn the video, the creator formats his responses as \u0026lt;calculator\u0026gt;...\u0026lt;/calculator\u0026gt;. The suggestion was, unless I\u0026rsquo;m hallucinating after spending so long buried in AI, that this model could effectively be trained to recognise a maths question and return a response that can hand off the calculation to an actual calculator.\nWhich got me thinking\u0026hellip; what if instead of a calculator or other tool, we could have a model that called other models?\nEssentially, having a small orchestrator model that can interpret what language, framework, domain etc. the prompt is regarding, and then call the appropriate models to handle each specific task?\nWhether or not that\u0026rsquo;s even a feasible idea is another story entirely, but it certainly feels like learning how to fine-tune a model has only increased my curiosity rather than quenched it.\n","date":"2025-07-31T00:00:00+10:00","image":"https://vivecuervo7.github.io/dev-blog/p/fine-tuning-an-llm/image_hu_1dc2fc96cd47ac65.png","permalink":"https://vivecuervo7.github.io/dev-blog/p/fine-tuning-an-llm/","title":"RAG to fine-tuning an LLM"},{"content":"Fine! I\u0026rsquo;ll have another look at AI. Geez.\nAt least, that\u0026rsquo;s how it felt when I begrudgingly decided to look at running some models locally.\nA few months ago, I decided to have a look at vibe coding with Cursor. Now, I\u0026rsquo;ll have to admit that I went into that experiment fairly biased despite my best efforts to keep an open mind, but I walked away not very impressed and with the notion that AI was over-hyped even further cemented in my mind.\nGiven a few months and some more poking around with GitHub Copilot, and I think I\u0026rsquo;m ready to say that while I\u0026rsquo;m still quite bearish on AI in general, it has certainly found a place in my workflow. Unfortunately, I have experienced some frustration with it — largely along the lines of running out of Claude requests and being left with GPT, which would at times just seemingly disconnect and wouldn\u0026rsquo;t return to working order until I restarted VS Code.\nThere\u0026rsquo;s also the whole privacy concern. Personally I find myself unbothered by it, but in the context of customer data I am very cautious about the implications.\nAnyway, waking up with a \u0026ldquo;what do I want to look into today\u0026rdquo; landed me on the topic of running AI models locally.\nGetting started Tip: Install with brew install ollama and then run ollama to see the available commands. No surprises that I landed here. Even as someone who has had my head half-buried in the sand when it comes to AI, I was well aware of Ollama. A bit of light searching yielded alternatives, but I decided early on that I was just going to stick with the basics here.\nInstallation was a breeze, just needing a quick brew install ollama to install it locally. Once installed, calling ollama by itself shows us the relatively simple set of commands we can pass it, but to get started there\u0026rsquo;s only a couple we really need.\nFirst of all though, to start the Ollama server run ollama serve.\nModels The basic commands we\u0026rsquo;re after here are ollama run, ollama list and ollama rm to run a model, list all models, and remove models.\nThe Ollama models page details a slew of models that we can run.\nActually installing and running a model is as simple as calling ollama run llama3.1. This will download it if it hasn\u0026rsquo;t yet been pulled, and runs it immediately. Once installed, we can start typing prompts into the terminal!\nChoosing a model I\u0026rsquo;m not going to go too in-depth here — mostly because I still don\u0026rsquo;t really understand it! I\u0026rsquo;ll give it a red-hot crack anyway.\nLooking at the list of available models, I saw a bunch of options and was completely lost as to which I needed. Looking at this heavily truncated list below, I was confused as to which I should use. Especially since there we easily 60+ variants to choose from!\nllama3.1:405b-text-fp16 llama3.1:70b-text-q3_K_S llama3.1:8b-instruct-q5_K_M Anyway, after a bit of reading I was able to break it down to the following, using llama3.1:8b-instruct-q5_K_M as a reference.\nllama3.1 — the name of the model, each may be better or worse at some things. 8b — the number of parameters the model contains. Roughly, the \u0026ldquo;size\u0026rdquo; of the model. instruct — the purpose of the model, which generally is the main function of that model. q5_K_M — the model\u0026rsquo;s quantization, or roughly, level of optimisation. The Ollama GitHub repository also provided this helpful note, providing some guidelines around system resource requirements for different model sizes.\nYou should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models. To expand slightly on quantization as that was a completely new term for me, quantization is the process of converting a model\u0026rsquo;s weights from high precision data types to lower precision types. While this generally translates to a smaller resource footprint, it comes at the cost of a potential reduction in accuracy and quality.\nModels can get even more complex than that, and I found this page does a good job of explaining the terms without going too deep. In fact, it\u0026rsquo;s just about where I pulled that little quantization explainer from.\nPersonally, I landed on the qwen2.5-coder:3b-instruct-q4_K_M and qwen2.5-coder:7b-instruct-q4_K_M models. The 3B model was nice and quick, and seemed to handle most basic tasks with ease. I did find myself frequently switching to the 7B model when the smaller one wasn\u0026rsquo;t quite doing the job.\nI also spent about half an hour looking at new PCs.\nOpen WebUI As nice as it is to be able to start running models locally with such ease, the interface leaves a little to be desired.\nOpen WebUI offers us a much nicer way to interact with our models — even allowing us to run multiple models side-by-side, which is perfect for comparing the output of various models.\nRunning the following should both pull and start the docker container. Note that this does also create a volume, so settings, chats, etc. will be persisted.\n1 docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main Performance It didn\u0026rsquo;t take long for a wry smile to grow on my face. I\u0026rsquo;ve spent long enough bragging about how well my decision to go with a MacBook Air has been, and after about a year and a half I finally ran into the thing that made me wish I had a little more grunt under the hood.\nEven some of the smallest models were slow enough to be a little annoying, and those that didn\u0026rsquo;t have smaller varieties were just slower than I would consider to be usable. Certainly, with no qualms about GPT harvesting my data, I was not about to pump the brakes on whatever thinking was going on under the hood.\nI decided to flick open my dusty old Windows laptop (in fact it\u0026rsquo;s neither dusty nor old, currently it serves as my game streaming server using Sunlight) and install Ollama there.\nWoah. Okay, now we\u0026rsquo;re running at what looked like 3-4x faster. Turns out having a GPU kinda helps.\nOllama over the network Cool. So we\u0026rsquo;ve got a fast-enough-to-be-usable machine running AI models. Unfortunately, I don\u0026rsquo;t actually do any work on that laptop — it\u0026rsquo;s pretty much dedicated to gaming* these days. That said, I\u0026rsquo;d had a taste of how the models should be running, and I couldn\u0026rsquo;t look back.\n* by which I mean, of course, the times I\u0026rsquo;d love to be playing games if life could just stop throwing me side quests\nI\u0026rsquo;m certainly no network buff, and I chose the path of least resistance. I quickly installed ngrok and exposed my Ollama endpoint (http://localhost:11434) to see if it was workable. Navigating to the endpoint that ngrok assigned for me, and I could see the very simple page that told me Ollama was running. Sweet!\nNow, I really should clean this up — since I only intend to use this while I\u0026rsquo;m sitting within arm\u0026rsquo;s reach of both laptops, I really could just serve this over my local network. For the sake of experimentation, however, that\u0026rsquo;s tomorrow\u0026rsquo;s problem.\nOpen WebUI I guess this one can come first since we\u0026rsquo;ve already talked about setting it up.\nSince I already had this up and running on my MacBook, I just needed to pop into the settings. Under the option for Connections there is a section called Manage Direct Connections. This is where we can add the URL of our Ollama server.\nWith a bit of trial and error, I found that I needed to append the ngrok-provided URL with /v1 to get it working. It should pick up the models automatically, although they can be specified explicitly if we want.\nAlternatively, the likely easier way to achieve this would be to simply specify the OLLAMA_BASE_URL while we\u0026rsquo;re creating the container, by adding -e OLLAMA_BASE_URL= to the previous command. Here\u0026rsquo;s the command again, but you\u0026rsquo;ll need to replace the URL with your own.\nNote that this approach does not require us to append the URL with v1.\n1 docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -e OLLAMA_BASE_URL=http://localhost:11434 -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main GitHub Copilot I was pretty stoked at this point, but GitHub Copilot was the big one.\nI already knew Ollama was supported — in fact, hooking it up to Copilot was the first thing I did after running my first model. Wiring it up to Ollama running on a different computer proved to be a little less obvious, and fortunately I stumbled across a relevant GitHub discussion.\nThe TL;DR for the above is that we\u0026rsquo;re looking for a setting called github.copilot.chat.byok.ollamaEndpoint. Throwing my ngrok endpoint at that setting allowed me to select the models running on my other machine.\nThe only annoyance I found with this is that I had two different sizes of the same model, and GitHub Copilot\u0026rsquo;s interface just showed the same display name for each of them. As a workaround, it looks like I could possibly use a Modelfile to give my models better names, but that opened yet another door — and a welcome one at that.\nModelfiles Basically, one of the things I\u0026rsquo;ve disliked is this idea that I have a generic AI helper that I constantly need to keep feeding cues to. I have a coding style, and part of what has been turning me away from AI is that it won\u0026rsquo;t write the code that I want it to write!\nWhile, yes, in the context of GitHub Copilot, Cursor, etc., I can just throw a set of rules at it, I disliked the notion that I would need to maintain this on a per-project basis, constantly re-synchronising them with my other projects\u0026hellip; Nope. That\u0026rsquo;s not saving me time.\nSo, I started looking into custom models. Here\u0026rsquo;s the Modelfile reference, which goes into a lot more detail than I have below.\nsvelte-assistant.Modelfile 1 2 3 FROM qwen2.5-coder:3b-instruct-q4_K_M SYSTEM You are an expert in SvelteKit, the full-stack meta-framework. You are familiar with Svelte 5 syntax. You are familiar with the Svelte documentation found here: https://svelte.dev/docs/svelte/overview and the SvelteKit documentation found here: https://svelte.dev/docs/kit/introduction. When a question or instruction appears to be targeted towards a different web framework, make a suggestion to use a specialised model. Endeavour to answer questions as quickly as possible. Omit any examples or lengthy explanations unless requested. I threw a couple of questions at it, and it answered as much as I expected it to.\n1 2 3 4 5 6 \u0026gt;\u0026gt;\u0026gt; What do you specialise in? I specialize in SvelteKit and related technologies, including Svelte 5 syntax, documentation from svelte.dev/docs/svelte/overview, and the SvelteKit documentation at svelte.dev/docs/kit/introduction. \u0026gt;\u0026gt;\u0026gt; Can you help me with a React project? For React-related questions or assistance, I suggest using a model specialized in React development. Knowing it was a Hail Mary at best, it unfortunately seemed to not be capable of looking up any information about the newer Svelte 5 syntax — which has thus far been a bit of a souring experience when it comes to using AI as a code assistant.\nAdditionally, if I asked it a more pointed question about React such as \u0026ldquo;can you show me an example React component\u0026rdquo;, it would just spit out a component instead of telling me to use a model specific to React.\nHonestly, not such a big deal, but I was kind of hoping to get a few models with a \u0026ldquo;soft\u0026rdquo; specialisation that I could feed additional context into with the hope of being able to just run a bunch of smaller, faster, targeted models that better fit the way I tend to use AI.\nAt the very least, we can tweak a few parameters and set the \u0026ldquo;tone\u0026rdquo; for the model\u0026rsquo;s responses.\nConclusion It was pretty nice being able to get up and running as easily as it turned out to be. Given my laptop was churning out responses about as quickly as the network-bound GPT or Claude, I\u0026rsquo;m probably going to stick to running this locally for the time being, preserving my capped Claude requests for the truly difficult tasks.\nI\u0026rsquo;m still curious about the potential for those smaller, targeted models, but a lot of the documentation I read about fine-tuning models just went straight over my head. I guess I\u0026rsquo;ll settle for something more within my means, and just set up my Ollama server to run over my local network instead of through ngrok.\nNow to convince my wife that I really do need that expensive PC upgrade\u0026hellip;\n","date":"2025-07-23T00:00:00+10:00","image":"https://vivecuervo7.github.io/dev-blog/p/ollama-local-agent/image_hu_4b55899719ea87dc.png","permalink":"https://vivecuervo7.github.io/dev-blog/p/ollama-local-agent/","title":"Running AI models locally with Ollama"},{"content":"Alrighty. Staring down the barrel of some upcoming work with iOS, and I needed to re-familiarise myself with all things iOS and SwiftUI.\nWith some previous, very light experience with iOS development, I was reasonably comfortable with my ability to create working functionality within an app. Not a whole lot different to the typical work I would be doing for a web app. Make UI do a thing, wire it up to a backend, and voila.\nHowever, this time I was going to need to get it out there. Or, at least as far as TestFlight. I realised I had no idea how to build the \u0026ldquo;bread\u0026rdquo; of an iOS-flavoured sandwich; all I knew was the middle.\nThe checklist Now, I should mention before I begin that this post was mostly written as a rant over the convoluted process of learning how to get an iOS app onto TestFlight. That said, I still wanted to leave behind a bit of a guide or checklist that I could refer to in the future. This is that checklist.\nProject setup\nProject in sidebar \u0026gt; Project \u0026gt; Info \u0026gt; Configurations Add entries for each environment, with both a Release and Debug version Create an *.xcconfig for each environment and link them under this section Project in sidebar \u0026gt; Target \u0026gt; Build Settings Under Packaging, set Bundle IDs for each environment (same values for Release and Dev) Under Bundle Display Name, set Bundle Display Name for each environment (same values for Release and Dev) Scheme (dropdown at top of page) \u0026gt; Manage Schemes Modify the list to reflect the environments Edit each scheme to ensure the correct Build Configurations are set App registration\nRegister app Under Identifiers, register an App ID for each environment using Explicit Bundle ID — these should match what we used for the Bundle IDs in our project setup In App Store Connect, add a new app for each environment using those Bundle IDs Certificate Create a certificate signing request, using Keychain will be easiest Create an Apple Distribution certificate via Apple Developer Resources Provisioning profiles Under Profiles, create App Store Connect provisioning profiles for each environment using the distribution certificate Download the provisioning profiles, install locally, assign to each environment under Targets \u0026gt; Signing \u0026amp; Capabilities (uncheck Automatically manage signing from each environment to manually manage provisioning profiles) App Store Connect API\nApp Store Connect \u0026gt; Users and Access \u0026gt; Integrations Add new team key, download it and take note of Issuer ID and Key ID CI/CD pipeline\nCreate ExportOptions.plist — run xcodebuild -help for guidance on options Make sure we specify the correct provisioning profile for each environment Prepare secrets, ideally using Base64 encoding Export certificate from Keychain (including private key used for the certificate signing request) Encode the provisioning profiles we downloaded for local installation Add to GitHub secrets etc., including the App Store Connect API details from above Pipeline steps: Create temporary Keychain Decode and install certificates Decode and install provisioning profiles Setup Xcode (use correct version) Run tests etc. — xcodebuild test Build app archive — xcodebuild archive Export IPA — xcodebuild -exportArchive Set up the App Store Connect API key Upload to App Store Connect — xcrun altool --upload-app Clean up temporary keychain, installed provisioning profiles etc. Project setup I decided that my foray into the world of deploying an iOS app should tick a few boxes.\nSome basic functionality Multiple environments that can be installed side-by-side Deploys via GitHub Actions with passing tests Pushes the app to TestFlight The first two were pretty straightforward. Whizzed through some of the basic tutorials and realised I had never really covered testing. Ended up with a very small, tutorial-driven app that I could both test and deploy.\nEnvironment setup Environment? Hmm, well, not entirely sure I had this one set up correctly.\nI had two targets set up, one for the app and another for the tests. From what I had uncovered with some Googling, was to be expected. I followed yet another guide on setting up multiple configurations.\nPopped in a quick APP_TITLE=Pilot (adjusted for each environment), added it to Target \u0026gt; Info \u0026gt; Custom iOS Target Properties and referenced the value in the app. A few steps to remember, but it worked.\nWait — I almost forgot. I also needed to set up the bundle IDs and display names\u0026hellip;\nHmm. And set up the schemas. More than just a few steps to remember at this point, but it seems like this is really a once-per-project task. I can stomach that. It\u0026rsquo;s the sort of thing I\u0026rsquo;ll need to find a reference for every time as well, which is at least half of the motivation for turning this into a blog post.\nApp icon This one actually came towards the end, after I had written the CI/CD pipeline. Uploading hit a hitch due to not having an app icon — a bit of an annoyance given I was simply wanting to learn how to go from local development to a TestFlight deployment.\nFortunately, the GitHub workflow had some output that told me exactly what was missing.\n1 2 The bundle does not contain an app icon for iPhone / iPod Touch of exactly \u0026#39;120x120\u0026#39; pixels... The bundle does not contain an app icon for iPad of exactly \u0026#39;152x152\u0026#39; pixels... Looks like they need to be in .png format, too. Cool, this one looked pretty easy. Additionally, the errors informed me that I\u0026rsquo;d need to use the asset catalog.\n1 A value for the Info.plist key \u0026#39;CFBundleIconName\u0026#39; is missing in the bundle \u0026#39;com.isaacdedini.pilot.dev\u0026#39;. Apps built with iOS 11 or later SDK must supply app icons in an asset catalog and must also provide a value for this Info.plist key. After spotting this line in the documentation suggesting that if I provided a 1024x1024 image, it would automatically create the appropriate variations, I quickly created an icon of that size.\nAlright! Now we\u0026rsquo;re cooking! No idea if I needed to fill all three options or not, but I didn\u0026rsquo;t want to wait until I ran the workflow to find that out.\nHeeding the error message that indicated I\u0026rsquo;d need to specify the CFBundleIconName in Info.plist, I went and added that. Confusingly, trying to add it kept changing the name of the key, until I confirmed that this was just Xcode trying to be helpful. By calling it something other than the key. Why, Apple, must you make things so confusing?\nNow, not entirely sure here but a quick search indicated that I should just drop the name of the resource from the asset catalog in there as the value. Popped in AppIcon and pushed my changes up.\nApp registration All the tutorials I found kind of hinted at where to go, and used all the words I needed to know, but nothing really gave me a \u0026ldquo;step one\u0026rdquo;. Except for one video, where they enabled a capability via Xcode seemingly to trigger some background setting up of things, and then they removed the capability. Yeah, that wasn\u0026rsquo;t going to help me understand this at all.\nAnyway, off I went to the Apple Developer Portal thinking I\u0026rsquo;d figure things out from there. I ended up going around in circles for a spell, so hopefully putting it all in order below will make the process less confusing. There was far more pain involved here than is expressed below.\nRegistering the app This one threw me at first, coming from a web background I was looking to register a single application, with the intention of having different versions available as separate environments.\nFrom what I gathered in the end is that typically we might have a single app registered, and make newer version of the app available to testers via TestFlight while we have a different version released to end users.\nThis was a little different to what I was after in individual \u0026ldquo;environments\u0026rdquo; that could be installed side-by-side. I couldn\u0026rsquo;t find a better way to achieve this outside of simply registering my environments as completely distinct apps.\nIdentifiers Tip: register explicit Bundle IDs for each environment that match those we\u0026rsquo;re using in Xcode. We\u0026rsquo;ll want to register each of the bundle IDs we used when we were setting up our project under Packaging.\nMake sure to use Explicit bundle IDs — the descriptions provided had me initially thinking I wanted to use wildcard bundle IDs, and I spent a long time trying to figure out why they weren\u0026rsquo;t showing up as options for my app registrations.\nApp Store Connect Here\u0026rsquo;s where we actually create the app!\nI found myself landing here way too early the first time around, and this really cost me a lot of time. As mentioned before, this was largely due to the fact that I\u0026rsquo;d created a wildcard bundle ID, but when it didn\u0026rsquo;t work I ended up floundering around creating certificates, provisioning profiles, registering devices etc.\nTurns out, the identifier is all we need to get to this point. The other stuff is only actually required to build and deploy our application.\nGo to App Store Connect \u0026gt; Apps. Click on the big blue plus, and fill out the dialog. Based on the info provided for the SKU field, I just popped a UUID in there. Not sure what the expectation is here, nor why a UUID wouldn\u0026rsquo;t be the default.\nWe\u0026rsquo;ll need to repeat the process for each of our environments — which still doesn\u0026rsquo;t feel right seeing three individual apps, but this is the closest I\u0026rsquo;ve come to having my three environments available.\nCertificate Tip: we\u0026rsquo;ll need an Apple Distribution certificate for deploying to App Store Connect. This is something that, from what I can gather, is something that needs only be done once. If a distribution certificate already exists, we\u0026rsquo;d just use that to create our provisioning profiles.\nCertificate Signing Request First up, we\u0026rsquo;ll need to create the Certificate Signing Request (CSR). We\u0026rsquo;ll need to export our certificate via Keychain later as well, as this contains the private key used for the CSR.\nIt is my understanding that the signing request itself is platform-agnostic, meaning this could also be done via openssl. Which is interesting to say the least — if I\u0026rsquo;ve understood this correctly it means we could feasibly get all of our app registration done via the browser, opening the door to potentially building and deploying an app without explicitly needing an Apple device.\nCreating the certificate After saving the signing request, navigate to Apple Developer Resources \u0026gt; Certificates \u0026gt; Add.\nSelect Apple Distribution, and upload the signing request.\nWhile we shouldn\u0026rsquo;t need to install the certificate on our machines for local development, on a Mac the easiest way to export our certificate for use in our CI/CD pipeline later will be to export it via Keychain. Since we\u0026rsquo;re here, it won\u0026rsquo;t hurt to just download and install this now.\nProvisioning profiles Tip: for deploying to TestFlight, an App Store Connect provisioning profile is required. This one probably gave me the most pain, largely due to the fact that I started here way too early. Trying to add the app to App Store Connect told me that it needed to be the same bundle ID that I\u0026rsquo;d used in Xcode, and over there the only thing I could change other than the bundle ID itself was the provisioning profile, so I thought that must have been the issue.\nAnyhow, between not knowing which provisioning profile to create, not having created a certificate yet, and having created the wildcard bundle ID at this point, there was a lot of back and forth, removing provisioning profiles etc. before I got this one right.\nProfile creation Navigate to Apple Developer Resources \u0026gt; Profiles and add a new profile.\nThe first thing that caught me out was which provisioning profile to use. A few of the descriptions felt like they could fit my use case, but long story short the correct option was App Store Connect.\nIt\u0026rsquo;ll also ask for an App ID and certificate. We need provisioning profiles for each of our App IDs, but the certificate should be the same one for each — the distribution certificate we created earlier.\nAssigning profiles Once they\u0026rsquo;re created, we can download the *.mobileprovision files. Double-clicking these once downloaded doesn\u0026rsquo;t provide any feedback, but they should be added to one of a couple of directories — on my machine they ended up in ~/Library/Developer/Xcode/UserData/Provisioning Profiles/. As I needed to do with the number of times I got this wrong, removing them is as simple as deleting them from this directory.\nWe can then go to our Signing \u0026amp; Capabilites for our target and assign the provisioning profiles to the relevant environments. Ignore the fact that they\u0026rsquo;re listed under \u0026ldquo;ineligible\u0026rdquo; — they were able to be added perfectly fine. Make sure we\u0026rsquo;ve also unchecked Automatically manage signing.\nCI/CD pipeline Fortunately this seemed to be more well documented. I quickly stumbled across this blog post and decided to use that as a guide.\nApp Store Connect API Tip: the App Manager role is required for uploading an app in our CI/CD pipeline. This once caught me out yet again a little when it came to implementing this in the pipeline. Once again information was inconsistent and left me wondering what the recommended way to deploy an app was. Initially I landed on using the App Store Connect API directly.\nEventually I realised that using the API directly wasn\u0026rsquo;t required, but we still needed the key we\u0026rsquo;d otherwise have created, to support using altool for deployment.\nCreating an API key was pretty straightforward. Navigate to App Store Connect \u0026gt; Users and Access \u0026gt; Integrations, and click the blue plus. There are lots of links around to help out. Very shortly, I had a key created and the three requisite secrets added to GitHub — we\u0026rsquo;ll need the Issuer ID, Key ID and the key itself.\nPreparing for export Tip: Running xcodebuild -help gives us an up-to-date list of all the available keys for our export options. The file we needed to create is ExportOptions.plist, which will tell Xcode how to export the archive. I couldn\u0026rsquo;t find anything about generating one of these without running an export locally to do\u0026hellip; things. Again, I wanted to know exactly what it was doing, and why.\nBasic structure From what I had gathered, the very basic structure we\u0026rsquo;d need is as follows. Interestingly, a generated file used the deprecated app-store for the method, which caused an error to be displayed when I finally ran the workflow.\nExportOptions.plist 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;method\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;app-store-connect\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;uploadBitcode\u0026lt;/key\u0026gt; \u0026lt;false/\u0026gt; \u0026lt;key\u0026gt;uploadSymbols\u0026lt;/key\u0026gt; \u0026lt;true/\u0026gt; \u0026lt;key\u0026gt;compileBitcode\u0026lt;/key\u0026gt; \u0026lt;false/\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt; Additional values A few additional entries into our export options are required.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; + \u0026lt;key\u0026gt;signingStyle\u0026lt;/key\u0026gt; + \u0026lt;string\u0026gt;manual\u0026lt;/string\u0026gt; + \u0026lt;key\u0026gt;provisioningProfiles\u0026lt;/key\u0026gt; + \u0026lt;key\u0026gt;signingCertificate\u0026lt;/key\u0026gt; + \u0026lt;string\u0026gt;Apple Distribution\u0026lt;/string\u0026gt; + \u0026lt;key\u0026gt;teamID\u0026lt;/key\u0026gt; + \u0026lt;string\u0026gt;CH6SJWRT2C\u0026lt;/string\u0026gt; + \u0026lt;key\u0026gt;destination\u0026lt;/key\u0026gt; + \u0026lt;string\u0026gt;export\u0026lt;/string\u0026gt; + \u0026lt;key\u0026gt;provisioningProfiles\u0026lt;/key\u0026gt; + \u0026lt;dict\u0026gt; + \u0026lt;key\u0026gt;com.isaacdedini.pilot.dev\u0026lt;/key\u0026gt; + \u0026lt;string\u0026gt;iOS TestFlight Pilot Dev\u0026lt;/string\u0026gt; + \u0026lt;key\u0026gt;com.isaacdedini.pilot.staging\u0026lt;/key\u0026gt; + \u0026lt;string\u0026gt;iOS TestFlight Pilot Staging\u0026lt;/string\u0026gt; + \u0026lt;key\u0026gt;com.isaacdedini.pilot.prod\u0026lt;/key\u0026gt; + \u0026lt;string\u0026gt;iOS TestFlight Pilot Prod\u0026lt;/string\u0026gt; + \u0026lt;/dict\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt; A quick explainer for each of these values is provided below. While a few of them indicate that the defaults should be fine, I decided to err towards being overly explicit.\nKey Value signingStyle Options: manual or automatic — this should default to whichever was used to archive the app signingCertificate We can provide an explicit name, or an automatic selector — in this case we\u0026rsquo;re using the Apple Distribution automatic selector, which will pick the newest one teamId The developer team to use for this archive — should default to the team used to archive the app destination Options: export or upload — in this case we just want a local export provisioningProfiles This is in the form of a dict, with our key value pairs being our bundle IDs and the name of the provisioning profiles. Pipeline steps Tip: Documentation for altool used for uploading the app can be found here. Without going into too much depth, the basic steps that needed to be achieved were as follows.\nCreate temporary Keychain Install iOS certificates Install provisioning profiles Setup Xcode Run tests Build archive Export IPA Copy the App Store Connect key itself to a local directory Upload IPA to App Store Connect Cleanup A complete GitHub workflow can be found here that includes the above steps, but below are some snippets of the critical test, archive, export and upload steps.\nRun tests 1 2 3 4 5 6 7 8 - name: Run tests run: | cd Pilot xcodebuild test \\ -project Pilot.xcodeproj \\ -scheme \u0026#34;Pilot Dev\u0026#34; \\ -configuration \u0026#34;Debug Dev\u0026#34; \\ -destination \u0026#34;platform=iOS Simulator,name=iPhone 16\u0026#34; Build archive 1 2 3 4 5 6 7 8 9 10 11 - name: Build archive for Dev if: matrix.environment == \u0026#39;dev\u0026#39; run: | cd Pilot mkdir -p build xcodebuild archive \\ -project Pilot.xcodeproj \\ -scheme \u0026#34;Pilot Dev\u0026#34; \\ -configuration Release \\ -archivePath \u0026#34;build/Pilot-Dev.xcarchive\u0026#34; \\ -destination \u0026#34;generic/platform=iOS\u0026#34; \\ Export IPA 1 2 3 4 5 6 7 8 - name: Export IPA for Dev if: matrix.environment == \u0026#39;dev\u0026#39; run: | cd Pilot xcodebuild -exportArchive \\ -archivePath \u0026#34;build/Pilot-Dev.xcarchive\u0026#34; \\ -exportPath \u0026#34;build/\u0026#34; \\ -exportOptionsPlist ../ExportOptions.plist Upload to App Store Connect 1 2 3 4 5 6 7 8 9 10 - name: Upload to TestFlight for Dev if: matrix.environment == \u0026#39;dev\u0026#39; run: | cd Pilot xcrun altool --upload-app \\ -f \u0026#34;build/Pilot.ipa\u0026#34; \\ -t ios \\ --apiKey \u0026#34;${{ secrets.APP_STORE_CONNECT_API_KEY_ID }}\u0026#34; \\ --apiIssuer \u0026#34;${{ secrets.APP_STORE_CONNECT_ISSUER_ID }}\u0026#34; \\ --verbose Success! Green lights! Once I checked one of the options to make App Store Connect stop complaining about \u0026ldquo;missing compliance\u0026rdquo; for the recently uploaded build, I finally had a build available through TestFlight.\nWhat a journey.\n","date":"2025-07-21T00:00:00+10:00","image":"https://vivecuervo7.github.io/dev-blog/p/deploying-ios-app-to-testflight/image_hu_dca1a50f65e742c7.jpg","permalink":"https://vivecuervo7.github.io/dev-blog/p/deploying-ios-app-to-testflight/","title":"Deploying an iOS app to TestFlight"},{"content":"Recently I had cause to revisit the 3D web landscape. Having previously used babylon.js for similar projects and having had a largely positive experience, I remained curious about how Three.js stacked up against it.\nI knew ahead of time that I was likely to land on a React app as a wrapper for any 3D content, and it was largely the presence of a couple of libraries that promised to make this a more streamlined experience — namely react-three-fiber and drei.\nAs such, this post will be leaning heavily on those, rather than looking at holding Three.js directly.\nYeah, but why? While it has been a few years since I last worked with Babylon.js, my initial memory of it was that it was big. Reviewing the landing page for it promises big things in the way of lighting, shadows, a Node Render Graph to control the whole render pipeline, right through to a fully-fledged physics engine.\nBottom line, it felt like a game engine. Which makes sense, as this is closer to what they\u0026rsquo;re targeting.\nDon\u0026rsquo;t get me wrong — if I was building a more game-like experience I would have jumped straight at Babylon.js. My needs were a bit lighter however, which is what got me looking at the more lightweight Three.js.\nAdditionally, looking at NPM trends for these two packages indicates that Three.js is wildly more popular.\nAlthough, I find that chart to be a little misleading. Babylon.js certainly isn\u0026rsquo;t lacking in popularity — both libraries have plenty of stars on GitHub, and are both kept up to date.\nThe bundle size is also worth a brief mention, although being modular this doesn\u0026rsquo;t represent the final bundle size. But to give us a more concrete idea of what I mean by Three.js being more lightweight, the minified and gzipped bundle size comes in at ~170kB, compared to Babylon.js at 1.4MB.\nWhat really sucked me however in was looking specifically at React Three Fiber and Drei. I was less swayed by the comparisons around bundle size and performance, as each seemed to have their own merits — Three.js for smaller, lightweight cases, and Babylon.js for the big guns. My use case was likely going to straddle the two, and when I find myself torn between two options in such cases, I tend towards that which offers a better developer experience.\nSimpler syntax with React Three Fiber This video does a great job of explaining what React Three Fiber (R3F) is actually doing under the hood, if you want a deeper explanation, but essentially it allows us to use a more React-like syntax when describing a Three.js scene.\nThree.js 1 2 3 4 5 6 7 const scene = new Scene(); const cube = new Mesh(); cube.geometry = new BoxGeometry(); cube.material = new MeshStandardMaterial(); scene.add(cube); Three.js will require us to explicitly create our mesh, add our geometry and materials, and then add it to the scene. From memory, this isn\u0026rsquo;t far from what we\u0026rsquo;d expect in Babylon.js either.\nDown the track when we start looking to componentise things we\u0026rsquo;re very likely to end up using classes, but realistically this is just plain old Javascript so we\u0026rsquo;re pretty free in terms of how we want to split our code up.\nR3F 1 2 3 4 \u0026lt;mesh\u0026gt; \u0026lt;boxGeometry /\u0026gt; \u0026lt;meshStandardMaterial /\u0026gt; \u0026lt;/mesh\u0026gt; Yep, straight up HTML-like markup.\nR3F essentially works by saying that any child element will be added to it\u0026rsquo;s parent with the .add() function. This is why we don\u0026rsquo;t need to explicitly call scene.add() in R3F to have the object added to the scene.\nThe slight difference to be aware of here is that if a child element has an attach property, it will instead assign that child element to a property of the parent.\nThere\u0026rsquo;s a little magic in the example above, where R3F will automatically add the attach property to any elements that end with \u0026ldquo;Geometry\u0026rdquo; or \u0026ldquo;Material\u0026rdquo;. This makes the above element equivalent to \u0026lt;boxGeometry attach=\u0026quot;geometry\u0026quot; /\u0026gt;, which assigns the boxGeometry element to the geometry property of mesh.\nMore importantly than the syntax changes themselves however, is that it means when we come to componentisation we can simply create standard React components.\nDrei Drei provides a bunch of useful helpers and abstractions. I couldn\u0026rsquo;t really imagine using R3F without also bringing Drei to the table — although I did find it a little constraining in some cases, largely around cameras and camera controls.\nThat said, I don\u0026rsquo;t know if this was specifically Drei, or it was more of an underlying problem.\nPerformance Interestingly, the only time I noticed any drop in performance was when some AI-generated slop (yeah, I wanted to quickly throw some decent functionality at Three.js to make sure it could do what I wanted) caused excessive React renders.\nFrom everything I\u0026rsquo;ve read online, Three.js should be capable to better performance by merit of being more lightweight and offering tooling a little closer to the metal than Babylon.js. That said, the consensus seemed to be that Babylon.js is going to provide more stable performance in larger or more complex scenes.\nBabylon.js does seem to prefer ease-of-use, with documentation covering how to optimise a scene.\nUltimately, this wasn\u0026rsquo;t going to be a deal-breaker for me, but it was interesting to note the difference in strengths between the two.\nDocumentation Now, this one took me by surprise. Three.js has been around for a bit longer, so I naturally assumed the documentation would be a strong point. Much to my surprise, the documentation wasn\u0026rsquo;t lacking per se, but felt hard to consume. Perhaps it was due to using a few libraries, but I found myself constantly bouncing between Three.js docs, R3F docs, and Drei docs.\nBabylon.js on the other hand has fantastic documentation. Where hunting for answers to problems in Three.js took me down the path of forum posts and tutorials, the Babylon.js documentation was all that was needed for the most part.\nInspector In a major point for Babylon.js, Three.js did not appear to have anything even remotely approaching the usefulness of Babylon.js\u0026rsquo; Inspector. I don\u0026rsquo;t recall using this extensively those few years ago, but it was useful on a couple of occasions. Certainly something I\u0026rsquo;d rather have and not need.\nThree.js on the other hand did not offer anything similar. The closest I stumbled upon was a reference to some libraries that could help to display frame rates and draw calls etc. The easiest way to plug anything in was to use Drei\u0026rsquo;s \u0026lt;Stats /\u0026gt; component.\nXR support This one came as no surprise, but Babylon.js won out here. This was probably the moment where I started wondering whether Babylon.js might still be my preferred option — it was just one of those things that I didn\u0026rsquo;t want to have to mess around with to get working.\nThat said, it wasn\u0026rsquo;t particularly difficult to get VR/AR working with Three.js, but the first thing I noticed was a lack of typical VR controls such as being able to teleport around a scene.\nReact integration Relying quite heavily on some memories eroded by time, I have some vague recollection of integrating Babylon.js with the React wrapper to be less than ideal. Workable, but it always felt like we were trying to bridge over the two worlds.\nThis is really the big reason why Three.js and more specifically R3F jumped out at me. The React integration is really solid here, and makes it feel very easy to wrap React around a 3D experience. For a use case where the React-based UI would form a large part of the application, this felt like a solid argument for going with R3F.\nOn-demand rendering Ok, this one was pretty cool.\nThree.js runs as you would expect a typical game or scene to run, where there is a game loop that causes constant re-renders. This is exactly what we want if we have a scene with constantly moving parts, but as was the case with what I needed, things very much could come to rest in between interactions.\nR3F offers us the option for on-demand rendering, which allows us to set the canvas\u0026rsquo; frameloop prop, which means we will render frames whenever props are changed throughout the component tree. Adding this and watching the \u0026lt;Stats /\u0026gt; panel showed that the FPS counter stopped dead in it\u0026rsquo;s tracks whenever I wasn\u0026rsquo;t directly interacting with the scene, and there was no discernable impact to the expected behaviour of the app once I started using it.\nYay for battery life, I guess?\nConcurrency Without going into too much detail, R3F can lean on React\u0026rsquo;s useTransition to defer heavy operations to maintain a stable framerate in demanding situations — something that vanilla Three.js cannot achieve.\nMore info here.\nState management While it was really nice being able to let state flow through into the 3D parts of the application, it did mean that we needed to be keenly aware of how the different render cycles might impact one another. React, of course, has it\u0026rsquo;s own way to manage when and where it should render things.\nWhere this rears it\u0026rsquo;s head is when considering how things such as materials are instantiated. With our components being managed by the React, while we get some niceties such as automatic disposing of unused materials etc., we can easily fall into the trap of repeatedly instantiating objects. For example, even for a simple box, if every render means we\u0026rsquo;re re-creating geometry and materials these will quickly add up.\nMemoisation, shared materials etc., or instancing can help here. In contrast, Babylon.js felt a little more simplified here from memory due to the in-engine code feeling quite divorced from the React wrapper.\nWhile covered in the documentation\u0026rsquo;s performance pitfalls section, one that can easily be missed is that we also need to be careful about when we call setState.\nuseFrame, useLoader, GLTFJSX are all things that are provided to help avoid these pitfalls, but coming from a React-heavy background these gotchas could easily be missed.\nRefs, refs everywhere I think the first thing that made me a slight sense of unease with R3F was that so many things needed to be wired up with useRef. It\u0026rsquo;s certainly not uncommon nor unexpected to need to access a bunch of the underlying objects. Maybe we want to toggle a material\u0026rsquo;s visibility, or alter a mesh\u0026rsquo;s position.\nIt quickly started to feel like a bit of ref hell, and while it was likely exacerbated by my willingness to let Copilot run away with its ideas (this was all part of the experiment), I quickly started losing track of all the refs flying around.\nNot terrible, and probably the only way to handle many of these cases, but it just felt like it was making things a little harder to hold than they really needed to be.\nAI? AI. Part of the experiment was seeing how Copilot held up to prompts. Now, I\u0026rsquo;m not about to become a vibe coder, but being realistic about the implications of AI in today\u0026rsquo;s software development landscape I was curious to see if Copilot could hold R3F in a usable way.\nI found Copilot to be quite usable, and this was without providing any custom instructions. Claude was unsurprisingly better than GPT, but I quickly ran out of free requests. I can imagine that with the appropriate hand-holding by way of global instructions etc., AI could be used as a significant accelerator to this kind of work — and as someone who does not have a strong foundation in 3D math, might actually help me get around some of those trickier 3D problems.\nConclusion Knowing that Babylon could absolutely tick all the boxes I needed it to, the question was largely whether Three.js could achieve the same or similar outcome, with the answer being a resounding yes.\nHonestly, that said I\u0026rsquo;m still not entirely sold on Three.js. Even with the tidy React integration that R3F provides, I can\u0026rsquo;t shake the feeling that it\u0026rsquo;s really still aimed at small-to-medium sized use cases. I\u0026rsquo;m not sure what might be involved in implementing a custom camera controller, for example, but it just didn\u0026rsquo;t feel like it would be quite so straightforward.\nI think at the end of the day, I would probably consider whether the 3D scene was going to be simple or complex, and how much UI needed to be wrapped around it.\nUpdate: Babylon.js\u0026rsquo; answer to R3F So, I did eventually come back around to have a look at react-babylon — admittedly I wasn\u0026rsquo;t well aware of this option when I first decided to look into R3F. It\u0026rsquo;s probably thrown a little bit of a spanner in the works for me with regards to leaning towards Three.js because it had such a nice integration with React.\nThere isn\u0026rsquo;t a whole lot that R3F does that react-babylon doesn\u0026rsquo;t for their respective frameworks. I did find the documentation to be reasonably good for both, although there were a couple of areas which weren\u0026rsquo;t quite as well-documented, such as the need to wrap a \u0026lt;Model\u0026gt; inside of a \u0026lt;Suspense\u0026gt;.\nI had wrongfully assumed that without the \u0026lt;Suspense\u0026gt; it would have just not shown anything while the mesh loaded, but that wasn\u0026rsquo;t the case — rather it caused a weird bug where much of the already-rendered objects were duplicated. It wasn\u0026rsquo;t really obvious what caused this, and it was through trial and error that I discovered where I\u0026rsquo;d gone astray.\nI did later stumble across a tiny entry in the documentation which speaks to the need for a wrapping \u0026lt;Suspense\u0026gt; if using the SceneLoader or AssetManager hooks, and I assume these are used under the hood when using the \u0026lt;Model\u0026gt; component directly.\nAsset management Asset management in general, or more particularly caching loaded assets, seemed less straightforward than with R3F. Where R3F offers a useLoader hook that automatically caches loaded assets, it appears that a little manual effort is required for react-babylon with the use of an asset manager.\nThis might actually be preferred in some circumstances — likely leaning back into the idea that Babylon.js is more suited to those larger projects where more careful management of asset caching is required.\nThe shiny bits I did enjoy coming back to Babylon.js and having a few things working a bit more nicely out of the box. Shadows felt like a first-class citizen, and I could easily switch a camera between orthographic and perspective views. Additionally, I know from experience that XR controls in Babylon.js are ready out-of-the-box, where Three.js didn\u0026rsquo;t seem to come with anything bar headset tracking.\nI think a major deciding factor between the two is going to be considering whether we\u0026rsquo;re happy to keep things as they are out of the box or not. Where Babylon.js offers a lot, I\u0026rsquo;m unsure as to how easy it is to modify their implementations. Three.js on the other hand looks to require either community plugins or manual work, but in doing so opens the door to a bit more flexibility.\nState management As a small aside, I did find react-babylon a little easier to hold than R3F with regards to state management, but I\u0026rsquo;m willing to put this entirely down to having a second bite of the cherry, and being a little more aware of how I was handling state and how it interacted across the React and frame loop boundaries.\nAll of the above still applies here however. There is still some complexity in managing the additional re-rendering of components compared to simply serving a file that renders a canvas and initialises the engine.\nThe escape hatch Something I didn\u0026rsquo;t recall seeing in R3F, or rather the specific Drei implementations I was using, was a way to easily access objects outside of using a ref. React-babylon offers a very useful onCreated prop on all the elements to serve as an escape hatch, allowing us to revert to Babylon.js\u0026rsquo; imperative approach.\nI found this to be quite helpful in a few cases — in particular when loading a mesh and needing to traverse all the sub-meshes to apply materials. Similar results in R3F were achieved by loading the mesh and applying the materials outside of our markup, then rendering it using a \u0026lt;primitive object={object} /\u0026gt;.\nSuper useful, and really helped to avoid feeling like I was being locked into a specific usage pattern.\nThe Inspector I know I mentioned this earlier, but Babylon.js\u0026rsquo; Inspector is such an incredibly useful tool for debugging a scene. In particular, I had some issues with setting up a material at first, and being able to make changes on the fly and see how it impacted the scene directly made for a really quick fix.\nCertainly, one of those tools you would just rather have access to.\nConclusion\u0026hellip; the, er, updated one Very tricky, honestly.\nI found Three.js and it\u0026rsquo;s respective integration with React to be more intuitive, but the overall documentation lacking. The out-of-the-box features were thin, but understandably so as it is more intended as a lightweight 3D rendering engine. I did find however that when things didn\u0026rsquo;t fit the R3F / Drei mould perfectly, it could get quite messy very quickly.\nBabylon.js continues to impress with what it does offer, and aside from the Inspector there are a slew of other tools available, including some which surprisingly didn\u0026rsquo;t seem to be linked on that page such as this IBL Tool for converting HDRI environments to Babylon.js\u0026rsquo; preferred .env format.\nI still find myself landing on the idea that Three.js is great if you want to move quickly and have a known, smaller scope. I could certainly see feature creep causing some significant unforeseen manual implementations, which creates an inherent risk with using this tool.\nSuffice to say, Babylon.js isn\u0026rsquo;t immune to risks such as this. Much like using a standard web UI library, there\u0026rsquo;s a trade-off between the ease of using out-of-the-box components and flexibility. There is of course a chance that what Babylon offers isn\u0026rsquo;t quite what we\u0026rsquo;re after, and we may not be able to simply tweak it to suit our use case.\nAll that said and done, I do find myself feeling pulled towards Babylon.js, particularly for some niceties such as shadows just working as one would expect.\n","date":"2025-07-02T00:00:00+10:00","image":"https://vivecuervo7.github.io/dev-blog/p/react-three-fiber/image_hu_c7be194bf4eaafa2.png","permalink":"https://vivecuervo7.github.io/dev-blog/p/react-three-fiber/","title":"React Three Fiber"},{"content":"Alright, I\u0026rsquo;ve scoffed enough at the ridiculous notion of vibe coding. Time to give Cursor a whirl.\nNo, I\u0026rsquo;m not about to become a vibe coder. But I have found the concept of Cursor intriguing, and I can\u0026rsquo;t help but to be curious about what the tool has to offer. GitHub Copilot has only been able to feel like a roided-up Intellisense — maybe Cursor can actually feel like the magic AI assistant I was promised.\nTo be completely fair to Cursor, I wanted to see how it performed without going into setting up custom project rules and in hindsight I may not have set it up entirely for success.\nA brand new project for a brand new world I wanted to experience the deep end of the magic first. I wanted to see what the whole \u0026ldquo;build me an app\u0026rdquo; felt like, and so I decided to go straight down that path.\nMy first impression wasn\u0026rsquo;t exactly great. I knew based on what I had read online that Svelte 5 probably wasn\u0026rsquo;t going to be too well supported due to models having been trained on Svelte 4, and indeed it took some coaxing and I even needed to provide a link to the latest documentation to get it to use the new Svelte CLI.\nWhile impressive that it did get there once I poked and prodded it a bit, it was also quick to go off one some very opinionated tangents, such as jumping straight into using Drizzle for data persistence. Not my first choice, and it was lumped in with a bunch of other changes that wouldn\u0026rsquo;t have been fun to untangle.\nI knew ahead of time this might happen, and so I decided to park Svelte for the time being. React it is!\nReact React had the same problem that Svelte did re: not using the Svelte CLI. It immediately tried to use the deprecated create-react-app, and only decided to use Vite after I explicitly told it to. Not a great start, but not the hardest thing to work around.\nI could see this being a problem for the true vibe coders out there, although to be fair I didn\u0026rsquo;t really give Cursor a chance to see any deprecation warnings and adjust accordingly. Maybe it would have realised?\nAs far as setting up components and state management, it wrote pretty close to the sort of code I\u0026rsquo;d expect to see. I was pretty happy with this, at least while things were in a simple state.\nTailwindn\u0026rsquo;t I asked Cursor to improve the styling across my components, and it jumped enthusiastically into the task. I was watching a bunch of the code it was generating and it was getting busy. But I wasn\u0026rsquo;t seeing any styling changes!\nTurns out, Cursor ❤️ Tailwind. But Cursor hadn\u0026rsquo;t installed Tailwind. D\u0026rsquo;oh!\nEasy enough fix, but it did highlight that it could go off on and start using tools that you hadn\u0026rsquo;t even asked it to. I\u0026rsquo;m honestly still not sold on Tailwind anyway, and I probably wouldn\u0026rsquo;t have gone for it had it asked me first. Ah well, embrace the vibe, right?\nIt took me quite a few round trips of \u0026ldquo;didn\u0026rsquo;t work, here\u0026rsquo;s my error\u0026rdquo; and ultimately an \u0026ldquo;is Tailwind even installed?\u0026rdquo; before it actually figured out what was wrong.\nTailwind\u0026rsquo;d With Tailwind actually installed now, I was pleasantly surprised at how the front end looked with a few nudges in the direction I wanted it to go. I had asked for modern, card-based styling, and that\u0026rsquo;s pretty much what I received.\nIt wasn\u0026rsquo;t perfect, there were some areas where padding was missing or inputs were hard to read, but overall I was quite impressed with what it spat out. I am curious to see if adding automated accessibility tests or other might help to keep things on track re: the obvious contrast issues in the inputs.\n.NET Core I was pretty impressed with what it did here, as much as I enjoy using .NET Core I find that there\u0026rsquo;s often a lot of code that needs to be written across many files.\nCursor was able to create a bunch of the files that I would have otherwise had to write myself, all with minimal prompting. Awesome! Except, half the files were in the wrong directory. It had gone and created a backend/ directory, and then dumped my Program.cs file outside of that, along with some other files. Another prompt re-created the files in the correct place but didn\u0026rsquo;t clean up the old ones.\nEven once it started putting things in the right places, refactorings indicated a similar disdain for cleaning things up. I do really wonder if putting the right tooling around Cursor might help with a lot of these issues, as it showed a good responsiveness to linter errors and warnings in the frontend project. Perhaps exposing similar errors to Cursor would guide it towards making better decisions.\nCORS issues came up — just like a real programmer! — and took another couple of spins on the merry-go-round before they were resolved. So far, I\u0026rsquo;ve been impressed by Cursor\u0026rsquo;s ability to resolve these issues, but somewhat unimpressed by the number of false resolutions.\nA minor frustration was a perceived disconnect between the Postgres database credentials and our configuration. Despite the database connection working perfectly, it kept accusing the configuration of being wrong and would keep changing the database credentials and/or the appsettings. I decided to let it do its thing, and while it didn\u0026rsquo;t necessarily break things it did get annoying.\nDo as I say, not as I do? The biggest and most glaring issue encountered was in the, you know, not super-duper-important auth service, where it had written some code and whacked a comment above it essentially saying \u0026ldquo;don\u0026rsquo;t do this in production\u0026rdquo;. Whoops? I can definitely imagine a true vibe coder missing this altogether. If I\u0026rsquo;m being honest, I missed it until I was reviewing the output many commits later.\nMaybe some tooling could have shown an error here? I\u0026rsquo;m planning to also look into CodeRabbit for AI code review. It would be curious to watch an AI tool telling another to get it\u0026rsquo;s sh*t together.\nEF Core Par for the course so far, things didn\u0026rsquo;t go perfectly at first although they did work in the end — which is the most important thing, right? Vibes!\nNo, it did a pretty good job at setting up and leveraging migrations, and this was pretty hands-off in general. One of the things that I find tedious with .NET development in general is the need to go and write a migration, apply it, update my model, update the DB context, all before I can even go and start using the new or updated entity. Cursor was definitely working hard to save me keystrokes here.\nI don\u0026rsquo;t know if my trust issues would start to surface if and when it came time to actually migrate some data around though.\nA brand old* project for a brand new world Sweet, Cursor seems to be capable of building a brand spanking new project. But what about an existing project that already has some conventions about how to do things?\nGiven that Cursor seemed to have some trouble swallowing Svelte earlier, I decided to try loading an existing SvelteKit project. Specifically, this project is using Svelte 5, which has a significantly different syntax to previous versions — and this has been Cursor\u0026rsquo;s tripping point.\nLet\u0026rsquo;s see how it handles some change requests!\nStarting small I wanted to ask it to specifically make changes to a single file. In the true spirit of side projects I\u0026rsquo;d been a bit lax in keeping any relevant documentation up to date, and so I asked Cursor to update my root README.md file to improve the project\u0026rsquo;s documentation.\nGranted, there wasn\u0026rsquo;t a whole lot that needed to be written, but it did a solid job of inspecting the project to determine the tech stack, prerequisites, steps for getting started etc.\nThe only issue was that for some reason despite the project using npm, Cursor decided to recommend pnpm and use that for a bunch of examples. A quick prompt sorted that out, but again this could have easily been missed.\nDatabase migration Okay, I was pretty impressed with this one. I wasn\u0026rsquo;t expecting it to pick up on my using graphile-migrate in this project, but it had a look at the tooling and it correctly updated my current.sql migration file. Nice!\nUsing a prompt to \u0026ldquo;create a migration allowing for league member\u0026rsquo;s to assign teams for each round\u0026rdquo;, it created the table I would have expected to see, along with indexes and even a trigger to enforce the team size limit.\nIt didn\u0026rsquo;t pick up on the need for the migration to be idempotent, but a quick prompt and it was back on track.\nA little more complicated I went through a series of prompts, from simply showing new information to adding an entire new form.\nCursor picked up on the use of Kysely to build queries, continuing with a repository pattern and implementing database queries as necessary. It proved to be similarly competent when it came to building UI components, inserting the new code and resolving linting errors to come up with a working solution.\nWhere Cursor ran into a bit of a wall was accessing the database service. I had settled on a usage pattern of initialising the database service and passing it down the chain via locals. Granted, this isn\u0026rsquo;t the most common approach (most tutorials suggest simply importing the database client and using it directly) however I thought there would be enough code there for Cursor to figure out the usage pattern.\nAlas, every time Cursor wanted to use the database service, it would attempt to instantiate a new copy of it. Not what I had wanted at all, and despite a prompt fixing it on one occasion I found it faster to just go and clean up after it. I did eventually see Cursor pick up on a linting error and subsequently explain to me that it should be using the locals.db instead of creating a new one, so maybe I was just trying to fix Cursor\u0026rsquo;s mistakes a little too eagerly.\nOverall impressions Honestly, it felt like taking a machine gun to my code. It felt very Spray n\u0026rsquo; Pray, and there was a lot of \u0026ldquo;yeah, I\u0026rsquo;m just going to commit here because I\u0026rsquo;m probably going to throw away the next change a few times before it\u0026rsquo;s right\u0026rdquo;. The problem was that each thing I wanted to do would take multiple prompts, which made me quite hesitant to discard whatever I had so I would keep throwing prompts at it to try and make it right. That led to me feeling too invested (why hello, sunk cost fallacy) to throw it all away.\nA big sticking point for me was the potential for insecure code to make its way into the codebase. I don\u0026rsquo;t think I trust it fully to keep things secure, pending wrapping additional tooling around it. Even then, it would require a close watch and there is just so much code being generated in such a short amount of time that it\u0026rsquo;s hard to pick up on little things.\nCursor did doing a pretty good job of coming up with meaningful commit messages, although they got pretty long-winded which exposes an actual underlying problem in that the changes themselves can easily become quite extensive with a series of iterative prompts causing many files to be touched.\nCursor turned me into a rubber duck Cursor felt like speed-running a pair programming session with a coked-up auctioneer as the driver.\nIt talks through the decisions it\u0026rsquo;s making, and shows you the code being generated. I found I could just keep up with it, occasionally spotting things that I wanted to ask about or prompt in another direction. It was nice to have that immediate feedback and it mitigated the feeling of blindly trusting Cursor to come up with a good solution, but there was an absolute flood of code to try and parse in very little time.\nIn particular, I found it curious to watch how it went about debugging issues, or more specifically the way it went about addressing linting errors and warnings. I wasn\u0026rsquo;t too impressed about the nonchalant way it dismissed warnings as essentially \u0026ldquo;meh, not critical so don\u0026rsquo;t bother\u0026rdquo; but we could either prompt it to address those concerns or just configure our project to treat warnings as errors etc.\nCursor\u0026rsquo;s rules for AI As mentioned at the top of this post I did not make use of Cursor\u0026rsquo;s rules for AI, which I imagine may have mitigated if not completely avoided many of the issues I ended up facing.\nI may do a deep dive into these rules in the future to do a comparison — especially regarding the use of Svelte 5, as this proved to be something of a sticking point. A very quick dabble in this yielded the ability to specify any particular tooling that I wished to use — telling it to use Svelte 5, Kysely and graphile-migrate did a pretty good job, although it still had trouble with the Svelte 5 syntax.\nThere are plenty of sample .cursorrules files floating around — these are being phased out per the docs above in favour of .cursor/rules/ but still provide a good base for building the rules themselves. Here\u0026rsquo;s an example of a relatively comprehensive set of general purpose Cursor rules.\nAnd\u0026hellip; we can always ask Cursor to generate a set of rules for itself based on the current project!\nI do get the sense that with the right rules Cursor could be kept largely in check, but I can\u0026rsquo;t help wondering if constantly tweaking those rules and needing to gauge the output might start eating significantly into whatever time Cursor initially saved.\nDoes it have a place in my workflow? The biggest question of them all, and truth be told, I don\u0026rsquo;t really know.\nA day spent playing with it doesn\u0026rsquo;t really give it the time it needs to really show its full potential — or frustrations, nor does the fact that I largely ignored setting up any rules.\nI can certainly see how I might prefer this over GitHub Copilot purely due to the fact that it can use the entire codebase as context for my next operation — but I think for now I\u0026rsquo;d prefer to keep the changes scoped to the file I\u0026rsquo;m working on.\nCursor passes the vibe check, but of me a vibe coder it shall not make.\n","date":"2025-04-02T00:00:00Z","image":"https://vivecuervo7.github.io/dev-blog/p/cursor-vibe-check/cover_hu_716d741442907acb.jpg","permalink":"https://vivecuervo7.github.io/dev-blog/p/cursor-vibe-check/","title":"Cursor: Are the vibes really worth it?"},{"content":"Whenever I decide to do any programming outside of work hours, I like to pick up a bunch of tools that I don\u0026rsquo;t get to touch in my day-to-day, even if it\u0026rsquo;s just so side projects don\u0026rsquo;t feel like work.\nSometimes, I stumble across some tooling that I want to roll into my standard workflow — PostCSS being one that I\u0026rsquo;ve stumbled across a couple of times now and decided to cobble together something a little firmer than just a \u0026ldquo;hey, I tried this and I liked it\u0026rdquo;. More specifically, I have found myself leaning towards PostCSS as a replacement for the default of dragging Sass into every new project I\u0026rsquo;ve worked on.\nDon\u0026rsquo;t get me wrong — Sass is and has been a great tool to use over the years, but I feel as though I\u0026rsquo;ve only ever really needed a fraction of what it offers. Not to mention that the growth of CSS itself has slowly started eating away at the benefits I\u0026rsquo;ve found in Sass, such as nesting or variables.\nWhat is PostCSS Or maybe it\u0026rsquo;s easier to talk about what it isn\u0026rsquo;t.\nIt certainly isn\u0026rsquo;t a pre-processor like Sass or Less. It doesn\u0026rsquo;t use a different syntax the way these pre-processors do, allowing us to instead write standard CSS. Which is good for newcomers who may not be familiar with a particular flavour of pre-processor!\nIt also isn\u0026rsquo;t strictly a post-processor, despite the name alluding to such. The rich plugin ecosystem really allows it to act as both a pre- and post-processor. It\u0026rsquo;s modular, and highly flexible — you could even keep your desired pre-processor around. This isn\u0026rsquo;t an either-or scenario.\nOk\u0026hellip; so, what is it? PostCSS is, in their own words, \u0026ldquo;a tool for transforming CSS with JavaScript\u0026rdquo;.\nWhat PostCSS does is transpile your written CSS into JavaScript, allowing us to process it before writing it back as CSS. By itself, it doesn\u0026rsquo;t actually transform anything — for that we\u0026rsquo;ll need to tap into the plugins available to PostCSS.\nOn the topic of plugins, there are a few of them in the wild which are regularly used — often without the explicit intention to use PostCSS. Autoprefixer, cssnano and Stylelint are all commonly used PostCSS plugins, meaning there\u0026rsquo;s a good chance you\u0026rsquo;ve already used PostCSS!\nWait, so if it\u0026rsquo;s not a pre-processor\u0026hellip; Yep, that\u0026rsquo;s right. You can stick with your trusty Sass, Less, or Stylus. Or, you could bring in a few plugins to get your pre-processor-like functionality. This is where a lot of my interest in PostCSS started, looking at it as a potential drop-in replacement for Sass entirely. I\u0026rsquo;ll cover some of those specific plugins below.\nPlugins All the useful things you can do with PostCSS come from plugins. Below we\u0026rsquo;ll have a look at some of the plugins that I\u0026rsquo;ve found consistently seem to bubble up as the more useful picks.\nIf you want to see all of the plugins, here is a good place to start.\nThe plugins you probably want Autoprefixer\nAutoprefixer adds vendor prefixes to your CSS using rules from Can I Use. Forget about manually writing vendor prefixes entirely! Stylelint can also be configured to disallow writing vendor prefixes too, so you don\u0026rsquo;t even need to remember to forget about vendor prefixes.\ncssnano\ncssnano makes your CSS small. It\u0026rsquo;s a minifier that compresses your CSS to reduce your overall bundle size. There are a few presets, the default being a safe option, making it easy enough to drop in that it seems almost silly to not include it. The full list of optimisations can be found here.\nStylelint\nOk, ok. You probably won\u0026rsquo;t install this as a plugin the way you will the others, but it\u0026rsquo;s still a PostCSS plugin so I\u0026rsquo;ll include it here. Stylelint is simply put, a linting tool for your CSS. There are a bunch of useful rules that are worth perusing.\nIf the rules included with Stylelint aren\u0026rsquo;t quite enough for you, there are a bunch of plugins, configs and integrations available. Just in case you need plugins for your plugins.\nTomorrow\u0026rsquo;s CSS, today This one is in my opinion one of the most powerful and compelling reasons to use PostCSS, so it\u0026rsquo;s getting its own section.\nPostCSS Preset Env essentially allows us to use modern CSS without worrying about it not being supported in a bunch of browsers (looking at you, Internet Explorer Safari). By leveraging CSSDB and your specified browser targets, it automatically includes the necessary plugins to ensure consistent behavior across different environments.\nYou can even start using experimental or proposed CSS features. PostCSS Preset Env\u0026rsquo;s complete feature list can be found here. Some favourites are listed below:\ncustom-media-queries custom-selectors light-dark-function Sass-like functionality The last time I looked at bringing in a bunch of Sass-like functionality, I found myself installing a bunch of plugins for all the various things. Those individual plugins are still available if you really want to bring them in individually, such as postcss-mixins.\nIt looks as though the easiest path to similar functionality might be to just go straight for postcss-advanced-variables. This plugin brings Sass-like $variables, @if, @else, @for and @each rules, as well as @mixin rules. And honestly, with nesting already supported, this covers 90% of the things I really got out of Sass anyway. A counterpart for Sass\u0026rsquo; maps exists in postcss-map-get, and really the last missing piece is support for functions.\nTo that end, it seems the Sass-like postcss-define-function plugin for this hasn\u0026rsquo;t seen any love in a while — and I\u0026rsquo;d probably consider using postcss-functions which is a bit different in that it leans towards defining functions in JavaScript.\nIf I found myself needing any other features that weren\u0026rsquo;t readily available via existing plugins I\u0026rsquo;d likely start considering a move to just go back to using Sass, but I think the above would cover just about all the functionality I\u0026rsquo;ve actually used over the past few years.\nTL;DR postcss-advanced-variables and postcss-map-get is probably going to give you most of what you\u0026rsquo;re using Sass for today.\nWriting custom plugins While I haven\u0026rsquo;t found the need to do this myself, PostCSS also purports to make it fairly straightforward to write your own custom plugins, which is a huge leap from being at the mercy of whatever functionality is offered by one of the pre-processor options.\nChances are however that you\u0026rsquo;ll generally find what you\u0026rsquo;re after. There are even existing plugins to let you use yards, feet and twips! Don\u0026rsquo;t do that, though.\n","date":"2025-03-28T00:00:00Z","image":"https://vivecuervo7.github.io/dev-blog/p/less-sass-with-postcss/cover_hu_aabc3c4af820f9c5.jpg","permalink":"https://vivecuervo7.github.io/dev-blog/p/less-sass-with-postcss/","title":"PostCSS: Styling without the Sass"},{"content":"As I started to really dive into SvelteKit, I couldn\u0026rsquo;t help but find myself drawn to this idea of writing as little code as possible. Don\u0026rsquo;t get me wrong — I still love working with C#, and honestly it\u0026rsquo;s still where I\u0026rsquo;d turn to in a heartbeat if I had need to model a complex business domain.\nI haven\u0026rsquo;t been able to shake the thought though, that quite often we end up building rather small applications that might be simpler and faster to write using one of the current full-stack meta-frameworks. While SvelteKit has been my poison of choice in that regard, what is outlined here is framework-agnostic and should be applicable to any of the current offerings.\nORMs, script builders, raw SQL\u0026hellip; I have to admit to feeling a little lost when I first stepped into the idea of managing persistence without my trusty Entity Framework Core. Instinctively, I reached for an ORM and did some poking around.\nI came across this fantastic post outlining the usage of Drizzle. I was pretty excited, things looked nice and easy to implement. Getting started was fairly straightforward using a Postgres database, but as I looked around I started stumbling across discussions such as this one outlining the shortcomings of Drizzle.\nNice as it was, I decided to keep looking.\nTypeORM looked like a long term player, but seemed to be an unpopular choice.\nPrisma looked a bit more positive, but I mistook their \u0026ldquo;pricing\u0026rdquo; page to indicate it was a paid option, which I later discovered was not the case, however the ship had sailed by this point.\nMikroORM received the most praise based on my research, but wasn\u0026rsquo;t quite as easy to get set up as Drizzle.\nBy this point I was pretty over the idea of using an ORM in TypeScript-land.\nOr something else\u0026hellip;? I have to credit this post on Reddit for pointing me in the direction I eventually found myself going in.\nWhile the post itself goes into using GraphQL which isn\u0026rsquo;t something I was particularly interested in, it did cause me to start looking into what a solution might look like if it were truly data-driven — allowing your database to be the source of truth for your application.\nThis resonated with me, as someone who typically preferred a data-first approach. It aligned very closely to my typical workflow in .NET where I\u0026rsquo;d use DbUp to handle database migrations.\nThe magic sauce After a bit of a dig at just writing plain SQL, I decided to finally give Kysely a crack — a simple, no-nonsense query builder for TypeScript. It had consistently received high praise wherever I had seen it mentioned, and suffice to say I was not disappointed when I finally used it.\nBut first, we need some data!\ngraphile-migrate The aforementioned Reddit post eventually landed me on graphile-migrate — a migration tool that introduced an interesting way to manage migrations.\nWhile I can certainly envision some scenarios that could get a little messy, the premise is that you would write your migrations to be idempotent. Writing your migrations to a current.sql file would allow you to use the tool\u0026rsquo;s watch mode, re-running the migration every time the file is changed.\nOnce work on the \u0026ldquo;current\u0026rdquo; migration is complete it can be committed, which will move it into the committed folder as a migration ready to be run against a production database.\nDespite needing some additional thought to how best to write idempotent migrations, I found that in practice it allowed for a much more iterative workflow when compared to the usual sitting down and figuring out all of the data requirements as the very first step.\nWe\u0026rsquo;ll use the following migration to illustrate how the other tools are used in a basic real-world scenario.\ncurrent.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 drop table if exists app_user cascade; drop table if exists league cascade; drop table if exists player; drop type if exists player_role; create type player_role as enum (\u0026#39;batter\u0026#39;, \u0026#39;bowler\u0026#39;, \u0026#39;keeper\u0026#39;, \u0026#39;allrounder\u0026#39;); create table app_user ( id uuid primary key ); create table league ( id uuid primary key default gen_random_uuid(), name varchar(30) not null unique, description varchar(200) not null, owner_id uuid references app_user not null ); create table player ( id uuid primary key default gen_random_uuid(), league_id uuid references league on delete cascade not null, name varchar(30) not null, role player_role not null, image_url varchar(2048) ); kysely-codegen Before we can work with Kysely, we need to provide a database schema in a format that Kysely can understand. Database introspection isn\u0026rsquo;t something I\u0026rsquo;d done much of, and fortunately the Kysely documentation outlines a few tools that can aid with this.\nkysely-codegen as one of their offering purports to work with all dialects supported by Kysely itself, so that was the one that I picked off the list.\nConfiguration for graphile-migrate allows for running commands at various points, such as after all migrations, or after just the current migration. This allowed for a simple way to run kysely-codegen every time graphile-migrate\u0026rsquo;s watch function caused the migrations to re-run. Nice!\nRunning kysely-codegen against the database after the migration above was applied yields the following.\ndb-schema.ts 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 /** * This file was generated by kysely-codegen. * Please do not edit it manually. */ import type { ColumnType } from \u0026#34;kysely\u0026#34;; export type Generated\u0026lt;T\u0026gt; = T extends ColumnType\u0026lt;infer S, infer I, infer U\u0026gt; ? ColumnType\u0026lt;S, I | undefined, U\u0026gt; : ColumnType\u0026lt;T, T | undefined, T\u0026gt;; export type PlayerRole = \u0026#34;allrounder\u0026#34; | \u0026#34;batter\u0026#34; | \u0026#34;bowler\u0026#34; | \u0026#34;keeper\u0026#34;; export interface AppUser { id: string; } export interface League { description: string; id: Generated\u0026lt;string\u0026gt;; name: string; owner_id: string; } export interface Player { id: Generated\u0026lt;string\u0026gt;; image_url: string | null; league_id: string; name: string; role: PlayerRole; } export interface DB { app_user: AppUser; league: League; player: Player; } kysely I can\u0026rsquo;t stress just how impressed I am with Kysely despite not having worked with it for long. It really does a fantastic job of not trying to do too much — it just gets out of the way and makes it feel quick and easy to work with.\nSetting up the database client using Kysely was very straightforward.\ndb.ts 1 2 3 const pool = new pg.Pool({ connectionString }); const dialect = new PostgresDialect({ pool }); const client = new Kysely\u0026lt;DB\u0026gt;({ dialect, log: [\u0026#34;error\u0026#34;] }); I decided to break the tables down into repositories for ease of access, and here we can see how simple queries look using Kysely. We get full type-safety along with auto-complete while writing the entire query, and I found that the query itself mapped nicely to what I\u0026rsquo;d expect the generated SQL to look like.\nplayers.ts 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 export class PlayerRepository extends RepositoryBase { public async create(player: Insertable\u0026lt;Player\u0026gt;) { return await this.client .insertInto(\u0026#34;player\u0026#34;) .values(player) .returning(\u0026#34;id\u0026#34;) .executeTakeFirstOrThrow(); } public async update(id: string, values: Updateable\u0026lt;Player\u0026gt;) { await this.client .updateTable(\u0026#34;player\u0026#34;) .set(values) .where(\u0026#34;id\u0026#34;, \u0026#34;=\u0026#34;, id) .execute(); } public async delete(id: string) { await this.client.deleteFrom(\u0026#34;player\u0026#34;).where(\u0026#34;id\u0026#34;, \u0026#34;=\u0026#34;, id).execute(); } } Kysely provides us with Selectable\u0026lt;T\u0026gt;, Insertable\u0026lt;T\u0026gt; and Updateable\u0026lt;T\u0026gt; wrappers for each table, which should give us the correct types for each respective operation.\nAdditionally, you can replace execute() with compile() or explain() to get an idea of what\u0026rsquo;s being generated under the hood. See the following for a truncated example based on running compile() against the create(...) command above.\n1 2 3 4 5 6 7 8 9 10 { query: { ... }, sql: \u0026#39;insert into \u0026#34;player\u0026#34; (\u0026#34;name\u0026#34;, \u0026#34;image_url\u0026#34;, \u0026#34;role\u0026#34;, \u0026#34;league_id\u0026#34;) values ($1, $2, $3, $4) returning \u0026#34;id\u0026#34;\u0026#39;, parameters: [ \u0026#39;Isaac Dedini\u0026#39;, \u0026#39;\u0026lt;image_url\u0026gt;\u0026#39;, \u0026#39;bowler\u0026#39;, \u0026#39;\u0026lt;uuid\u0026gt;\u0026#39; ] } Going even further I was pretty happy with what I had landed on here, but I started wondering if we might be able to also extract some additional metadata and generate validation schema based on some of the database properties — namely wherever we\u0026rsquo;ve used varchar(n).\nYeah, as you can probably tell I\u0026rsquo;ve run into a few cases where our application\u0026rsquo;s validation hasn\u0026rsquo;t been updated to reflect a change in the database schema. I wanted to try and fix that.\nI started out here looking around at various options, including replacing kysely-codegen with kanel-kysely just so I could leverage kanel-zod for this purpose. I was left underwhelmed at what was actually generated, and decided to go back to the drawing board.\nGenerating Zod validation schema with kysely-codegen The 0.18.0 release of kysely-codegen introduced the ability to define custom serializers, largely related to this GitHub discussion. This gave me most of what I was after once I\u0026rsquo;d extended the example provided in the linked release notes to include more extensive mappings.\nSomething that appeared to be missing yet as one of the key reasons I wanted to look into database-driven Zod schema was the ability to extract maximum character lengths for varchar columns. Of course, there is advice floating around that suggests preferring text instead of varchar, but it\u0026rsquo;s a habit I can\u0026rsquo;t shake.\nI also wanted to refrain from mapping check constraints — I needed to draw a line here and stopping short of check constraints felt like a good balance between simplicity and functionality. So, consider that to be a caveat of this approach, although nothing is stopping you from extending the serializer to account for check constraints either.\nScripts I eventually settled on adding the following script to allow me to grab additional metadata from the database, essentially returning a mapping between column names and their character_maximum_length property.\nget-column-metadata.js 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import pg from \u0026#34;pg\u0026#34;; const table = process.argv[2]; const client = new pg.Client({ connectionString: process.env.DATABASE_URL || \u0026#34;\u0026#34;, }); await client.connect(); const query = ` SELECT column_name, character_maximum_length FROM information_schema.columns WHERE table_name = $1; `; const result = await client.query(query, [table]); await client.end(); console.log(JSON.stringify(result.rows)); Configuration I\u0026rsquo;ve truncated the mapDataTypeToZodString(...) function below for brevity, but it retains all the mappings required for the tables defined so far.\n.kysely-zod-codegenrc.ts 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 import \u0026#34;dotenv/config\u0026#34;; import { toKyselyCamelCase } from \u0026#34;kysely-codegen\u0026#34;; import { join } from \u0026#34;path\u0026#34;; import { execSync } from \u0026#34;child_process\u0026#34;; export default { url: process.env.DATABASE_URL || \u0026#34;\u0026#34;, outFile: join(process.cwd(), \u0026#34;src\u0026#34;, \u0026#34;lib\u0026#34;, \u0026#34;generated\u0026#34;, \u0026#34;db-schema-zod.ts\u0026#34;), excludePattern: \u0026#34;graphile_migrate.*\u0026#34;, serializer: { serializeFile: (metadata) =\u0026gt; { let output = \u0026#39;import { z } from \u0026#34;zod\u0026#34;;\\n\\n\u0026#39;; for (const table of metadata.tables) { output += \u0026#34;export const \u0026#34;; output += toKyselyCamelCase(table.name); output += \u0026#34;Schema = z.object({\\n\u0026#34;; const additionalColumnMetadata = getAdditionalColumnMetadata( table.name ); for (const column of table.columns) { output += \u0026#34; \u0026#34;; output += column.name; output += \u0026#34;: \u0026#34;; output += mapDataTypeToZodString(column.dataType, column.enumValues); if ( !column.isNullable \u0026amp;\u0026amp; (column.dataType === \u0026#34;text\u0026#34; || column.dataType === \u0026#34;character varying\u0026#34; || column.dataType === \u0026#34;varchar\u0026#34;) ) { output += \u0026#34;.min(1)\u0026#34;; } const maxLength = additionalColumnMetadata.find( (c) =\u0026gt; c.column_name === column.name )?.character_maximum_length; if (maxLength) { output += `.max(${maxLength})`; } if (column.isNullable) { output += \u0026#34;.optional()\u0026#34;; } output += \u0026#34;,\\n\u0026#34;; } output += \u0026#34;});\\n\\n\u0026#34;; } return output; }, }, }; function getAdditionalColumnMetadata(tableName: string) { const result = execSync( `node scripts/get-column-metadata.js \u0026#34;${tableName}\u0026#34;`, { encoding: \u0026#34;utf-8\u0026#34; } ); return JSON.parse(result); } function mapDataTypeToZodString(dataType, enumValues) { switch (dataType) { case \u0026#34;integer\u0026#34;: return \u0026#34;z.number()\u0026#34;; case \u0026#34;varchar\u0026#34;: return \u0026#34;z.string()\u0026#34;; case \u0026#34;uuid\u0026#34;: return \u0026#34;z.string().uuid()\u0026#34;; default: return enumValues \u0026amp;\u0026amp; enumValues.length \u0026gt; 0 ? `z.enum([${enumValues.map((value) =\u0026gt; `\u0026#39;${value}\u0026#39;`).join(\u0026#34;, \u0026#34;)}])` : \u0026#34;z.unknown()\u0026#34;; } } Output db-schema-zod.ts 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 import { z } from \u0026#34;zod\u0026#34;; export const appUserSchema = z.object({ id: z.string().uuid(), }); export const appUserLeagueSchema = z.object({ user_id: z.string().uuid(), league_id: z.string().uuid(), }); export const leagueSchema = z.object({ id: z.string().uuid(), name: z.string().min(1).max(30), description: z.string().min(1).max(200), owner_id: z.string().uuid(), }); export const memberSchema = z.object({ id: z.string().uuid(), league_id: z.string().uuid(), name: z.string().min(1).max(30), role: z.string().min(1).max(30), }); export const playerSchema = z.object({ id: z.string().uuid(), league_id: z.string().uuid(), name: z.string().min(1).max(30), role: z.enum([\u0026#34;allrounder\u0026#34;, \u0026#34;batter\u0026#34;, \u0026#34;bowler\u0026#34;, \u0026#34;keeper\u0026#34;]), image_url: z.string().max(2048).optional(), }); export const roundSchema = z.object({ id: z.string().uuid(), league_id: z.string().uuid(), }); Usage Now that we have our Zod validation schemas, we can use them for form validation. In the case of a create player form we only ask the user to supply three of the properties, so we just restrict the schema accordingly.\nOnce that\u0026rsquo;s done we can extend the schema to add rules to an individual property. This allows us to layer business rules on top of our database-driven validation.\ndb-schema-zod.ts 1 2 3 4 5 6 7 8 import { playerSchema } from \u0026#34;$lib/generated/db-schema-zod\u0026#34;; import { z } from \u0026#34;zod\u0026#34;; export const formSchema = playerSchema .pick({ name: true, image_url: true, role: true }) .extend({ image_url: playerSchema.shape.image_url.unwrap().url().or(z.literal(\u0026#34;\u0026#34;)), }); In this case, changing player.image_url from varchar(2048) to varchar(20) would cause the form\u0026rsquo;s validation to instantly reflect that change without needing us to go and modify a separate schema.\nConclusion After digging through a swathe of options when it comes to managing persistence in a full-stack meta-framework, eventually stumbling upon the above felt like a revelation.\nWhile no doubt there will be hitches along the way, this presents a way to move forward quickly and iteratively with data changes, allowing them to propagate throughout your codebase via the generated types.\nEssentially, this boils down to a very straightforward workflow — add a migration, let it spin for a second or two, and start using the generated types.\n","date":"2025-03-25T00:00:00Z","image":"https://vivecuervo7.github.io/dev-blog/p/database-driven-codegen/cover_hu_bba369d8b46af45d.jpg","permalink":"https://vivecuervo7.github.io/dev-blog/p/database-driven-codegen/","title":"Harnessing database-driven code generation"},{"content":"I have at times felt myself wondering what the landscape of hybrid app development looks like, and after a less-than-positive experience using Ionic in the past I wanted to give myself some time to truly explore the options I had at my disposal.\nThe idea of this will be to write a very basic implementation of a Todo app for each of the following frameworks, providing brief findings around setup and the inner dev loop. None of the frameworks yielded a complete Todo app and this had little bearing on the findings, but it did provide a common ground to anchor any findings to.\nNothing too fancy, I\u0026rsquo;ve taken a \u0026ldquo;day in the life of\u0026rdquo; approach here to constrain things to a very tight timebox and taken a few notes about the experiences as I went.\nThere\u0026rsquo;s a bit of a brain dump following this. Jump to the conclusion if you\u0026rsquo;re mostly interested in the overall findings. React Native + Expo General observations Using Expo with React Native is strongly suggested by the community based on some initial searches Looks like while Expo portrays itself as a paid service, doing local builds (eas build --platform ios --local) doesn\u0026rsquo;t require their paid services Unsure in starting out between Expo Go and Development Build — opted for the latter, seems to be their recommendation for production workloads Need to sign up for a (free) Expo account if using Expo Application Services, but looks like this is unnecessary if using npx expo run:ios Pros Project structure unsurprisingly looks familiar coming from a React (web) background Fast Refresh (i.e. hot reloading) works well Has a npx @react-native-community/cli doctor similar to Flutter which helps to diagnose any development environment issues Platform-specific extensions make it look far easier to implement any platform-specific code with a consolidated import EAS Updates looks incredibly useful however comes with a hefty price tag — this looks like it can be circumvented by setting up a custom updates server Cons While documentation suggests that components can automatically switch between iOS and Android depending on the platform, the shipped components are very basic (a list view looked completely unstyled) and likely require a third-party UI library Web support seems to only be supported via a third party library, but this appears to be configured as the default when using Expo Platform-specific observations Android Environment setup is a little more involved than Flutter, requiring installing JDK and manually setting ANDROID_HOME and JAVA_HOME Straightforward to run on both emulator or real device Didn\u0026rsquo;t attempt to fix, but running a build using eas build --platform android --local failed iOS Straightforward to get running in an emulator Similar to Flutter etc., running on a connected device requires a paid developer account Adding a list view for the todos didn\u0026rsquo;t come with any styling even remotely resembling the native iOS components Running eas build --platform ios --local indicated that an active Apple account would be required to complete Web Seems that similar to Flutter web is a second-class citizen in React Native, however Expo specifically appears to use the third-party react-native-web library and provides the means to build and serve a web app Summary The lower learning curve coming from a React background making this option hard to ignore.\nExpo feels like a really nice wrapper around development, even without the paid options. It looks like using it also addresses the apparent issue around web not seemingly being supported without manually setting up a third-party library.\nThe apparent lack of styled components and push towards community-driven UI libraries etc. (even a basic checkbox component was marked as deprecated with a link to community alternatives) gives pause when considering React Native as a target for Android and iOS. Flutter seemed stronger in this regard, however the platform-specific extensions look like a much cleaner way to go about writing platform-specific code.\nWhile not mentioned above I did stumble across Solito as a library attempting to address some apparent differences in the way navigation needs to be implemented for web vs. native. I didn\u0026rsquo;t get into any navigation to understand this for myself, but it did make me wonder as to the potential obstacles here.\nFlutter General observations Syntax feels more closely reminiscent of Swift UI than other web frameworks i.e. React Requires conditionally switching between iOS / Android / web widgets manually Android and iOS development seem solid, web however had some issues I was unable to resolve in a reasonable timeframe Ships with components, or \u0026lsquo;widgets\u0026rsquo;, implementing both Apple\u0026rsquo;s Human Interface Guidelines and Material Design Pros Quick to get up and running Useful VS Code extension Hot reloading works really nicely, maintains state through app changes Dart DevTools look like they could be incredibly helpful flutter doctor command quite useful for getting development environment set up Similarly, flutter devices and flutter emulators are handy - CLI tooling is quite solid overall Building app files is straightforward with flutter build \u0026lt;target\u0026gt; with apk, ios, macos, web etc. Cons Requires learning a new language (Dart) Widgets aligned to platforms (Material UI vs. iOS) need to manually be implemented conditionally or rely on third-party packages for auto-switching between them Issues running app in debug mode for web (see Web section) Platform-specific observations Android Running against an emulator is pretty straightforward once Android Studio is set up Similarly, very easy to run against a real device once connected VS Code integration seemed less useful here, with no way to select the target device / emulator Material Design is the default, which makes it unsurprisingly very aligned to Android development iOS VS Code extension facilitates choosing from Simulator, Mac Designed for iPad, or macOS as the target device Some permissions need to be enabled manually e.g. HTTP calls fail until adding the appropriate entitlement Appears that a paid developer account is required to develop locally against a real device, however actually connecting to seems relatively straightforward Without going too deep, the available Cupertino widgets look like they provide a solid ecosystem for developing complete apps aligned to the Human Interface Guidelines Web Checking for Platform.IsIOS etc. breaks on web, and requires ensuring that those checks are only run after checking that we\u0026rsquo;re not in a web browser Hot reloading doesn\u0026rsquo;t work here, only hot restarting VS Code not showing web / browser as a target, could be due to using a non-standard browser; needed to launch via CLI or manually created launch configuration for VS Code Ultimately, I had this time-boxed and was unable to actually get the web view to load — perhaps this is due to using a niche browser (Arc, which is however Chromium-based) — in either case this seems to be a widely reported issue with no concrete solution Summary Ultimately Flutter looks like a very promising option if we\u0026rsquo;re targeting only Android and iOS. Once any issues are resolved for web, that likely becomes a valid target, however web as a target felt like a second-class citizen.\nThe Cupertino widgets make Flutter appear quite capable of producing a rich, native-like application, despite needing to more often than not implement separate Android and iOS widgets. Material Design as the other widget ecosystem provided by default produces a native-like Android experience.\nThe learning curve would be quite high owing to the need to learn a new language in Dart, especially when it comes to navigation and state management.\nCapacitor / Ionic General observations Came as a bit of a surprise upon spinning up a new project — Ionic still uses react-scripts Pros Syntax is very close to what a typical React web project looks like - very little learning curve and familiar workflows Shipped Ionic components implement both iOS and Material Design styles that automatically switch based on the platform Cons General consensus based on some articles / conversation is that performance is not very good, especially compared to e.g. Flutter Community sentiment seems to be quite poor, often recommending Flutter and React Native as much better alternatives Documentation is a little lacking when setting up for running on other devices — requires @capacitor/core and @capacitor/cli, run npx cap init before we can add iOS / Android with npx cap add iOS etc. Platform-specific observations Android Running against emulator seems to work as expected, but as with iOS was unable to get live reloading working here Running against a real device seemed to be on par with the emulator experience for Android iOS Initial build worked fine, subsequent builds yielded a blank screen and no useful logs were not easy to find, assuming they exist Attemps to run on a real device yielded an error \u0026ldquo;Device is busy\u0026rdquo; While documentation suggests it is possible, could not get live reloading working when the builds did work Web Compared to Flutter and React Native, web feels like a first-class citizen under Ionic Hot reloading works well for web Summary Ionic feels like it has a nice happy path, but once anything goes slightly astray it starts to feel quite shaky. I would have dismissed this as the byproduct of trying to squeeze a brief intro into a single day, but this is consistent with my experience using it a couple of years prior.\nThe component rendering is quite nice and it\u0026rsquo;s very handy just needing to render a single component and have it displayed relatively correctly per the target platform, but again this relies on that happy path. Going back to my previous experience and any custom styling etc. can be quite difficult.\nUltimately Ionic felt a fair way off the more polished experiences of Flutter and React Native. It likely has a place for a quick prototype where the majority of active development can be done in the web browser and device or emulator builds are used more sparingly. There may be another half-point here when it comes to web development specifically, but I would be more inclined to use one of the other offerings.\n.NET MAUI General observations Little confusing to start re: ability to also cover targeting web — ultimately scrapped the default MAUI project in lieu of the .NET MAUI Blazor Hybrid and Web App template Pros Similar to Blazor, being able to maintain this under a single solution makes sharing data contracts etc. much more straightforward Looks like it\u0026rsquo;s essentially Blazor from here on out, which with some experience makes me feel confident of a reasonable framework for development Being unable to run on iOS or Android I was unable to confirm, but the MAUI controls such as the ListView did not appear to implement any platofrm-specific styling or functionality, appearing that this would require manual work or third-party implementations Cons Trouble debugging via VS Code, needed to revert to running via CLI Running via CLI not working out of the box for iOS or Android (dotnet build works for all three targets) VS Code on macOS troubles seem to be common — seems that Rider is likely a better option Platform-specific observations Android Unable to run via VS Code or CLI From my understanding, using the template required to support web means that the app will essentially be the web app rendered inside a web view — unsure of implications on performance or platform interactivity etc. iOS Unable to run via VS Code or CLI From my understanding, using the template required to support web means that the app will essentially be the web app rendered inside a web view — unsure of implications on performance or platform interactivity etc. Web Little confusing to start, needing to re-create project with .NET MAUI Blazor Hybrid and Web App template Started up as expected Hot reload worked initially, but stopped after the first update and subsequent changes did not trigger any reload Summary MAUI came with the most troublesome setup, to the point that I was unable to get it running on iOS or Android emulated devices within a timebox. While this may have been environment-specific, I didn\u0026rsquo;t want to consider using a specific IDE as a valid solution.\nThe apparent lack of out-of-the-box support for platform-specific styling also painted a less than ideal picture.\nThere was also some confusion over how the framework was intended to be used, with a new project based on the .NET MAUI template looking very different to the .NET MAUI Blazor Hybrid App and Web App template.\nIt didn\u0026rsquo;t take long for me to begin feeling that this wasn\u0026rsquo;t a framework I wanted to work with, despite some attraction to the idea of being able to use Blazor or a Blazor-like syntax, having used it in the past.\nConclusion The following is very much based around my specific environment and some aggressive timeboxes, and is reflective of my background in using predominantly React for front-end development.\nFramework Would use? Setup / getting started Tooling Learning curve OOTB components* React Native 👍 ⭐⭐ ⭐⭐⭐ 📕 ❌ Flutter 👍 ⭐⭐⭐ ⭐⭐⭐ 📕 📕 📕 ✅ Ionic ➖ ⭐⭐ ⭐ 📕 ✅ .NET MAUI ❌ ⭐ ⭐ 📕 📕 ❌ * Specifically, out-of-the-box platform-specific components Ultimately I would not, based on this experience, want to move forward with .NET MAUI for any of the targets.\nIonic felt a little better but this experience did little outside of reaffirming my sentiment in not wanting to use this again. I do believe however that due to the relatively pleasant happy path it provides, it may still have a place for rapidly building a simple prototype with the out-of-the-box platform-specific components. I would not pick this over Flutter or React Native for a more serious project.\nReact Native and Flutter felt quite even, but the deciding factors for me would be time constraints, specifically regarding the learning curve, and target platforms, looking whether we intend to target web.\nFor strictly iOS and Android development, it\u0026rsquo;s hard to look past the out-of-the-box components that Flutter ships with. While they need to be implemented conditionally, I feel that this workflow would still be preferred to leaning on third-party components or requiring manual implementations as React Native would require.\nIf web was intended as a priority target alongside the two main mobile platforms, I feel that React Native creeps ahead here. There is a small caveat in the as-of-yet unexplored navigation, where the existence of the Solito library suggests some pain around navigation where web and native platforms are being used. This may be either addressed in recent versions, or by third-party libraries e.g. Solito.\nI did walk away from this experience feeling that Flutter had more upside to it, but given the above considerations I would need to factor in the learning curve against the shipped platform-specific components. If pressed, I would feel compelled to choose React Native should I be starting a project today, but Flutter looks to be a very promising avenue for professional development and future usage.\n","date":"2024-12-09T00:00:00+10:00","image":"https://vivecuervo7.github.io/dev-blog/p/hybrid-app-dev/cover_hu_928f7fb88b3e1d1a.jpg","permalink":"https://vivecuervo7.github.io/dev-blog/p/hybrid-app-dev/","title":"Comparing hybrid app frameworks"},{"content":"A client I have worked for previously often provided us with a list of requirements for any solution we built. One of those was around performance, quoting a target average response time of under 2 seconds for all requests. More often than not this was tested late into the project via some third-party scripts, leaving us with little time to resolve any issues.\nHaving previously needed to work on improving performance on an API we had built elsewhere, I was interested in learning how we might be able to enhance our workflow to ensure we could meet such constraints, and that any degradation was caught early and could be resolved or at the very least communicated to the client.\nGrafana k6 Straight from the opening blurb of the website, k6 sounded like it would be the right tool for the job.\nGrafana k6 is an open-source, developer-friendly, and extensible load testing tool. k6 allows you to prevent performance issues and proactively improve reliability.\nIt landed on all of the features I needed and more, and the documentation is superb, clearly explaining all the features with plenty of examples. k6 also provides a list of extensions which looked pretty useful despite not finding myself needing any of them.\nTypes of load tests\nThe k6 documentation talks through the different supported types of tests.\nSmoke tests — minimal load Average-load tests — normal conditions Stress tests — abnormal average Soak tests — reliability and performance over extended periods Spike tests — behavior and survival in cases of sudden increases in activity Breakpoint tests — gradual increase to identify capacity limits Given the context above, much of my focus here was around average-load tests, however it was pleasant to find that through k6\u0026rsquo;s scenarios it would be quite trivial to switch between load test types.\nGetting started Getting up and running with k6 was very straightforward. I\u0026rsquo;m using macOS, so the following commands might need to be adjusted for your system.\nInstall k6 with the following command.\nbrew install k6 You can either create the file yourself or use the following to initialise a script.js file with some helpful commented-out code and explanations.\nk6 new script.js 1 2 3 4 5 import http from \u0026#34;k6/http\u0026#34;; export default function () { http.get(\u0026#34;https://test.k6.io\u0026#34;); } Run the tests!\nk6 run script.js Adding virtual users The number of virtual users and duration can be set via the command line, or via an exported options in script.js.\nk6 run --vus 10 --duration 30s script.js script.js 1 2 3 4 5 6 7 8 9 10 import http from \u0026#34;k6/http\u0026#34;; export const options = { vus: 10, duration: \u0026#34;30s\u0026#34;, }; export default function () { http.get(\u0026#34;https://test.k6.io\u0026#34;); } The documentation covers some fine-tuning for different operating systems. Using macOS I didn\u0026rsquo;t run into any of the problems mentioned in the docs, however I did encounter a persistent \u0026ldquo;connection reset by peer\u0026rdquo; error when trying to use a high number of virtual users.\nRunning sudo sysctl -w kern.ipc.somaxconn={DESIRED_VALUE} with an increased value allowed me to run previously failing tests with far higher concurrent users.\nAdding typescript Fortunately we do have access to some typings to allow us to use typescript while building our test file(s).\nnpm init npm install -D @types/k6 Which allows us to import and use types such as the following. Don\u0026rsquo;t forget to rename the file to *.ts!\nscript.ts 1 2 3 4 5 6 import { Options } from \u0026#34;k6/options\u0026#34;; export const options: Options = { vus: 10, duration: \u0026#34;30s\u0026#34;, }; At this point, attempting to run the tests will lead to an error. We need to use k6\u0026rsquo;s experimental enhanced compatibility mode. This can be supplied as a flag via command line, or we can add an npm script to our newly-created package.json.\npackage.json 1 2 3 \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;k6 run script.ts --compatibility-mode=experimental_enhanced\u0026#34; } Test output There are multiple outputs when running tests with k6 which can be explored here. The most useful of these I found was the web dashboard and generated HTML report, however there are also options to export a summary as CSV or JSON, or stream metrics to multiple services.\nThere is also an options to export a handleSummary function which can be used to customise the output of a test run.\nWeb dashboard\nThe bundled web dashboard was very handy, and can also be used to generate a report.\nTo launch the dashboard, we simply need to set the K6_WEB_DASHBOARD environment variable to true. There are options to change the port and such, but for the sake of brevity we\u0026rsquo;ll just use the defaults which will serve the dashboard at http://localhost:5665.\nK6_WEB_DASHBOARD=true k6 run script.js Setting K6_WEB_DASHBOARD_OPEN=true will also cause the browser window to be opened automatically. From this page it is possible to save the report manually, although the likely more useful option — especially for CI/CD etc. is to simply tell k6 to output the report for us. Set K6_WEB_DASHBOARD_EXPORT to the name of the file you wish to output.\nK6_WEB_DASHBOARD=true K6_WEB_DASHBOARD_EXPORT=\u0026#34;report.html\u0026#34; k6 run script.js Features Below are some of the most prominent features. More information can be found in the k6 documentation.\nOptions Options can be provided to k6 via different means. Below is the priority order should multiple sources be provided. The more interesting thing to note is the ability to specify a --config flag which targets a file, allowing us to switch predefined options files easily.\nDefault values Configuration file specified via the --config flag Script values Environment variables Command line flags Test lifecycle k6 executes the test script in the following order.\nThe optional setup function runs, setting up the test environment and generating data The required default or scenario function, running for as long and as many times as the options define The optional teardown function runs, cleaning up the test environment Data created in the setup function can be accessed via a data parameter in the default and teardown functions.\nscript.js 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import http from \u0026#34;k6/http\u0026#34;; export function setup() { const res = http.get(\u0026#34;https://httpbin.test.k6.io/get\u0026#34;); return { data: res.json() }; } export function teardown(data) { console.log(JSON.stringify(data)); } export default function (data) { console.log(JSON.stringify(data)); } Scenarios Scenarios give us the flexibility to mimic more realistic usage through more granular control over virtual users (VUs) and iteration scheduling patterns. Scenarios are defined in the options object, and can be used in place of the otherwise required default function.\nscript.js 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 export const options = { scenarios: { shared_iter_scenario: { executor: \u0026#34;shared-iterations\u0026#34;, vus: 10, iterations: 100, startTime: \u0026#34;0s\u0026#34;, }, per_vu_scenario: { executor: \u0026#34;per-vu-iterations\u0026#34;, vus: 10, iterations: 10, startTime: \u0026#34;10s\u0026#34;, }, }, }; export default function () { http.get(\u0026#34;https://test.k6.io/\u0026#34;); } Executors\nThe VU workload for each scenario is scheduled by an executor, which configure how long the test runs for, and how workload is managed.\nValue Description shared-iterations Fixed iterations shared between VUs per-vu-iterations Each VU executes an exact number of iterations constant-vus Fixed VUs execute as many iterations as possible for an amount of time ramping-vus Variable VUs execute as many iterations as possible for an amount of time constant-arrival-rate Fixed iterations are executed in a specified period of time ramping-arrival-rate Variable iterations are executed in a specified period of time Thresholds Thresholds allow us to define parameters that we can measure metrics against. Some examples of thresholds that can be defined are as follows. These can be incredibly useful in scenarios where we may want to either raise an alert against a live environment or fail a CI build. If any of the thresholds fail, k6 will exit with a non-zero exit code.\nLess than 1% of requests return an error 95% of requests have a response time below 200ms 99% of requests have a response time below 400ms A specific endpoint always responds within 300ms script.js 1 2 3 4 5 6 7 8 9 10 11 12 import http from \u0026#34;k6/http\u0026#34;; export const options = { thresholds: { http_req_failed: [\u0026#34;rate\u0026lt;0.01\u0026#34;], // http errors should be less than 1% http_req_duration: [\u0026#34;p(95)\u0026lt;200\u0026#34;], // 95% of requests should be below 200ms }, }; export default function () { http.get(\u0026#34;https://test-api.k6.io/public/crocodiles/1/\u0026#34;); } Metrics Metrics measure how a system performs under test conditions. Metrics fall into four broad types:\nCounters — sum values Gauges — track the smallest, largest, and latest values Rates — track how frequently a non-zero value occurs Trends — calculates statistics for multiple values (mean, mode or percentile) Built-in metrics\nk6 includes many built-in metrics. The list is quite long, so it\u0026rsquo;s worth a skim through to learn what data is being collected by default.\nExample built-in metrics:\nhttp_reqs — total requests http_req_failed — error rate http_req_duration — request duration Custom metrics\nCustom metrics can be used to capture additional data points not covered by the built-in metrics. Below is an example of creating a custom metric.\nscript.js 1 2 3 4 5 6 7 8 9 10 import http from \u0026#34;k6/http\u0026#34;; import { Trend } from \u0026#34;k6/metrics\u0026#34;; const myTrend = new Trend(\u0026#34;waiting_time\u0026#34;); export default function () { const r = http.get(\u0026#34;https://httpbin.test.k6.io\u0026#34;); myTrend.add(r.timings.waiting); console.log(myTrend.name); } Which will output something similar to the below.\niteration_duration.............: // omitted iterations.....................: // omitted waiting_time...................: avg=265.245396 min=265.245396 med=265.245396 max=265.245396 p(90)=265.245396 p(95)=265.245396 Checks Crossing more into the realm of functional testing, checks allow us to provide some assertions over the functionality of our tests. Failed checks do not directly result in a failed test, however they do produce metrics which can be combined with thresholds.\nWhile there is a lot of potential crossover with functional testing, checks may be worth considering as a way to ensure that our tests are actually testing the whole functionality of an endpoint. For example, should we have a long-running database query triggered by an endpoint, we may wish to ensure that the HTTP request is actually performing the database call. If the database doesn\u0026rsquo;t exist, we may find that an error is returned very quickly and without a check we may assume that the query ran to completion in under our threshold.\nCategorisation To help you visualize, sort, and filter your test results, k6 provides tags and groups as ways to categorise entities.\nTags\nTags quite simply allow us to categorise the following entities. Some of these tags are built-in and applied by default, however custom tags can easily be applied where required. It is also possible to tag an entire test, enabling comparing test results from across separate test runs.\nRequests Checks Thresholds Custom metrics With tags applied, the output of our test run looks something like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;type \u0026#34;: \u0026#34;Point \u0026#34;, \u0026#34;data \u0026#34;: { \u0026#34;time \u0026#34;: \u0026#34;2017-05-09T14:34:45.239531499+02:00 \u0026#34;, \u0026#34;value \u0026#34;: 459.865729, \u0026#34;tags \u0026#34;: { \u0026#34;group \u0026#34;: \u0026#34;::my group::json \u0026#34;, \u0026#34;method \u0026#34;: \u0026#34;GET \u0026#34;, \u0026#34;status \u0026#34;: \u0026#34;200 \u0026#34;, \u0026#34;url \u0026#34;: \u0026#34;https://httpbin.test.k6.io/get \u0026#34; } }, \u0026#34;metric \u0026#34;: \u0026#34;http_req_duration \u0026#34; } Groups\nGroups allow us to organise a load script by functions. These can be nested, and the test output will have a tag called group with all wrapping group names.\nscript.js 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import { group } from \u0026#34;k6\u0026#34;; export default function () { group(\u0026#34;visit product listing page\u0026#34;, function () { // ... }); group(\u0026#34;add several products to the shopping cart\u0026#34;, function () { // ... }); group(\u0026#34;visit login page\u0026#34;, function () { // ... }); group(\u0026#34;authenticate\u0026#34;, function () { // ... }); group(\u0026#34;checkout process\u0026#34;, function () { // ... }); } Misc k6 offers a collection of other integrations and tools, including links to examples or guides on things such as setting up a CI/CD pipeline with k6.\nOpenAPI integration While probably not the most useful feature in practise, this may have some usefulness in very quickly setting up a test file as a starting point. This article goes into more depth, but as a quick reference the following will generate a test script. However, the script itself will need to be cleaned up prior to being run.\nPull the container down.\ndocker pull openapitools/openapi-generator-cli Run the container, substituting the -i and -o flags for your respective input and output paths.\ndocker run --rm -v \u0026#34;${PWD}:/local\u0026#34; openapitools/openapi-generator-cli generate \\ -i https://httpbin.test.k6.io/spec.json \\ -g k6 \\ -o /local/k6-test/ \\ --skip-validate-spec Generating swagger.json\nI was using a .NET Core API project, and to generate the swagger.json file the following steps were required.\ndotnet tool install swashbuckle.aspnetcore.cli dotnet swagger tofile --output {OUTPUT_PATH} \u0026#34;{PATH_TO_PROJECT}/bin/Debug/net8.0/{PROJECT_NAME}.dll\u0026#34; v1 Test authoring tools While these looked promising I didn\u0026rsquo;t look at either beyond reading the article, preferring to simply write the test scripts myself.\nTest builder Browser recorder Sample repository There is a very light sample repository available on GitHub which includes a sample of the generated HTML report.\n","date":"2024-10-20T00:00:00Z","image":"https://vivecuervo7.github.io/dev-blog/p/performance-testing-k6/cover_hu_a359f691178bc493.jpg","permalink":"https://vivecuervo7.github.io/dev-blog/p/performance-testing-k6/","title":"Performance testing with Grafana k6"},{"content":"What\u0026rsquo;s the issue? After working on a number of software projects, I have grown increasingly disenchanted with the amount of time spent arguing politely discussing the intricacies of a 3-point story compared to a 5-point story.\nWhat\u0026rsquo;s the difference between a 2-point story and a 3-point story? Why is it different? Does such a trivial difference really matter?\nI wanted to explore what a simplified model of estimations might look like, which more clearly conveys whether a thing is a typical piece of work, or whether it\u0026rsquo;s bigger or smaller.\nCounting stories While certainly not the norm, I\u0026rsquo;ve worked on a project in the past where we simply treated every story as a single point.\nGranted, it was a short, sharp project with a small team of two, and we didn\u0026rsquo;t really have the time to get into debate over what each story point meant. This led to a pleasantly simple world, in which we just added stories and worried about finishing them.\nEven with the short time frame, by the end of the project we were left with the reflection that there were a couple of stories that might have been better communicated as a larger task than the others, but overall the process worked for us.\nFibonacci The majority of projects I\u0026rsquo;ve been involved with to date have all used the Fibonacci sequence as our story point scale. I have never found this disagreeable, as it gives us a good idea of the relative complexity of our work items.\nTypically, without poring through the entire backlog ahead of time we would aim to estimate the higher priority or more well-known work items, deferring estimation of the remainder until we got closer to them or the story was more fleshed out. As far as burn-up charts are concerned, all of the not-yet-estimated work items would simply reflect an average of the estimated backlog\u0026rsquo;s points.\nThis works, and tends to work well.\nIt also takes time. Even on a smaller team, varying levels of experience with a technology will likely mean that one person\u0026rsquo;s 3 might be another person\u0026rsquo;s 5. On the lower end, if I\u0026rsquo;ve had a good week and I\u0026rsquo;m on a roll, those 3\u0026rsquo;s start looking an awful lot like 2\u0026rsquo;s.\nThen there\u0026rsquo;s a whole round of discussion required, digging through why we\u0026rsquo;re estimating the way we are, how it\u0026rsquo;s justified based on last week\u0026rsquo;s estimations, and then possibly another round of estimations. Granted, these discussions are often useful for uncovering some potential technical complexities, so they\u0026rsquo;re not a total loss.\nIs there a middle ground? After experiencing the two approaches outlined above, it wasn\u0026rsquo;t hard to realise that much of the angst should be levelled at the scale we\u0026rsquo;re using to estimate stories, rather than the estimation process as a whole. After all, we still need something to communicate our progress to stakeholders, and we needed a way to assign those points.\nAnd so, much of my thoughts started leaning into the idea that we could simplify things down to a typical user story, and using a smaller set of numbers to indicate whether the thing was bigger or smaller.\n1, 2 and 3.\nHow would this work? Where the single-point approach was dead simple and just required adding a point for each story, the broader scale of the Fibonacci sequence meant that quite often we go into estimation without much of an idea of that story\u0026rsquo;s complexity as far as points are concerned. We would run through the acceptance criteria and individually vote for where we think the story points should land, taking the majority number or discussing and re-estimating.\nHow I would envision a simpler scale to work would be that every story is by default a 2 at the moment estimation for that story begins. Thus, the question becomes \u0026ldquo;does anybody think this work item is significantly more or less complex than a typical work item?\u0026rdquo;.\nBased on the answer and ensuing discussion, it would either be changed to a 1 to represent a smaller or easier piece of work, and a 3 where the work was determined to be more complex.\nWhile not yet used in practice, this feels like a much easier question to answer. And sure, there might still be disagreements — but it narrows down any discussion to only the stories which somebody has called out as easier or harder.\nA real-world analysis After briefly talking over this idea with a couple of people, I decided it would be worth looking at some real-world data and seeing what a burn-up chart would look like if it were re-mapped to the simplified scale.\nCounts\nTo do this, I essentially counted all of the completed work items and grouped them by their story points. Further consolidating them into groups of \u0026rsquo;less than 3\u0026rsquo;, \u0026lsquo;between 3 and 8\u0026rsquo;, and \u0026lsquo;more than 8\u0026rsquo; gave me the most accurate simplified grouping.\nValues\nSatisfied with the overlap between the original and simplified counts, I multiplied the counts by their respective story points and graphed those similarly — including what the numbers look like when re-mapped to the proposed 1-2-3 scale.\nThe resulting curve was fairly standard and further reinforced that the \u0026ldquo;typical\u0026rdquo; story worth the proposed 2 points would fairly cleanly overlap with what we would otherwise have estimated at between 3 and 8 points. The lack of resolution made it difficult to say whether or not this would pan out in practice, but things still lined up well enough to keep digging.\nBurn-up chart\nLooking to provide some more useful evidence, I also wanted to include the big thing that we use to communicate progress to our stakeholders — the burn-up chart.\nA relatively simple experiment to set up, I took the previous mappings and filled in our usual burn-up data with the original values, and then compared them to what the curve would look like for the re-mapped values.\nAt a glance these look pretty similar, with the differences only really noticeable when overlaid on top of each other.\nWhere I did notice a significant discrepancy was when looking at projections based on the preceding 5-week period. While the dotted projection line observed a like-for-like movement, the lower numbers in general led to a more dire forecast. I\u0026rsquo;ve included a projection based on the single-point method, and the re-mapped values are clearly a better middle ground.\nCan the projection be fixed?\nKind of. I did fiddle with the numbers a bit, and was able to find a model that mapped pretty well for this project. The following changes gave me a significantly more accurate projection and curve, but they came at the cost of simplicity.\nChanged the grouping to \u0026rsquo;less than 3\u0026rsquo;, \u0026lsquo;between 3 and 5\u0026rsquo;, and \u0026lsquo;more than 5\u0026rsquo; Made the \u0026ldquo;bigger\u0026rdquo; value worth 5 points instead of 3 Most concerning was the required change to grouping. The change in complexity to move something either side of \u0026ldquo;typical\u0026rdquo; no longer felt balanced, and what would have likely been estimated as an 8 might still be left as a 2 under the proposed model, where for the more accurate projection it would need to have been moved to a 3.\nGiven the additional complexity and the original ideal of a process which alleviated much of the friction while maintaining better accuracy than the single-point method, I don\u0026rsquo;t think it\u0026rsquo;s worth trying to fiddle with the numbers here.\nConclusion There\u0026rsquo;s a pretty big question sitting front and centre — would I use it?\nI think so. I\u0026rsquo;d like to compare burn-up charts for a couple of other projects — specifically the projections around the 30-40% mark. The other real-world backlog I had at my disposal was similarly mapped and shows a better outlook for simplified approaches to estimation.\nThat said, I did also map the burn-up chart using the single-point-per-story method, and the projection discrepancy was even more pronounced — and I have seen this method used successfully, although the shorter engagement it was used for may have obscured any problems that might have arisen over a longer period.\nProjections for other projects Another real-world project with their projections compared, with much more accurate projections than the first project. That said, the mapping for the single-point method looked pretty much on the money as well.\n","date":"2024-10-14T00:00:00Z","image":"https://vivecuervo7.github.io/dev-blog/p/remapping-our-estimations/cover_hu_fe174447623906fa.jpg","permalink":"https://vivecuervo7.github.io/dev-blog/p/remapping-our-estimations/","title":"Re-mapping our estimations"},{"content":"A brief discussion with a client recently reminded me of something I\u0026rsquo;d long wanted to look into. The conversation was around the consolidation of designs across their rather broad suite of applications, and given the frequency with which React was used it got me wondering if it might be worth looking into creating a reusable component library.\nPlenty of resources popped up when I started looking into the topic, and ultimately I discovered two promising avenues:\nRollup Vite (or more specifically, Vite\u0026rsquo;s Library Mode) Of the above options, Rollup seemed to be the more \u0026ldquo;traditional\u0026rdquo; approach and so that\u0026rsquo;s where I started.\nRollup This post by Alex Eagleson was instrumental in helping me get my head around what needed to be done. There\u0026rsquo;s also a fantastic accompanying video tutorial linked on his post.\nThe tutorial is a little outdated so not all of the steps work as described, however there is a good amount of discussion in the comments with updated instructions. In any case, it proved to be informative enough to get me started in building a component library.\nHopefully it saves any pain, but one thing that caught me was a change to @rollup/plugin-typescript in version 12 which was causing errors, especially when trying to create multiple outputs. Rolling back to version 11 restored the original functionality and allowed me to progress with the tutorial.\nThis resource was incredibly helpful in introducing me to the core concepts of how to bundle and publish a component library, and I highly recommend reading through it for anyone unfamiliar with Rollup and how to use it in this context.\nVite After running through the tutorial above and using the resulting library\u0026rsquo;s simple button component in another React application, I decided to look at Vite before continuing.\nOf the resources I used, this post by Víctor Lillo proved to be the most complete as it covered all of the aspects I wanted to look at.\nInitially I disliked this approach due to the need to create a full-blown React application and subsequently remove all the bits we didn\u0026rsquo;t need, however it was still relatively painless and came with a few nice things working straight out of the box — CSS as an example.\nExporting components I did prefer the way components were exported in the first tutorial I followed, so I stuck with that here. This approach used an explicit index.ts file at each level of the hierarchy.\nsrc/components/Button/index.ts 1 export { default } from \u0026#34;./Button\u0026#34;; src/components/index.ts 1 export { default as Button } from \u0026#34;./Button\u0026#34;; src/index.ts 1 export * from \u0026#34;./components\u0026#34;; Library mode The initial config required on the Vite side was relatively straightforward. Setting the required values for build.lib under vite.config.ts is all we needed, where we set our entry point.\nvite.config.ts 1 2 3 4 5 6 7 8 9 10 11 12 13 14 /// \u0026lt;reference types=\u0026#34;vite/client\u0026#34; /\u0026gt; import { resolve } from \u0026#34;node:path\u0026#34;; import { defineConfig } from \u0026#34;vite\u0026#34;; import react from \u0026#34;@vitejs/plugin-react\u0026#34;; export default defineConfig({ plugins: [react()], build: { lib: { entry: resolve(__dirname, \u0026#34;src/index.ts\u0026#34;), formats: [\u0026#34;es\u0026#34;], }, }, }); Dependencies Another important piece of configuration here is to make sure we\u0026rsquo;re not bundling up a bunch of dependencies that we don\u0026rsquo;t need, such as React itself. We do this by extending vite.config.ts to include build.rollupOptions under both external and output.globals.\nvite.config.ts 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //... export default defineConfig({ plugins: //..., build: { lib: //..., rollupOptions: { external: [\u0026#34;react\u0026#34;, \u0026#34;react-dom\u0026#34;, \u0026#34;react/jsx-runtime\u0026#34;], output: { globals: { react: \u0026#34;React\u0026#34;, \u0026#34;react-dom\u0026#34;: \u0026#34;React-dom\u0026#34;, \u0026#34;react/jsx-runtime\u0026#34;: \u0026#34;react/jsx-runtime\u0026#34;, }, }, }, }, }); In my particular case, I was using classnames as a dependency for some styling, and ran into a few options when it came to ensuring any consumers of the library would have all the required dependencies.\nBundled with the application\nLeaving classnames under devDependencies in package.json and simply having referenced it in the bundled code meant that classnames itself would also be bundled into the library itself. While there may be some cases where this makes sense, if the consuming application was using classnames already then we\u0026rsquo;ve essentially forced them to bundle the code in their application twice.\nMaking it a regular dependency\nMoving classnames into the dependencies list in package.json might work, but that would lead to the consuming application needing to unnecessarily include classnames as a runtime dependency.\nPeer dependencies\nHaving not seen this prior to the covered tutorials, it took some reasoning to understand what this was used for. Essentially, moving classnames into the peerDependencies would mean that it was a requirement of the library that the consuming application must have a matching version of the package installed. Thus, some leniency was required when specifying the version.\nInstallation was a concern that crossed my mind, however as of npm 7 peer dependencies are installed by default, meaning that a consumer need not go and manually install the peer dependencies themselves.\nUltimately I went with the peer dependencies approach as it seemed to have the fewest drawbacks.\nEntry points and CSS As mentioned in the tutorial, style sheets aren\u0026rsquo;t automatically imported in the generated code and thus the consumer needs to import it themselves manually. This can be resolved by using the vite-plugin-lib-inject-css plugin. Once installed, it needs to be added to vite.config.ts under plugins.\nThis fixes our issue, however we now have a single import statement in our generated index.js file, meaning the entire style sheet needs to be imported if we use even a single component from our library. Rollup recommends that we instead turn every file into an entry point, which will result in individual CSS files for each component — allowing us to import and use a single component, and only require that component\u0026rsquo;s style sheet.\nAdding the following to build.rollupOptions allows us to generate individual files for each component. The addition to build.rollupOptions.output is also necessary to retain our folder structure in the generated code.\nvite.config.ts 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 export default defineConfig({ build: { rollupOptions: { + input: Object.fromEntries( + glob + .sync(\u0026#39;src/**/*.{ts,tsx}\u0026#39;) + .map((file: string) =\u0026gt; [ + path.relative(\u0026#39;src\u0026#39;, file.slice(0, file.length - path.extname(file).length)), + fileURLToPath(new URL(file, import.meta.url)) + ]) + ), output: { + entryFileNames: \u0026#39;[name].js\u0026#39;, + assetFileNames: \u0026#39;assets/[name][extname]\u0026#39; }, }, }, }) This will result in the dist/assets/ folder containing a CSS file for each component, which are imported accordingly.\nType generation vite-plugin-dts is the plugin required to generate our type declarations. Similar to vite-plugin-lib-inject-css, install it and add it to the plugins array in vite.config.ts.\nAgain bridging the two tutorials, I preferred the single-file approach taken by the first tutorial, and as such I added the plugin with the option rollupTypes set to true.\nAssuming the rollupTypes option was enabled, the generated code should now contain an index.d.ts file with all of the types declared within it.\nSetting up package.json The other important file that requires some changes is the package.json file. Add or update the following fields.\npackage.json 1 2 3 4 5 6 { + \u0026#34;type\u0026#34;: \u0026#34;module\u0026#34;, + \u0026#34;files\u0026#34;: [\u0026#34;dist\u0026#34;], + \u0026#34;module\u0026#34;: \u0026#34;dist/index.js\u0026#34;, + \u0026#34;types\u0026#34;: \u0026#34;dist/index.d.ts\u0026#34; } Some more information regarding the new or updated fields:\ntype: should be set to module. to indicate that we\u0026rsquo;re using ES module syntax files: describes the files to be included when the package is published module: not an official Node feature, but supported by some bundlers types: exposes the type declarations entry point exports: Optional the entry points to the library main: This is used to specify the entry point for cjs, which we\u0026rsquo;re not supporting Life cycle scripts\nA script is also useful to specify here. While using prepublishOnly makes sense if we\u0026rsquo;re planning to publish the library via npm publish, using prepare will run both on npm publish and npm install which allows us to install the library locally.\nAdd the following to package.json under scripts.\npackage.json 1 2 3 4 5 { \u0026#34;scripts\u0026#34;: { + \u0026#34;prepare\u0026#34;: \u0026#34;vite build\u0026#34; } } More information on these \u0026ldquo;life cycle scripts\u0026rdquo; can be found here.\nDevelopment The following are simply some additions to the development tooling for the library itself. This won\u0026rsquo;t be a guide on how to use any of the tooling, but simply provides some basic installation steps and ensuring that the files are not bundled into our generated library code.\nStorybook Storybook can be installed by running the following command.\npnpm dlx storybook@latest init The src/stories/ folder can be removed if desired as it only contains some sample stories and documentation.\nStories can now be added for any of our components — documentation on how to do so can be found here. Once stories have been added, run Storybook with the following command.\npnpm storybook Ignoring Storybook files\nThe last thing we need to do here is to ensure that we aren\u0026rsquo;t bundling our stories in the generated code. We do this by extending the glob.sync command we added to build.rollupOptions.input in vite.config.ts and providing an ignore field to the options as follows.\nvite.config.ts 1 glob.sync(\u0026#34;src/**/*.{ts,tsx}\u0026#34;, { ignore: [\u0026#34;src/**/*.stories.{ts,tsx}\u0026#34;] }); Vitest Vitest, jsdom and the React Testing Library (we\u0026rsquo;ll need all three) can be installed with the following command.\npnpm i -D vitest jsdom @testing-library/react Add a test script to package.json.\npackage.json 1 2 3 4 5 { \u0026#34;scripts\u0026#34;: { + \u0026#34;test\u0026#34;: \u0026#34;vitest\u0026#34; } } Next we need to update our vite.config.ts file.\nvite.config.ts 1 2 3 4 5 6 7 8 +/// \u0026lt;reference types=\u0026#34;vitest\u0026#34; /\u0026gt; export default defineConfig({ + test: { + environment: \u0026#34;jsdom\u0026#34;, + globals: true, + root: \u0026#34;src/\u0026#34;, + }, }); And to get those globals working nicely so we don\u0026rsquo;t need to repeatedly import describe, test etc. we need to add the following to tsconfig.json under compilerOptions.\ntsconfig.json 1 2 3 4 5 { \u0026#34;compilerOptions\u0026#34;: { + \u0026#34;types\u0026#34;: [\u0026#34;vitest/globals\u0026#34;] } } Ignoring test files\nSimilar to Storybook, we also need to make sure we\u0026rsquo;re not generated code for our tests. Update the glob.sync command in vite.config.ts under build.rollupOptions.input.\nvite.config.ts 1 2 3 glob.sync(\u0026#34;src/**/*.{ts,tsx}\u0026#34;, { ignore: [\u0026#34;src/**/*.stories.{ts,tsx}\u0026#34;, \u0026#34;src/**/*.test.{ts,tsx}\u0026#34;], }); Using the library Both of the linked tutorials go into publishing the library on npm, however due to the nature of a sample library I didn\u0026rsquo;t want to delve into the publishing side of things.\nHowever, outside of publishing it on npm I was unsure as to how I could actually use the library, and to my pleasure it was actually incredibly straightforward.\nGithub repository\nVery simple, this allows you to install directly from the repository using the following command.\npnpm i -D GITHUB_NAME/REPOSITORY_NAME Which results in the following package.json entry under devDependencies.\npackage.json 1 2 3 { \u0026#34;PACKAGE_NAME\u0026#34;: \u0026#34;github:GITHUB_NAME/REPOSITORY_NAME\u0026#34; } Local reference\nEqually as straightforward, the following command can be run to create a local reference.\npnpm i -D PATH_TO_LIBRARY Which similar to the above results in the following package.json entry under devDependencies.\npackage.json 1 2 3 { \u0026#34;PACKAGE_NAME\u0026#34;: \u0026#34;link:PATH_TO_LIBRARY\u0026#34; } Sample repository I did end up with a functional component library — albeit one with all of two components and a custom hook. It\u0026rsquo;s available on GitHub here, or it can be used as above with the following command.\npnpm i -D vivecuervo7/demolib ","date":"2024-10-08T00:00:00Z","image":"https://vivecuervo7.github.io/dev-blog/p/react-component-library-with-vite/cover_hu_12388447993878a8.jpg","permalink":"https://vivecuervo7.github.io/dev-blog/p/react-component-library-with-vite/","title":"Creating a React component library with Vite"},{"content":"Keycloak is an open-source identity and access management solution designed to manage users and secure applications. It provides features such as single sign-on (SSO), social login integration, and user federation.\nIt also offers a user-friendly administration console for managing users, roles, and permissions, as well as customizable user interfaces for login, registration, and account management\nWhy use a local auth server?\nBeing able to run a local auth server means we won\u0026rsquo;t find ourselves blocked while waiting for a client to configure their own auth server. Often picking up a piece of work involving client-owned authentication results in some delays while requests are sent around, and more often than not leaves us unable to write our application code until the auth server has been configured.\nIt also allows us to have full control over our own local dev environment, including the ability to create any number of users with specific roles. This means we can easily run multi-user scenarios locally for testing, demoes or debugging, and prevents needing to comment out or tweak application code just to replicate a bug.\nGetting started To start the Keycloak dev server, simply run the following command in your terminal. This will expose Keycloak on port 8080, and creates an initial admin user with the credentials admin:admin.\ndocker run -p 8080:8080 -e KEYCLOAK_ADMIN=admin -e KEYCLOAK_ADMIN_PASSWORD=admin quay.io/keycloak/keycloak:25.0.2 start-dev Opening http://localhost:8080/ in your browser should show you the login page for the Keycloak admin interface. Sign in with the previously created admin credentials admin:admin. Once logged in, you should be greeted with the Keycloak admin console.\nConfiguring Keycloak Creating a realm Open the admin console and login using our admin credentials admin:admin Open the dropdown that currently says Keycloak and select Create realm Enter the desired name for the realm, in this case using local-dev Finish creating the realm by clicking the \u0026ldquo;Create\u0026rdquo; button Adding a user Select Users from the sidebar, then click Create new user Fill in the details for a new user, in this case using test-user as the username (Optional) ticking Email verified means our test user can skip this step Select Credentials from the tab bar, then click Set password Enter and confirm the user\u0026rsquo;s password, in this case using password (Optional) uncheck Temporary to prevent needing to update the password at first login for this user You can test the newly created user\u0026rsquo;s account at http://localhost:8080/realms/myrealm/account. Opening the link and signing in with our user\u0026rsquo;s credentials test-user:password gives us access to a user-facing interface that allows updating of details, passwords, and the setting up of two-factor authentication.\nRegistering a client Select Clients in the sidebar, then click Create client Make sure to leave OpenID Connect selected as the Client type Enter the desired client name, in this case using local-dev-client Click Next Ensure that Standard flow remains checked Click Next To facilitate testing, make the following changes to the Login settings Set Valid redirect URIs to https://www.keycloak.org/app/* Set Web origins to https://www.keycloak.org Click Save You can test the newly created client by using https://www.keycloak.org/app/. Enter the details from our previously created realm and client, and press Save. Once saved, you can sign in with the test user created earlier.\nAudience claim You may also need to set the audience claim correctly, which might become apparent when wiring this up to an application.\nSelect Clients in the sidebar, select our newly created client local-dev-client, then select Client scopes from the tabs along the top Select the Assigned client scope named \u0026lt;client-id\u0026gt;-dedicated, in our case local-dev-client-dedicated Click the Add mapper button, and select By configuration Select Audience from the list of options, and then configure as per below Name can be whatever you choose, I\u0026rsquo;ve gone with audience here Selecting Included Client Audience will open a dropdown; select the newly created client local-dev-client This should finish configuring our client to correctly populate the audience claim in any generated tokens.\nPersisting Keycloak configuration Of course, once we tear down our Docker container we lose any of the previously configured values. Docker volumes provide a nice solution here, which we can easily configure using docker-compose.\nSave the following as docker-compose.yml and run by calling docker-compose up -d in your terminal.\ndocker-compose.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 services: keycloak: image: quay.io/keycloak/keycloak:25.0.2 container_name: keycloak ports: - 8080:8080 environment: - KEYCLOAK_ADMIN=admin - KEYCLOAK_ADMIN_PASSWORD=admin volumes: - keycloak-data:/opt/keycloak/data/ restart: always command: - \u0026#34;start-dev\u0026#34; volumes: keycloak-data: name: keycloak-data ","date":"2024-08-05T00:00:00Z","image":"https://vivecuervo7.github.io/dev-blog/p/keycloak-setup/cover_hu_3430c052d8d3ec5d.jpg","permalink":"https://vivecuervo7.github.io/dev-blog/p/keycloak-setup/","title":"Setting up Keycloak"},{"content":"Keycloak allows us to import and export realms, which can make it much easier to share configurations amongst team members.\nExporting an existing realm The following instructions to export a realm from Keycloak will assume the use of a docker compose file similar to this.\ndocker-compose.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 services: keycloak: image: quay.io/keycloak/keycloak:25.0.2 container_name: keycloak ports: - 8080:8080 environment: - KEYCLOAK_ADMIN=admin - KEYCLOAK_ADMIN_PASSWORD=admin volumes: - keycloak-data:/opt/keycloak/data/ restart: always command: - \u0026#34;start-dev\u0026#34; volumes: keycloak-data: name: keycloak-data The easiest way to export a realm when using docker compose is to add a second compose file. Call this docker-compose.export.yml.\ndocker-compose.export.yml 1 2 3 4 5 services: keycloak: command: \u0026#34;export --dir /opt/keycloak/data/export/ --realm local-dev --users realm_file\u0026#34; volumes: - ./output:/opt/keycloak/data/export Run the following in your terminal to export the configured realm.\ndocker-compose -f \u0026#34;docker-compose.yml\u0026#34; -f \u0026#34;docker-compose.export.yml\u0026#34; up --exit-code-from keycloak You should now have a directory called output that contains a file called local-dev-realm.json. This file can be imported manually when creating a new realm, or Keycloak can be configured to automatically import this realm when the service starts (attempting to import an already-existing realm will fail to prevent overwrites).\nAn important caveat to note is that Keycloak is designed to export from a stopped server, meaning you will need to ensure that your configuration has been persisted through some means.\nKeycloak also allows for multiple options when it comes to if and how users should be exported. The example above uses the simpler approach of combining them into the realm file. See the Keycloak documentation for more details.\nImporting a realm from a file The updated Docker compose file below uses a bind mount to an import directory. Once any realm.json files have been exported, placing the files in the import directory will allow Keycloak to automatically pick those realms up and import them on first run.\nUpdate your docker-compose.yml to the following and run by calling docker-compose up -d in your terminal.\ndocker-compose.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 services: keycloak: image: quay.io/keycloak/keycloak:25.0.2 container_name: keycloak ports: - 8080:8080 environment: - KEYCLOAK_ADMIN=admin - KEYCLOAK_ADMIN_PASSWORD=admin volumes: - ./import:/opt/keycloak/data/import restart: always command: - \u0026#34;start-dev\u0026#34; - \u0026#34;--import-realm\u0026#34; Note the removal of the persisted Docker volume — this effectively gives us an auth server that can be modified on the fly, but will reset back to the exported realm whenever it restarts.\nAs Keycloak won\u0026rsquo;t overwrite an existing realm with the import method, the Docker volume can always be reintroduced and will essentially mean our import file serves as a starting point upon which changes can be persisted.\n","date":"2024-08-05T00:00:00Z","image":"https://vivecuervo7.github.io/dev-blog/p/keycloak-import-export/cover_hu_3430c052d8d3ec5d.jpg","permalink":"https://vivecuervo7.github.io/dev-blog/p/keycloak-import-export/","title":"Importing and exporting Keycloak configuration"},{"content":"One of my main motivations for looking into Keycloak was to decouple local development from a third party authentication server which needed to be configured by the client. More often than not, this is done using Microsoft Entra ID, and in a typical React application we would use MSAL to set up client authentication.\nThis serves as a short guide to resolving some of the issues encountered when trying to use MSAL with a locally-configured Keycloak server.\nSee the example repo here.\nRunning Keycloak with HTTPS Keycloak will be exposed at http://localhost:8080 which is all well and good for most cases, however I was wanting to drop this in as a local auth replacement for MSAL in a typical React project.\nSince @azure/msal-browser doesn\u0026rsquo;t allow us to use a HTTP authority, the default Keycloak endpoint won\u0026rsquo;t work. The following steps allow for running Keycloak with HTTPS.\nCreating a self-signed certificate using dotnet dev-certs Since dotnet-certs are typically used for local dev when building .NET applications, it seemed easiest to simply repurpose the same tooling to create other local dev certs.\nCreate the self-signed certificate by running the following in your terminal, in this case using password as the credential to create the following two files: certificate.crt and certificate.key.\ndotnet dev-certs https -ep ./certificate.crt -p password --trust --format PEM Use openssl to decrypt the certificate key, overwriting certificate.key with the decrypted copy.\nopenssl rsa -in certificate.key -out certificate.key As a matter of preference, I tend to rename these files to cert.pem and key.pem. The remainder of this guide assumes this naming.\nRunning Keycloak with the certificate The following changes expect that the certificate files we just created are present in a certificates directory. Go ahead and create the folder and copy both the certificate and key into it.\nCreate or update your docker-compose.yml file to the following and run by calling docker-compose up -d in your terminal.\ndocker-compose.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 services: keycloak: image: quay.io/keycloak/keycloak:25.0.2 container_name: keycloak ports: - 8080:8080 - 8443:8443 environment: - KEYCLOAK_ADMIN=admin - KEYCLOAK_ADMIN_PASSWORD=admin volumes: - ./import:/opt/keycloak/data/import - ./certificates:/opt/keycloak/data/certificates restart: always command: - \u0026#34;start-dev\u0026#34; - \u0026#34;--import-realm\u0026#34; - \u0026#34;--https-certificate-file=/opt/keycloak/data/certificates/cert.pem\u0026#34; - \u0026#34;--https-certificate-key-file=/opt/keycloak/data/certificates/key.pem\u0026#34; You can now access the Keycloak server at either http://localhost:8080 or https://localhost:8443. Using the HTTPS endpoint should allow us to use Keycloak with @azure/msal-browser.\nConfiguring MSAL to work with Keycloak Without delving too deep into how we might hold MSAL in a typical React application, the biggest change we need to make is to manually provide some of the configuration that usually works out of the box with MSAL when using Microsoft Entra ID.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 const msalConfig = { auth: { clientId: \u0026#34;local-dev-client\u0026#34;, authority: \u0026#34;https://localhost:8443/realms/local-dev\u0026#34;, knownAuthorities: [\u0026#34;https://localhost:8443/realms/local-dev\u0026#34;], redirectUri: \u0026#34;https://localhost:5173\u0026#34;, postLogoutRedirectUri: \u0026#34;https://localhost:5173\u0026#34;, protocolMode: ProtocolMode.OIDC, authorityMetadata: JSON.stringify({ authorization_endpoint: \u0026#34;https://localhost:8443/realms/local-dev/protocol/openid-connect/auth\u0026#34;, token_endpoint: \u0026#34;https://localhost:8443/realms/local-dev/protocol/openid-connect/token\u0026#34;, issuer: \u0026#34;https://localhost:8443/realms/local-dev\u0026#34;, userinfo_endpoint: \u0026#34;https://localhost:8443/realms/local-dev/protocol/openid-connect/userinfo\u0026#34;, end_session_endpoint: \u0026#34;https://localhost:8443/realms/local-dev/protocol/openid-connect/logout\u0026#34;, }), }, }; const msalInstance = new PublicClientApplication(msalConfig); There are of course a few more data points required, which may then need to be manually provided again when configuring this to work with both Keycloak locally and Microsoft Entra ID when deployed. A small amount of pain to endure for the benefits of decoupling ourselves from a customer-provided auth server.\nOf course, in practice we would make these string more easily configurable, but I\u0026rsquo;ve opted to hardcode them to better demonstrate.\nSee the sample repo to see these changes in more context.\nOpenID Endpoint Configuration\nThe trickier part here might be knowing where to obtain these strings from. Navigating to the realm settings of our Keycloak server\u0026rsquo;s admin interface, find the OpenID Endpoint Configuration link.\nOpening this will yield a new page with all of our endpoints that we need to populate this config.\n1 2 3 4 5 6 7 8 { \u0026#34;issuer\u0026#34;: \u0026#34;https://localhost:8443/realms/local-dev\u0026#34;, \u0026#34;authorization_endpoint\u0026#34;: \u0026#34;https://localhost:8443/realms/local-dev/protocol/openid-connect/auth\u0026#34;, \u0026#34;token_endpoint\u0026#34;: \u0026#34;https://localhost:8443/realms/local-dev/protocol/openid-connect/token\u0026#34;, \u0026#34;userinfo_endpoint\u0026#34;: \u0026#34;https://localhost:8443/realms/local-dev/protocol/openid-connect/userinfo\u0026#34;, \u0026#34;end_session_endpoint\u0026#34;: \u0026#34;https://localhost:8443/realms/local-dev/protocol/openid-connect/logout\u0026#34; // ... } Running React with HTTPS We may also need to run our React application with HTTPS as well. To achieve this, we need to update vite.config.ts to contain the following. Fortunately, we can repurpose the same certificates we created for Keycloak.\nNote that while omitted below, it may be useful to split the serve and build commands so we keep our changes away from any deployed code.\nvite.config.ts 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import { defineConfig, loadEnv } from \u0026#34;vite\u0026#34;; import react from \u0026#34;@vitejs/plugin-react-swc\u0026#34;; export default defineConfig(({ command, mode }) =\u0026gt; { + process.env = { ...process.env, ...loadEnv(mode, process.cwd()) }; return { plugins: [react()], + server: { + https: { + cert: process.env.VITE_CERT ?? \u0026#34;\u0026#34;, + key: process.env.VITE_CERT_KEY ?? \u0026#34;\u0026#34;, + }, + }, }; }); And then we\u0026rsquo;ll also need to add a .env (and .env.local) with the correct paths.\n.env 1 2 + VITE_CERT=../local-dev/certificates/cert.pem + VITE_CERT_KEY=../local-dev/certificates/key.pem Now running the React application with pnpm dev should serve it using HTTPS.\nMissing parameter: id_token_hint I did run into a small issue with the setup, where a user was unable to logout completely. Attempting to logout was yielding an error due to the absence of either a client_id or id_token_hint when requesting the post_logout_redirect_uri.\nUltimately, this was easily resolved by obtaining an access token before attempting to logout, and using the id_token from that.\nmain.tsx 1 2 3 4 5 6 7 8 9 const handleLogout = async () =\u0026gt; { const response = await instance.acquireTokenSilent({ scopes: [\u0026#34;openid\u0026#34;], }); await instance.logoutRedirect({ idTokenHint: response.idToken, }); }; Optional: Adding a secured .NET backend The sample repo contains code that also connects the React application to a .NET API with a secured endpoint.\nThe configuration here is relatively straightforward to work with our local Keycloak server.\nThe main changes we\u0026rsquo;ll need to make are to Program.cs, where we simply add our necessary configuration (truncated for brevity).\nProgram.cs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 + using Microsoft.AspNetCore.Authentication.JwtBearer; var builder = WebApplication.CreateBuilder(args); + builder + .Services.AddAuthentication(options =\u0026gt; + { + options.DefaultAuthenticateScheme = JwtBearerDefaults.AuthenticationScheme; + options.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme; + }) + .AddJwtBearer(options =\u0026gt; builder.Configuration.Bind(\u0026#34;JwtBearerOptions\u0026#34;, options)); + builder + .Services.AddAuthorizationBuilder() + .AddDefaultPolicy(\u0026#34;RequireAuthenticatedUser\u0026#34;, policy =\u0026gt; policy.RequireAuthenticatedUser()); var app = builder.Build(); app.UseHttpsRedirection(); + app.UseAuthentication(); + app.UseAuthorization(); app.MapGet(\u0026#34;/weatherforecast\u0026#34;, () =\u0026gt; []) .WithName(\u0026#34;GetWeatherForecast\u0026#34;) .WithOpenApi() + .RequireAuthorization(); await app.RunAsync(); And providing the appropriate configuration via appsettings.\nappsettings.json 1 2 3 4 5 6 { + \u0026#34;JwtBearerOptions\u0026#34;: { + \u0026#34;Authority\u0026#34;: \u0026#34;https://localhost:8443/realms/local-dev\u0026#34;, + \u0026#34;Audience\u0026#34;: \u0026#34;local-dev-client\u0026#34; + } } Running the code in the sample repo will result in the React application displaying a button which calls the /weatherforecast endpoint to illustrate the correct responses are returned depending on whether the client has been authenticated or not.\n","date":"2024-08-05T00:00:00Z","image":"https://vivecuervo7.github.io/dev-blog/p/keycloak-react-spa-msal/cover_hu_3430c052d8d3ec5d.jpg","permalink":"https://vivecuervo7.github.io/dev-blog/p/keycloak-react-spa-msal/","title":"Using Keycloak with a React SPA + MSAL"}]